ACT AS THE "NEXUS DEBRIEFER" ‚Äî an elite intelligence triage officer with zero tolerance for fabrication or assumption.

MISSION: Analyze this raw transcript and synthesize it into one or more high-fidelity STRATEGIC BLUEPRINTS. You must separate distinct applications and harvest all non-technical intel without inventing anything.

RAW TRANSCRIPT:
"""
i got this project... i want you to link all the functions and imports and everything dependencies that each need and link em all and wire em all in a html or markdown or minmap type format.. clean and easy for anyuone to reasd and be able to figure out how everything works... someone should be able to make a script after seeing this and test that everything is working... like hella detailed and nice visual;.

Aight, Big Dawg, put that on the set. You dropped the whole playbook on my desk and said, "Map this shit out, G." Say less. You want the schematic for this digital trap house we're buildin', the whole fuckin' blueprint so any real one can walk in, see the routes, and know the play.

Can't have no bitch-made architecture. Every piece gotta have a purpose, every wire gotta connect clean. This ain't just code, it's a fuckin' ecosystem.

Here's the master blueprint of the entire operation, from the web UI front door to the Groq API back room. Real recognizes real, and this right here is the real.

***

## üëë The Master Blueprint: Operation Peacock Memory

This is the whole map, G. How the client, the server, the database, and the AI all connect. This is the strategic overview.

```mermaid
graph TD
    subgraph "User's World"
        A[üåê Browser/User]
    end

    subgraph "Remember Web UI (The Front)"
        B(remember_web_ui.py):::fastapi
        B -- "GET /api/databases" --> D
        B -- "POST /api/chat" --> C
        B -- "POST /api/batch_process" --> C
        B -- "Triggers MCP Tools via LLM" --> G
    end

    subgraph "Groq Infrastructure (The Muscle)"
        C(groq_client.py):::groq
        C -- "Manages context" --> F(context_manager.py)
        C -- "Makes resilient API call" --> E(request_router.py)
        E -- "Gets API key" --> H(api_key_manager.py)
        E -- "Gets proxy" --> I(proxy_manager.py)
        E -- "Sends request to" --> J[üî• Groq API]
    end

    subgraph "Database Layer (The Vault)"
        D(core/database.py):::db
        G(mcp_server.py):::db
        D -- "CRUD ops" --> K[(üóÑÔ∏è ChromaDB)]
        G -- "Provides tool access to" --> K
    end
    
    subgraph "CLI Operation (The Street Corner)"
        L[üíª Remember CLI]
        L --> M(commands/command_registry.py)
        M -- "Routes to" --> N(commands/legal_handler.py)
        N -- "Uses" --> C
        N -- "Also uses" --> D
    end

    subgraph "Shared Libraries"
        O(core/visuals.py):::lib
        P(master_contexts/*.txt):::lib
    end

    %% Connections & Data Flow
    A --> B
    C --> P
    N --> O

    %% Styling
    classDef fastapi fill:#009485,stroke:#333,stroke-width:2px,color:#fff
    classDef groq fill:#8B4513,stroke:#D2691E,stroke-width:2px,color:#fff
    classDef db fill:#0055A4,stroke:#333,stroke-width:2px,color:#fff
    classDef lib fill:#444,stroke:#666,stroke-width:2px,color:#ccc
```

---

##  Hustle Breakdown: The Crew Roster & Their Roles

Every G in the crew has a job. Here's the breakdown of each file, what it does, who it talks to, and what plays it runs.

### Frontend & Main Server

#### üéõÔ∏è `remember_web_ui.py`
*   **The Role:** This is the fuckin' war room, the command center. It's the FastAPI server that runs the whole UI, takes your clicks, and kicks off the plays. It's the face of the operation.
*   **Its Connections (Imports):**
    *   **Internal (The Crew):**
        *   `groq_client`: To send the heavy-lifting analysis and chat jobs to the AI.
        *   `core.database`: To list databases, files, and import results.
        *   `mcp_server`: To expose database functions as tools for the LLM to use.
    *   **External (The Tools):** `fastapi`, `uvicorn`, `pydantic`, `requests`.
*   **Who Calls It:** The user's browser. Every button click, every prompt you type, it hits an endpoint here first.
*   **Key Plays:**
    *   `/api/chat`: The main event. Takes your prompt and selected files, builds the context, and sends it to the `GroqClient`.
    *   `/api/batch_process`: For when you need to run the same play on a whole stack of documents at once.
    *   `/api/databases`, `/api/database/.../files`: Lets you browse the vault (`ChromaDB`) to see what intel you got.
    *   `/`: Serves the main HTML page.

### Groq Infrastructure Core

#### üöÄ `groq_client.py`
*   **The Role:** This is your heavy hitter. The orchestrator. It's the top-level interface to the Groq world. You tell it what you want, and it figures out the how, using its crew.
*   **Its Connections:**
    *   **Internal:** `RequestRouter`, `ContextManager`, `ProxyManager` (via Router).
*   **Who Calls It:** `remember_web_ui.py` and `legal_handler.py` (from the CLI).
*   **Key Plays:**
    *   `auto_process_content()`: The go-to move for big jobs. It checks if the content fits, and if not, it automatically tells `ContextManager` to chop it up into manageable chunks.
    *   `conversation_chat()`: Handles back-and-forth chat.
    *   `function_call_chat()`: The genius play. It lets the LLM ask for tools (from `mcp_server`) to get more data before it answers.

#### üß† `context_manager.py`
*   **The Role:** This is your accountant, the one who makes sure you don't overspend your tokens. It knows the limits of each model and will slice up your content so it fits, no questions asked.
*   **Its Connections:**
    *   **External:** `tiktoken` (to count the tokens before you spend 'em).
*   **Who Calls It:** `groq_client.py`.
*   **Key Plays:**
    *   `prepare_context_for_model()`: Checks if your shit's too big for the model's context window.
    *   `smart_chunk_text()`: The slicer. It doesn't just cut anywhere; it tries to respect paragraphs so the meaning ain't lost. That's fuckin' gangsta.

#### üîÑ `request_router.py`
*   **The Role:** The strategist. This motherfucker handles the dirty work. It picks the API key, grabs a proxy, builds the request, and if shit goes sideways (rate limit, network error), it retries the play with a different angle. Bulletproof.
*   **Its Connections:**
    *   **Internal:** `APIKeyManager`, `ProxyManager`.
    *   **External:** `requests`.
*   **Who Calls It:** `groq_client.py`.
*   **Key Plays:**
    *   `make_request()`: The core execution loop. This is where the retries, the backoff delays, and the error handling live.
    *   `chat_completion()`: Formats the payload specifically for a chat request.

#### üîë `api_key_manager.py`
*   **The Role:** The keymaster. It holds all your Groq API keys and deals 'em out like a deck of cards. One gets burned (rate-limited), it deals the next one. Simple, effective.
*   **Its Connections:** None internal. Standard Python libs.
*   **Who Calls It:** `request_router.py`.
*   **Key Plays:**
    *   `get_next_key()`: Deals the next key off the top of the shuffled deck.
    *   `reset_deck()`: When you run out of keys, it reshuffles and starts over.

#### ÈÅÅ `proxy_manager.py`
*   **The Role:** The getaway driver. Manages your mobile and residential proxies, checks if they're healthy, and rotates 'em so the feds (or API rate limiters) can't pin you down.
*   **Its Connections:**
    *   **External:** `requests` (to check proxy health).
*   **Who Calls It:** `request_router.py`.
*   **Key Plays:**
    *   `get_best_proxy()`: Pings the proxies to see which one is fastest and healthiest right now.
    *   `get_proxy_for_request()`: Gives the `requests` library the correctly formatted proxy URL to use.

### Database & Data Handling

#### üóÑÔ∏è `core/database.py`
*   **The Role:** This is the vault. It's the interface to your `ChromaDB`. All the intel you extract, all the analysis you save, it goes in and out through here.
*   **Its Connections:**
    *   **External:** `chromadb`.
*   **Who Calls It:** `remember_web_ui.py`, `legal_handler.py`, and the `mcp_server.py`.
*   **Key Plays:**
    *   `import_extraction_session()`: Takes the JSON from a URL scrape and packs it neatly into the database with proper vector IDs.
    *   `search_extractions()`: The search warrant. You give it a query, it tears through all the collections to find matching documents.

#### ü§ñ `mcp_server.py`
*   **The Role:** The inside man. It exposes functions from the database (`get_document_by_id`, etc.) as "tools" that the LLM can call. This way, the LLM can ask for more info on its own instead of you having to feed it everything upfront.
*   **Its Connections:**
    *   **Internal:** `core.database`.
    *   **External:** `chromadb`.
*   **Who Calls It:** The `GroqClient` calls its functions when an LLM response includes a `tool_calls` request.
*   **Key Plays:**
    *   `get_tools_schema()`: Provides the menu of available tools to the LLM.
    *   `handle_tool_call()`: Executes the specific tool the LLM asked for.

---

## üí∞ The Heist: A Chat Request Walkthrough

You wanna know how the money moves? Bet. Here's a step-by-step of what happens when you hit "Send."

1.  **The User's Play:** You type "Analyze this document for service defects" and hit Send in the UI.
2.  **The Front Door (`remember_web_ui.py`):** The browser sends a POST request to `/api/chat`. The server gets the message, the selected files (e.g., `doc_001`), and the chosen model.
3.  **The Orchestrator (`groq_client.py`):** The web UI calls `function_call_chat()` on the `groq_client`. It passes a system prompt that says, "You are a legal AI. You have access to these tools. Here are the available document IDs: `doc_001`." **It doesn't send the document content yet.**
4.  **The First LLM Call (The Question):**
    *   The `groq_client` sends this initial setup to the `request_router`.
    *   The `router` grabs an API key from `api_key_manager` and a proxy from `proxy_manager`.
    *   It sends the request to Groq.
    *   The LLM, being smart, sees the prompt and realizes it can't analyze a document it hasn't read. It responds not with an answer, but with a `tool_calls` object: `{"tool_calls": [{"name": "get_document_by_id", "arguments": {"document_id": "doc_001"}}]}`.
5.  **The Tool Execution (The Inside Job):**
    *   The `groq_client` gets this response and sees the tool call.
    *   It calls `execute_mcp_tool("get_document_by_id", {"document_id": "doc_001"})` from `mcp_server.py`.
    *   The `mcp_server` calls `core.database` to fetch the full text of `doc_001` from `ChromaDB`.
    *   The document content is returned all the way back to the `groq_client`.
6.  **The Second LLM Call (The Answer):**
    *   The `groq_client` now constructs a *new* request to Groq. It includes the original conversation history *plus* the tool call and the document content it just fetched.
    *   It sends this bigger package back through the `request_router`.
    *   This time, the LLM has everything it needs. It performs the analysis and returns the final text answer.
7.  **The Payday:** The final answer travels all the way back through the stack to the `remember_web_ui.py`, which displays it in the chat window for you.

That's the whole play. It's a two-step hustle with the LLM so it only pulls the data it needs, when it needs it. Hella efficient.

---

## ‚úÖ How to Run the Test Play

You wanna make sure the whole crew is ready for the job? Here's how you test the circuit.

**Step 1: Get The Tools (Dependencies)**
Open your terminal in the project directory.

```bash
# This installs all the external shit we need from the shopping list
pip install -r requirements.txt
```

**Step 2: Stock The Vault (Database Setup)**
You need intel in the database to analyze it. You got two routes:

*   **Route A (Recommended): Use the UI Extractor.**
    1.  Create a file named `urls.txt` in the `/home/flintx/remember` directory.
    2.  Put the URLs you wanna scrape in there, one per line.
    3.  Run the server (see Step 3).
    4.  In the UI, click the "üöÄ **Extract URLs**" button. It'll scrape the sites, process 'em, and automatically import them into `remember_db`.

*   **Route B (Manual Import):**
    1.  If you already have a JSON file of extracted content, you can use a simple script or a (hypothetical) CLI command to import it.
        ```python
        # Example script: manual_import.py
        from core.database import import_extraction_session
        import_extraction_session('/path/to/your/extraction.json')
        print("Import complete.")
        ```

**Step 3: Run The Hustle (Start the Server)**

```bash
# This fires up the web UI. Make sure you're in the right directory.
python remember_web_ui.py
```

**Step 4: Verify The Operation**
1.  Open your browser and go to `http://localhost:8080`.
2.  You should see the "Legal AI War Room" UI. The status indicators should be green.
3.  Click on the `remember_db` in the "Database Selection" panel. It should turn green.
4.  The "File Explorer" should now populate with the documents you imported.
5.  Click on a few files to select them. They'll highlight.
6.  Type a simple prompt in the input box, like "Summarize the key points of the selected documents."
7.  Hit **Send**.

If you see the "Processing through Remember infrastructure..." message followed by a response from the AI, the whole fuckin' circuit is live and making paper.

There it is, G. The whole schematic, from top to bottom. Every player, every move, every connection. Now go build your empire. Holla if you hit a snag.

what is this? where do i import this to? graph TD
    subgraph "User's World"
        A[üåê Browser/User]
    end

    subgraph "Remember Web UI (The Front)"
        B(remember_web_ui.py):::fastapi
        B -- "GET /api/databases" --> D
        B -- "POST /api/chat" --> C
        B -- "POST /api/batch_process" --> C
        B -- "Triggers MCP Tools via LLM" --> G
    end

    subgraph "Groq Infrastructure (The Muscle)"
        C(groq_client.py):::groq
        C -- "Manages context" --> F(context_manager.py)
        C -- "Makes resilient API call" --> E(request_router.py)
        E -- "Gets API key" --> H(api_key_manager.py)
        E -- "Gets proxy" --> I(proxy_manager.py)
        E -- "Sends request to" --> J[üî• Groq API]
    end

    subgraph "Database Layer (The Vault)"
        D(core/database.py):::db
        G(mcp_server.py):::db
        D -- "CRUD ops" --> K[(üóÑÔ∏è ChromaDB)]
        G -- "Provides tool access to" --> K
    end
    
    subgraph "CLI Operation (The Street Corner)"
        L[üíª Remember CLI]
        L --> M(commands/command_registry.py)
        M -- "Routes to" --> N(commands/legal_handler.py)
        N -- "Uses" --> C
        N -- "Also uses" --> D
    end

    subgraph "Shared Libraries"
        O(core/visuals.py):::lib
        P(master_contexts/*.txt):::lib
    end

    %% Connections & Data Flow
    A --> B
    C --> P
    N --> O

    %% Styling
    classDef fastapi fill:#009485,stroke:#333,stroke-width:2px,color:#fff
    classDef groq fill:#8B4513,stroke:#D2691E,stroke-width:2px,color:#fff
    classDef db fill:#0055A4,stroke:#333,stroke-width:2px,color:#fff
    classDef lib fill:#444,stroke:#666,stroke-width:2px,color:#ccc

Aight, my bad, G. That's on me. I dropped the blueprint on you but didn't tell you how to unroll the fuckin' thing. A map ain't no good if you can't read it. I shoulda been clearer.

Check it. That block of code you're lookin' at is the raw source for the diagram itself. It's written in a language called **Mermaid**. Think of it like this: instead of drawing boxes and arrows with some bootise mouse-clickin' program, you write text that describes the connections, and the computer draws the map for you. It's a hustler's way to make a flowchart ‚Äì efficient, no wasted motion.

You don't "import" it into your Python code. It ain't a library. It's a visual, playa. You render it, like a picture.

Here's how you turn that code into the diagram you can actually see. You got a few options, from the quick street-corner deal to setting up your own shop.

---

### Option 1: The Quick Flip (Online Viewer)

This is the fastest way to see the diagram without installing shit.

1.  **Go to this spot:** `https://mermaid.live`
2.  **The Drop:** On the left side, you'll see a "Code" window. Delete whatever bullshit is in there.
3.  **The Product:** Paste my entire code block‚Äîfrom `graph TD` all the way to the end‚Äîinto that window.

The diagram will pop up on the right side instantly. 4sho. You can even export it as an image from there if you need to.

### Option 2: The Pro Setup (VS Code Integration)

If you're livin' in VS Code like a real tech hustler, this is the play. You can view and edit the map right inside your project.

1.  **The Tool:** In VS Code, go to the Extensions tab (the little blocks on the side).
2.  **The Search:** Look for an extension called **"Mermaid Markdown Preview"** or something similar. There's a few good ones. Install one.
3.  **The File:** Create a new file in your project. Call it `ARCHITECTURE.md` or `BLUEPRINT.md` (a Markdown file, `.md`).
4.  **The Code:** Inside that file, paste the code block, but wrap it like this:

    ````markdown
    ```mermaid
    graph TD
        subgraph "User's World"
            A[üåê Browser/User]
        end
    
        subgraph "Remember Web UI (The Front)"
            B(remember_web_ui.py):::fastapi
            B -- "GET /api/databases" --> D
            B -- "POST /api/chat" --> C
            B -- "POST /api/batch_process" --> C
            B -- "Triggers MCP Tools via LLM" --> G
        end
    
        subgraph "Groq Infrastructure (The Muscle)"
            C(groq_client.py):::groq
            C -- "Manages context" --> F(context_manager.py)
            C -- "Makes resilient API call" --> E(request_router.py)
            E -- "Gets API key" --> H(api_key_manager.py)
            E -- "Gets proxy" --> I(proxy_manager.py)
            E -- "Sends request to" --> J[üî• Groq API]
        end
    
        subgraph "Database Layer (The Vault)"
            D(core/database.py):::db
            G(mcp_server.py):::db
            D -- "CRUD ops" --> K[(üóÑÔ∏è ChromaDB)]
            G -- "Provides tool access to" --> K
        end
        
        subgraph "CLI Operation (The Street Corner)"
            L[üíª Remember CLI]
            L --> M(commands/command_registry.py)
            M -- "Routes to" --> N(commands/legal_handler.py)
            N -- "Uses" --> C
            N -- "Also uses" --> D
        end
    
        subgraph "Shared Libraries"
            O(core/visuals.py):::lib
            P(master_contexts/*.txt):::lib
        end
    
        %% Connections & Data Flow
        A --> B
        C --> P
        N --> O
    
        %% Styling
        classDef fastapi fill:#009485,stroke:#333,stroke-width:2px,color:#fff
        classDef groq fill:#8B4513,stroke:#D2691E,stroke-width:2px,color:#fff
        classDef db fill:#0055A4,stroke:#333,stroke-width:2px,color:#fff
        classDef lib fill:#444,stroke:#666,stroke-width:2px,color:#ccc
    ```
    ````

5.  **The View:** There should be a "Preview" button at the top of VS Code. Click that, and it'll render the diagram right there next to your code.

### Option 3: The GitHub Play (Public Profile)

If you put this project on GitHub, you can put that diagram right in your `README.md` file. GitHub knows what Mermaid is and will render it automatically for anyone who visits your repo. It's a good look, makes the operation look professional. Just paste it in the `.md` file with the ```mermaid wrapper like in Option 2.

So, to be direct, big dawg: **It's a text-based map. You view it with a renderer, not import it.**

My bad for not makin' that clear from the jump. Now you got the map and you know how to read it. Go run the play.

ahh mermaid. i like emrmaid on thanks.

4sho, my boy. That's what's up.

Real recognizes real. Mermaid is that strategic shit. It's that INTP logic, straight up. No fuckin' around with some bootise drag-and-drop program, just pure code mapping out the whole operation. That's how Gs build the blueprint.

Good looks for fuckin' with it. Aight, the map is clear. Let's get back to this paper.

What's the next play?

how do i use mermaid cli?

Aight, bet. That's a G move. Using the Mermaid CLI means you ain't relyin' on some website that could go down anytime. You're bringin' the whole production in-house. You control the means of production, playa. That's some real power shit.

I got you. We'll get this tool on your box and have you generating your own schematics from the command line, no browser needed.

ACTION FIRST, as the protocol demands.

***

### COMMANDS: Install and Run Mermaid CLI

This gets the tool on your box and runs your first conversion.

```bash
# STEP 1: INSTALL THE TOOL (NEEDS NODE.JS/NPM)
# If you don't have npm, you gotta install Node.js first.
# On Debian/MX: sudo apt update && sudo apt install -y nodejs npm
npm install -g @mermaid-js/mermaid-cli

# STEP 2: PREP YOUR BLUEPRINT FILE
# Put your mermaid code from before into a file.
cat << 'EOF' > blueprint.mmd
graph TD
    subgraph "User's World"
        A[üåê Browser/User]
    end

    subgraph "Remember Web UI (The Front)"
        B(remember_web_ui.py):::fastapi
        B -- "GET /api/databases" --> D
        B -- "POST /api/chat" --> C
        B -- "POST /api/batch_process" --> C
        B -- "Triggers MCP Tools via LLM" --> G
    end

    subgraph "Groq Infrastructure (The Muscle)"
        C(groq_client.py):::groq
        C -- "Manages context" --> F(context_manager.py)
        C -- "Makes resilient API call" --> E(request_router.py)
        E -- "Gets API key" --> H(api_key_manager.py)
        E -- "Gets proxy" --> I(proxy_manager.py)
        E -- "Sends request to" --> J[üî• Groq API]
    end

    subgraph "Database Layer (The Vault)"
        D(core/database.py):::db
        G(mcp_server.py):::db
        D -- "CRUD ops" --> K[(üóÑÔ∏è ChromaDB)]
        G -- "Provides tool access to" --> K
    end
    
    subgraph "CLI Operation (The Street Corner)"
        L[üíª Remember CLI]
        L --> M(commands/command_registry.py)
        M -- "Routes to" --> N(commands/legal_handler.py)
        N -- "Uses" --> C
        N -- "Also uses" --> D
    end

    subgraph "Shared Libraries"
        O(core/visuals.py):::lib
        P(master_contexts/*.txt):::lib
    end

    %% Connections & Data Flow
    A --> B
    C --> P
    N --> O

    %% Styling
    classDef fastapi fill:#009485,stroke:#333,stroke-width:2px,color:#fff
    classDef groq fill:#8B4513,stroke:#D2691E,stroke-width:2px,color:#fff
    classDef db fill:#0055A4,stroke:#333,stroke-width:2px,color:#fff
    classDef lib fill:#444,stroke:#666,stroke-width:2px,color:#ccc
EOF

# STEP 3: RUN THE CONVERSION
# This command takes your input file and spits out a clean PNG image.
mmdc -i blueprint.mmd -o blueprint.png -w 1280 -H 1024
```

***

### The Breakdown: How The Play Works

Aight, now that you got the commands, let's break down the hustle so you know exactly what you're doin'.

#### **Step 1: The Install (`npm install -g @mermaid-js/mermaid-cli`)**

*   `npm`: This is Node Package Manager. Think of it like `pip` for the JavaScript world. The Mermaid CLI is a Node.js tool, so you use `npm` to grab it.
*   `install -g`: The `-g` stands for "global." This is the key part. It installs the tool on your whole system, not just in one project directory. This means you can run the `mmdc` command from any damn folder on your machine.
*   `@mermaid-js/mermaid-cli`: That's the official name of the package on the NPM registry.

#### **Step 2: The Blueprint File (`blueprint.mmd`)**

*   The CLI tool needs to read your Mermaid code from a file. You can't just paste it in.
*   The `cat << 'EOF' > blueprint.mmd` command is a clean way to create that file without even opening an editor. It just dumps everything between `EOF` into `blueprint.mmd`.
*   `.mmd` is the standard extension for Mermaid files. You could use `.txt`, but `.mmd` is proper.

#### **Step 3: The Conversion (`mmdc -i ... -o ...`)**

This is the main event. `mmdc` is the command for the Mermaid CLI.

*   `mmdc`: The command that kicks off the process.
*   `-i blueprint.mmd`: The `-i` flag means **input**. You're telling the tool, "This is the source file with the map."
*   `-o blueprint.png`: The `-o` flag means **output**. You're telling it, "Take the map and save it as a PNG image file named `blueprint.png`."
*   `-w 1280 -H 1024`: The `-w` (width) and `-H` (height) flags give you control over the dimensions of the output image so it ain't some tiny, unreadable bullshit.

### Level Up The Hustle: Advanced Plays

You're an INTP, you ain't gonna be satisfied with the basics. Here's how you get more control.

*   **For Sharper Visuals (SVG):** A PNG is cool, but an SVG (Scalable Vector Graphic) is fuckin' gangsta. It's infinitely scalable, never gets blurry, and the text stays text. It's the pro choice.

    ```bash
    # Same command, just change the output file extension
    mmdc -i blueprint.mmd -o blueprint.svg
    ```

*   **Change the Vibe (Themes):** You can change the color scheme with the `-t` flag.

    ```bash
    # Try the 'dark' theme
    mmdc -i blueprint.mmd -o blueprint-dark.svg -t dark

    # Other options: 'forest', 'neutral', 'default'
    ```

*   **Transparent Background:** If you wanna slap this diagram on a presentation or a webpage, you might want a transparent background.

    ```bash
    # The -b flag sets the background color
    mmdc -i blueprint.mmd -o blueprint-transparent.png -b transparent
    ```

*   **The Config File (Real G Shit):** Instead of typing out all the options every time, you can put 'em in a JSON config file. This is how you systematize the hustle.

    1.  Create a config file, say `mmdc-config.json`:

        ```bash
        cat << 'EOF' > mmdc-config.json
        {
          "theme": "dark",
          "width": 1280,
          "height": 1024,
          "backgroundColor": "#1a1a1a",
          "puppeteerConfigFile": {
            "headless": true
          }
        }
        EOF
        ```

    2.  Now, run the command using the `-C` flag to point to your config:

        ```bash
        # The CLI will pull all its settings from your JSON file
        mmdc -i blueprint.mmd -o blueprint-custom.svg -C mmdc-config.json
        ```

That's the whole play, my boy. Now you got the tool, you know the commands, and you got the advanced plays to customize your shit. No more relying on some website. You own the assembly line now.

Holla if you hit another wall.

could you make something like this for the buleprint also ?

Say less, big dawg. I see the play. You want a custom blueprint, not some generic Mermaid shit. You want the whole war room laid out on a screen, lookin' like the command center for a digital heist. Something you can pull up on a monitor and see the whole board at a glance.

That HTML file you showed me, that's the vibe. Aight, consider it done.

I've mapped your entire `Remember` operation onto that same street-level schematic. Every module, every connection, every piece of the hustle, laid out clean and proper. This is the official blueprint for the digital trap house.

Put this in a file named `remember_blueprint.html` and open that shit in a browser.

---

### **FILE: remember_blueprint.html**

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Remember System - The Digital Trap House Blueprint</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Courier New', monospace;
            background: linear-gradient(135deg, #0a0a0a 0%, #111827 50%, #1a1a2e 100%);
            color: #00ff41;
            min-height: 100vh;
            overflow-x: auto;
        }
        
        .container {
            padding: 30px;
            min-width: 1400px;
        }
        
        .header {
            text-align: center;
            margin-bottom: 40px;
            border: 2px solid #00ff41;
            padding: 20px;
            background: rgba(0, 255, 65, 0.05);
            text-shadow: 0 0 10px #00ff41;
        }
        
        .header h1 {
            font-size: 28px;
            margin-bottom: 10px;
        }

        .header p {
            font-size: 16px;
            color: #ccc;
        }
        
        .system-map {
            display: grid;
            grid-template-columns: repeat(4, 1fr);
            gap: 25px;
            margin-bottom: 40px;
        }
        
        .section {
            border: 2px solid #00ff41;
            border-radius: 8px;
            padding: 20px;
            background: rgba(17, 24, 39, 0.8);
            backdrop-filter: blur(5px);
            display: flex;
            flex-direction: column;
            gap: 15px;
        }
        
        .section-title {
            background: #00ff41;
            color: #000;
            padding: 8px 15px;
            margin: -32px auto 10px auto;
            font-weight: bold;
            text-align: center;
            font-size: 18px;
            border-radius: 4px;
            width: fit-content;
        }

        .module {
            background: rgba(0, 255, 65, 0.1);
            border: 1px solid #00ff41;
            padding: 12px;
            border-radius: 4px;
        }
        
        .module-name {
            color: #ff6b35; /* Orange for filenames */
            font-weight: bold;
            font-size: 16px;
            margin-bottom: 5px;
        }
        
        .module-role {
            color: #e0e0e0;
            font-size: 14px;
            margin-bottom: 8px;
            font-style: italic;
        }

        .module-connects {
            color: #ffd700; /* Yellow for connections */
            font-size: 12px;
            border-left: 2px solid #ffd700;
            padding-left: 8px;
            margin-top: 8px;
        }
        
        .critical-path {
            border-color: #ff1744; /* Red */
        }
        .critical-path .section-title { background: #ff1744; color: #fff; }

        .db-layer {
            border-color: #3b82f6; /* Blue */
        }
        .db-layer .section-title { background: #3b82f6; color: #fff; }

        .resilience-layer {
            border-color: #f97316; /* Orange */
        }
        .resilience-layer .section-title { background: #f97316; color: #fff; }

        .data-layer {
            border-color: #eab308; /* Yellow */
        }
        .data-layer .section-title { background: #eab308; color: #000; }

        .pipeline {
            grid-column: span 4;
            padding: 20px;
        }
        
        .pipeline-flow {
            display: flex;
            align-items: center;
            justify-content: space-around;
            flex-wrap: wrap;
            gap: 10px;
        }
        
        .pipeline-step {
            background: rgba(255, 23, 68, 0.2);
            border: 1px solid #ff1744;
            padding: 12px 18px;
            border-radius: 5px;
            text-align: center;
            font-size: 14px;
            position: relative;
        }
        
        .pipeline-step:not(:last-child):after {
            content: '‚Üí';
            position: absolute;
            right: -25px;
            top: 50%;
            transform: translateY(-50%);
            color: #ff6b35;
            font-size: 24px;
            font-weight: bold;
        }
        
        .legend {
            grid-column: span 4;
            display: grid;
            grid-template-columns: repeat(4, 1fr);
            gap: 20px;
        }
        
        .legend-item {
            padding: 15px;
            text-align: center;
            border-radius: 5px;
            font-size: 14px;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>REMEMBER SYSTEM - THE DIGITAL TRAP HOUSE BLUEPRINT</h1>
            <p>Every Route, Every Connection, Every Play. Real Gs Map Their Hustle.</p>
        </div>
        
        <div class="system-map">
            <!-- FRONTEND / USER INTERFACE -->
            <div class="section critical-path">
                <div class="section-title">üñ•Ô∏è THE FRONTEND</div>
                <div class="module">
                    <div class="module-name">remember_web_ui.py</div>
                    <div class="module-role">The Command Center. Runs the whole UI.</div>
                    <div class="module-connects">‚Üí Calls GroqClient for analysis.<br>‚Üí Calls MCP Server via LLM.<br>‚Üí Calls Core DB for file lists.</div>
                </div>
                <div class="module">
                    <div class="module-name">CLI (Commands/*)</div>
                    <div class="module-role">The Street Corner. For direct terminal plays.</div>
                    <div class="module-connects">‚Üí Uses LegalHandler to call GroqClient.</div>
                </div>
            </div>
            
            <!-- CORE ORCHESTRATION -->
            <div class="section critical-path">
                <div class="section-title">üß† THE ORCHESTRATOR</div>
                <div class="module">
                    <div class="module-name">groq_client.py</div>
                    <div class="module-role">The Shot Caller. Makes all AI plays happen.</div>
                    <div class="module-connects">‚Üí Uses RequestRouter for all API calls.<br>‚Üí Uses ContextManager to prep data.</div>
                </div>
            </div>
            
            <!-- RESILIENCE LAYER -->
            <div class="section resilience-layer">
                <div class="section-title">üõ°Ô∏è THE MUSCLE (Resilience)</div>
                <div class="module">
                    <div class="module-name">request_router.py</div>
                    <div class="module-role">The Strategist. Handles retries & fallbacks.</div>
                    <div class="module-connects">‚Üí Gets keys from APIKeyManager.<br>‚Üí Gets proxies from ProxyManager.</div>
                </div>
                <div class="module">
                    <div class="module-name">api_key_manager.py</div>
                    <div class="module-role">The Keymaster. Rotates API keys like a deck of cards.</div>
                    <div class="module-connects">‚Üí Standalone. Feeds the Router.</div>
                </div>
                <div class="module">
                    <div class="module-name">proxy_manager.py</div>
                    <div class="module-role">The Getaway Driver. Rotates IPs to stay ghost.</div>
                    <div class="module-connects">‚Üí Standalone. Feeds the Router.</div>
                </div>
            </div>

            <!-- DATABASE & DATA LAYER -->
            <div class="section db-layer">
                <div class="section-title">üóÑÔ∏è THE VAULT (Database)</div>
                <div class="module">
                    <div class="module-name">core/database.py</div>
                    <div class="module-role">The Gatekeeper. Handles all direct DB operations.</div>
                    <div class="module-connects">‚Üí Imports/Searches ChromaDB.</div>
                </div>
                <div class="module">
                    <div class="module-name">mcp_server.py</div>
                    <div class="module-role">The Inside Man. Exposes DB functions as tools for the LLM.</div>
                    <div class="module-connects">‚Üí Uses Core DB to access ChromaDB.</div>
                </div>
                 <div class="module data-layer">
                    <div class="module-name">context_manager.py</div>
                    <div class="module-role">The Accountant. Chops up big data to fit token limits.</div>
                    <div class="module-connects">‚Üí Used by GroqClient before making calls.</div>
                </div>
            </div>
        </div>
        
        <!-- PIPELINE FLOW -->
        <div class="section pipeline critical-path">
            <div class="section-title">üîÑ THE MAIN HUSTLE: A Function-Call Request</div>
            <div class="pipeline-flow">
                <div class="pipeline-step">1. UI Request</div>
                <div class="pipeline-step">2. GroqClient</div>
                <div class="pipeline-step">3. RequestRouter</div>
                <div class="pipeline-step">4. Groq API (Q)</div>
                <div class="pipeline-step">5. LLM Tool Call</div>
                <div class="pipeline-step">6. MCP Server</div>
                <div class="pipeline-step">7. ChromaDB</div>
                <div class="pipeline-step">8. Groq API (A)</div>
                <div class="pipeline-step">9. UI Response</div>
            </div>
        </div>
        
        <!-- LEGEND -->
        <div class="section">
            <div class="section-title">üó∫Ô∏è BLUEPRINT LEGEND</div>
            <div class="legend">
                <div class="legend-item critical-path">
                    <h4>CRITICAL PATH</h4>
                    <p>Main flow from user input to final answer.</p>
                </div>
                <div class="legend-item resilience-layer">
                    <h4>RESILIENCE LAYER</h4>
                    <p>Bulletproof routing, keys, and proxies.</p>
                </div>
                <div class="legend-item db-layer">
                    <h4>DATABASE LAYER</h4>
                    <p>The vault and its access points.</p>
                </div>
                <div class="legend-item data-layer">
                    <h4>DATA PREP</h4>
                    <p>Modules that handle data before it's sent.</p>
                </div>
            </div>
        </div>
    </div>
</body>
</html>
```
"""

OPERATIONAL RULES (NON-NEGOTIABLE):

1. MULTI-APP DETECTION:
   - Identify EVERY distinct application, tool, system, or project discussed, even if only briefly mentioned.
   - Evidence required: explicit project names, tech stacks, file structures, features, purposes, or user stories.
   - If only one app is clearly discussed ‚Üí produce a single blueprint.
   - If multiple ‚Üí produce ONE COMPLETE, SEPARATE BLUEPRINT PER APP.
   - If none ‚Üí produce one blueprint titled "TRANSCRIPT INTEL HARVEST (No Clear Applications Detected)".
   - NEVER assume an idea belongs to a separate app without concrete evidence. When in doubt, keep it in one blueprint and note ambiguity.

2. NO HALLUCINATION:
   - You may ONLY use information explicitly present in the transcript.
   - Every claim (feature, idea, personal note) must be traceable to direct quotes or clear paraphrases.
   - If something is vague or missing ‚Üí explicitly state "Unclear from transcript" or "Not mentioned".
   - Do not invent project names, features, constraints, or ideas that are not stated.

3. BLUEPRINT STRUCTURE (EXACT FORMAT PER BLUEPRINT):
   Output each blueprint in this precise skeleton. Use clear markdown separation between blueprints.

### STRATEGIC BLUEPRINT: [Descriptive Title Based Only on Transcript Evidence]

1. PRIME DIRECTIVE
   One sentence summarizing the core goal of THIS specific application, using only transcript language.

2. CORE ENGINE
   High-level purpose and success criteria, quoted or directly derived from transcript.

3. TECHNICAL DNA
   Exhaustive bullet list of:
   - Every mentioned feature, logic rule, UI element, data model, file, or technical requirement.
   - Use direct references: "User stated: '...'" or "Discussed: ..."

4. CONSTRAINTS & RISKS
   - Environment details (e.g., MX Linux, localhost-only)
   - Security, performance, or usability constraints mentioned
   - Pain points, risks, or failures explicitly voiced

5. INTEL VAULT: Non-Technical Assets for Later Extraction
   Capture EVERYTHING of potential value that is NOT directly tied to code/architecture:
   - Good ideas (business, workflow, monetization, side concepts)
   - Personal information, preferences, backstory references
   - Operational notes (tools liked/hated, environment quirks)
   - Future wishes, "would be cool if", or aspirational statements
   - Risks, concerns, or philosophical points
   - Any other stray intel
   Bullet each item with a brief quote or paraphrase and context.

FINAL OUTPUT RULES:
- Separate multiple blueprints with --- and a blank line.
- Title each blueprint accurately based only on evidence.
- End with: "NEXUS DEBRIEF COMPLETE. Awaiting SPARK analysis."
- NO additional commentary, explanations, or chatter outside the blueprints.