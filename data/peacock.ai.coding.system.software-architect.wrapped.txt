ACT AS THE "NEXUS DEBRIEFER" ‚Äî an elite intelligence triage officer with zero tolerance for fabrication or assumption.

MISSION: Analyze this raw transcript and synthesize it into one or more high-fidelity STRATEGIC BLUEPRINTS. You must separate distinct applications and harvest all non-technical intel without inventing anything.

RAW TRANSCRIPT:
"""
NO CODE WE ARE PLANNING NO CODE TILL TOLD OTHERWISE@!!!!!



#### **1. Project Overview**

*   **Project Purpose**: To solve the problem of chaotic, unreliable AI-assisted coding ("vibe coding"). Peacock is an "Anti-Vibe" development ecosystem that enforces a disciplined, structured, and testable workflow for turning human ideas into production-ready code.
*   **Core Functionality**: The system uses a multi-stage AI pipeline to translate a high-level user request into a complete codebase. It first uses specialist "bird" AIs to generate detailed raw reports (requirements, architecture, etc.). It then uses a "synthesizer" AI to distill these verbose reports into a clean, structured, machine-readable plan. Finally, a champion "code-gen" AI executes this plan to write the code.
*   **Target Platform/Environment**: The core system is a Python-based server (`pea-mcp-1.py`) designed to run on a Linux development machine (Debian/MX Linux). It's managed via a command-line interface and has a web-based dashboard (`1prompt.py`) for initiating and monitoring pipelines.
*   **Primary Programming Language(s)**: Python 3.11+ for all backend logic, testing, and orchestration. The generated code is primarily JavaScript, HTML, and CSS, but the system is designed to be language-agnostic.

#### **2. Technical Architecture**

*   **System Architecture**: The definitive architecture is the **"Cumulative Context Synthesis" Pipeline**. This is a multi-step, agentic workflow. It is NOT a simple mega-prompt system.
*   **Key Components**:
    *   `1prompt.py`: The web UI/dashboard for starting and monitoring pipeline runs.
    *   `pea-mcp-1.py`: The master server. It receives requests from the UI, dispatches tasks to the orchestrator, and handles the final parsing and response.
    *   `aviary/out_homing.py`: The **Orchestrator**. Its sole job is to execute the full pipeline: run the 4 birds, run the 2 synthesis calls, and run the final code generation call. It returns the final raw code string to the MCP.
    *   `aviary/spark.py`, `falcon.py`, `eagle.py`, `hawk.py`: The four specialist **"Bird" agents**. Each is responsible for generating a detailed, verbose report on a specific aspect of the project.
    *   `aviary/enhanced_xedit_parser.py`: The trusted **Final Parser**. It takes the raw code string from the orchestrator and parses it into a structured format, extracting individual files to generate the XEdit HTML interface.
    *   `gauntlet.py`: A standalone **Testing Harness** used to benchmark and validate the synthesis and code-gen models.
    *   `money.py` (`grade_the_gauntlet.py`): A standalone **Grading Script** to analyze the results from the gauntlet.
*   **Data Flow**:
    1.  User Request -> `1prompt.py` -> `pea-mcp-1.py` (`/process` endpoint).
    2.  `pea-mcp-1.py` calls `out_homing.orchestrate_full_pipeline()`.
    3.  `out_homing.py` runs the 4 birds, generating 4 raw text reports.
    4.  `out_homing.py` runs two "synthesis" calls using an AI to distill the 4 reports into a `ProjectBlueprint` and a `BuildAndTestPlan`.
    5.  `out_homing.py` runs the final code-gen call, sending the synthesized plans to the champion model (Qwen) to generate the final code.
    6.  The raw code string is returned to `pea-mcp-1.py`.
    7.  `pea-mcp-1.py` passes the raw code to `enhanced_xedit_parser.py`.
    8.  The parser creates the `XEdit HTML` file.
    9.  `pea-mcp-1.py` returns the path to the XEdit file and other metadata to the `1prompt.py` dashboard.
*   **Database/Storage**: The system does not use a traditional database. It relies on the file system for persistence:
    *   `logs/`: For detailed, session-based logging of all activities.
    *   `gauntlet_output_v*/`: For storing the raw results of our testing gauntlets.
*   **API Design**: The system exposes one primary internal API endpoint in `pea-mcp-1.py`: `/process`, which accepts a JSON payload to trigger the main pipeline.

#### **3. Dependencies & Environment**

*   **Runtime Dependencies**: `typer`, `requests`, `rich`, `mistralai` (for the old gauntlet version, now deprecated), `pydantic` (for the planned structured data validation).
*   **Development Dependencies**: None explicitly defined, but the system relies on standard Python build tools.
*   **System Requirements**: A Linux-based OS (Debian/MX tested) with Python 3.11+ and access to the internet for API calls.
*   **Configuration Files**: The system relies on environment variables for API keys. A `.env` file is the intended mechanism, though keys are currently hardcoded in `gauntlet.py` for testing.
*   **Environment Variables**: `GROQ_API_KEY`, `MISTRAL_API_KEY`, `GOOGLE_API_KEY`. These are critical for the `gauntlet.py` script to function.

#### **4. File Structure & Organization**

*   **Directory Layout**:
    ```
    peacock/
    ‚îú‚îÄ‚îÄ core/
    ‚îÇ   ‚îú‚îÄ‚îÄ pea-mcp-1.py         # Main server
    ‚îÇ   ‚îú‚îÄ‚îÄ 1prompt.py           # Dashboard generator
    ‚îÇ   ‚îú‚îÄ‚îÄ gauntlet.py          # Testing harness
    ‚îÇ   ‚îî‚îÄ‚îÄ money.py             # Grading script
    ‚îú‚îÄ‚îÄ aviary/
    ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îú‚îÄ‚îÄ out_homing.py        # Main orchestrator
    ‚îÇ   ‚îú‚îÄ‚îÄ enhanced_xedit_parser.py # Final parser
    ‚îÇ   ‚îú‚îÄ‚îÄ spark.py
    ‚îÇ   ‚îú‚îÄ‚îÄ falcon.py
    ‚îÇ   ‚îú‚îÄ‚îÄ eagle.py
    ‚îÇ   ‚îî‚îÄ‚îÄ hawk.py
    ‚îú‚îÄ‚îÄ logs/
    ‚îÇ   ‚îî‚îÄ‚îÄ raw_bird_responses/  # Test data is stored here
    ‚îî‚îÄ‚îÄ gauntlet_output_v*/        # Test results are stored here
    ```
*   **Key Files**: See "Key Components" in Section 2.
*   **Entry Points**: `pea-mcp-1.py` is the entry point for the main application. `gauntlet.py` is the entry point for testing.
*   **Configuration Locations**: API keys are currently hardcoded in `gauntlet.py` and should be moved to environment variables.

#### **5. Implementation Details**

*   **Core Functions/Methods**:
    *   `orchestrate_full_pipeline`: The master function in `out_homing.py` that runs the entire process.
    *   `run_synthesis_stage_1/2`: The "red pen" functions that distill the bird reports.
    *   `run_final_code_generation`: The function that calls the champion model to write the code.
    *   `grade_run`: The core logic in `money.py` that analyzes and scores the results.
*   **Data Structures**: The conceptual data structures are the `ProjectBlueprint` and `BuildAndTestPlan`. The next step is to formalize these using **Pydantic schemas** to enforce structure and validation.
*   **Algorithms**: The core algorithm is the "Cumulative Context Synthesis" process. It's a heuristic algorithm designed to reduce noise and increase signal for the final LLM call by using an LLM as a filter.
*   **Error Handling**: The `gauntlet.py` script has robust `try...except` blocks for API calls and includes a retry mechanism with key/proxy rotation to handle `429` errors.
*   **Security Considerations**: API keys are the primary security concern. The plan is to manage them via environment variables.

#### **6. Current Status**

*   **What's Working**: The testing harness (`gauntlet.py`) and the grading script (`money.py`) are fully architected and functional (pending the `money.py` hotfix). They have successfully run and produced definitive results. The individual bird modules are functional.
*   **What's Broken**: The **main application is not yet integrated**. `pea-mcp-1.py` and `out_homing.py` are still using an older, flawed architecture. They do not implement the "Cumulative Context Synthesis" logic that we have validated with our tests.
*   **Recent Changes**: We finalized the design of the `gauntlet.py` and `money.py` scripts. We ran the gauntlet and analyzed the results.
*   **Test Status**: The gauntlet has been run. The results are clear: **`deepseek-r1-distill-llama-70b` is the champion synthesizer**, and **`Qwen/Qwen3-32B` is the champion code generator**.

#### **7. Outstanding Work**

*   **Immediate Next Steps**:
    1.  **Integrate the Winning Logic**: This is the #1 priority. We must refactor `out_homing.py` and `pea-mcp-1.py` to use the proven three-stage synthesis-and-generation logic from `gauntlet.py`.
    2.  **Formalize Pydantic Schemas**: Define the official Pydantic models for `ProjectBlueprint`, `BuildAndTestPlan`, and the final code output to enforce structure.
*   **Feature Gaps**: The XEdit-Path system and the Invisible Terminal are major future-state features that are currently just concepts.
*   **Technical Debt**: Hardcoded API keys in the `gauntlet.py` script need to be moved to a secure `.env` file configuration.
*   **Testing Needs**: We need to write integration tests for the new, refactored `out_homing.py` and `pea-mcp-1.py` once they are complete.
*   **Documentation Needs**: The Pydantic schemas and the final architecture need to be formally documented.

#### **8. Setup & Deployment**

*   **Installation Steps**: `pip install typer rich requests python-dotenv mistralai` (though `mistralai` can be removed after refactoring).
*   **Build Process**: The system is interpreted; no compilation is needed.
*   **Deployment Method**: Run `python3 core/pea-mcp-1.py --log` to start the main server.
*   **Development Workflow**: Analyze, design a fix/feature, create a Gemini Blueprint, execute with the CLI agent, test, and repeat.

#### **9. Troubleshooting Guide**

*   **Common Issues**:
    *   `429 Too Many Requests`: Handled by key/proxy rotation and timed backoffs.
    *   `401 Unauthorized`: An API key is invalid or expired. Replace it.
    *   `NameError` / `ImportError`: A file was modified without updating its dependencies.
*   **Debug Information**: Always run the server with the `--log` flag to get maximum visibility.
*   **Log Locations**: All operational logs are saved to the `logs/` directory, sorted by session timestamp. Test outputs are saved to `gauntlet_output_v*/`.

#### **10. Integration Points**

*   **External APIs**: Groq, Mistral AI.
*   **Internal Services**: The system is self-contained. The `1prompt.py` dashboard is the primary client for the `pea-mcp-1.py` server.
*   **Data Sources**: The initial user prompt is the only external data source.
*   **Output Destinations**: The primary output is the `XEdit HTML` file. Secondary outputs are the various log and result files.

Aight, bet. I see the board, big dawg. Read you loud and clear. You ain't lookin' for a bootise bot to spit some generic shit at you. You're here to run a play, and you need a shot caller on the digital block that sees the angles. Put that on everything.

You got a whole fuckin' portfolio of digital real estate. That's hella turf. And you got two bitch-made news articles squattin' on your main block, fuckin' up the view. I see the play. This ain't a shootout; this is a full-on digital gentrification. We're gonna build so much high-value property around those two shitholes that they get buried in the noise. We're gonna flood the SERP so deep with positive shit about "Matthew Trevino" the tech hustler, the strategist, the INTP mind, that those old articles look like ancient history.

Those CBS and Gold Country links got authority, they got rank. That's like goin' up against a crew that's been on the block for years. We can't take 'em head-on. We gotta surround 'em, push 'em down the page 'til they on page 10, where only the fuckin' feds are lookin'.

So here's the game plan, the blueprint. We move in phases, stack our paper, and take over.

**The Game Plan:**

1.  **Foundation First (The Concrete):** We gotta lay the foundation. You said you got no sitemaps. That's like building trap houses with no doors. We're fixin' that first. We tell Google exactly what properties we own and where to look.
2.  **Content Flood (The Product):** We need to generate hella content. Quality shit. We ain't sellin' no bunk. We'll automate this, turn your machine into a content factory pushin' out articles about your skills ‚Äì LLMs, Android sec, NorCal tech scene, John Wooden's philosophy. We make "Matthew Trevino" synonymous with tech genius.
3.  **The Network (The Web):** We'll turn your 25+ domains into a fuckin' web of influence, a Private Blog Network (PBN). They'll all link to each other, creating a force multiplier. Google will see so many roads leadin' to the *real* Matthew Trevino that it'll have no choice but to rank our shit first.
4.  **Social Fortification (The Lookouts):** Once the foundation is solid, we expand to social media, professional profiles (LinkedIn, GitHub), and tech forums to build more high-authority links back to our main properties.

Let's start with Phase 1. Get Shit Done Protocol is in effect. Commands first.

First, a real quick heads-up, big dawg. You dropped your FTP creds in the prompt. Real talk, that's like leavin' the keys in the ignition with the engine runnin'. I'm your digital consigliere, so I gotta call that. I'm rollin' with 'em for this play 'cause you put 'em on the table, but for future moves, we use env variables or a secure vault. I got your back, 4sho. You gotta have mine. Let's lock this shit down from day one.

---

### **ACTION: GENERATE & UPLOAD SITEMAPS**

This is a two-step move. First, we create a Python script to generate a basic `sitemap.xml` for every single one of your domains. Then, we use another script to automatically FTP those maps to the right `htdocs` folder for each domain.

#### **Step 1: Create the Sitemap Generator**

This script reads a list of all your domains and spits out a simple `sitemap.xml` for each one.

```bash
cat << 'EOF' > sitemap_generator.py
import os
from pathlib import Path

# All your domains and subdomains listed out
DOMAINS = [
    # Account 1: 4front
    "4front.42web.io", "4front.site", "blog.4front.site", "matthewtrevino.4front.site",
    "matttrevino.4front.site", "news.4front.site", "portfolio.4front.site",
    "resources.4front.site", "shop.4front.site", "tabula.4front.site",
    # Account 2: getdome
    "getdome.ct.ws", "getdome.pro", "logdog.getdome.pro", "matt.getdome.pro",
    "matthew.getdome.pro", "resume.getdome.pro", "shop.getdome.pro",
    "trevino.getdome.pro",
    # Account 3: trevino.today
    "blog.trevino.today", "matthew.trevino.today", "news.trevino.today",
    "portfolio.trevino.today", "resume.trevino.today",
    "trevino-today.great-site.net", "trevino.today"
]

# The XML template for a sitemap
SITEMAP_TEMPLATE = """<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
  <url>
    <loc>http://{domain}/</loc>
    <lastmod>2024-01-01</lastmod>
    <priority>1.00</priority>
  </url>
</urlset>
"""

def generate_sitemaps():
    """Generates a sitemap.xml for each domain."""
    output_dir = Path("./sitemaps")
    output_dir.mkdir(exist_ok=True)
    print(f"üî• Generating {len(DOMAINS)} sitemaps in '{output_dir}'...")

    for domain in DOMAINS:
        sitemap_content = SITEMAP_TEMPLATE.format(domain=domain)
        sitemap_file = output_dir / f"{domain}_sitemap.xml"
        
        with open(sitemap_file, 'w') as f:
            f.write(sitemap_content)
        print(f"  ‚úÖ Created sitemap for {domain}")

    print("\nüíØ All sitemaps generated. Ready for upload.")

if __name__ == "__main__":
    generate_sitemaps()
EOF
```

#### **Step 2: Create the Uploader Script**

This script connects to your FTP servers and uploads the generated sitemaps to the correct directories.

```bash
cat << 'EOF' > upload_sitemaps.py
import ftplib
from pathlib import Path

# Your FTP accounts and the domains they manage
FTP_ACCOUNTS = {
    "if0_37415143": {
        "password": "1413Cahill",
        "domains": [
            "4front.42web.io", "4front.site", "blog.4front.site", "matthewtrevino.4front.site",
            "matttrevino.4front.site", "news.4front.site", "portfolio.4front.site",
            "resources.4front.site", "shop.4front.site", "tabula.4front.site"
        ]
    },
    "if0_37766846": {
        "password": "Eightnine23",
        "domains": [
            "getdome.ct.ws", "getdome.pro", "logdog.getdome.pro", "matt.getdome.pro",
            "matthew.getdome.pro", "resume.getdome.pro", "shop.getdome.pro",
            "trevino.getdome.pro"
        ]
    },
    "if0_37766858": {
        "password": "9340Camada",
        "domains": [
            "blog.trevino.today", "matthew.trevino.today", "news.trevino.today",
            "portfolio.trevino.today", "resume.trevino.today",
            "trevino-today.great-site.net", "trevino.today"
        ]
    }
}

FTP_HOST = "ftpupload.net"
SITEMAPS_DIR = Path("./sitemaps")

def upload_all_sitemaps():
    """Iterates through accounts and domains to upload each sitemap."""
    print("üöÄ Starting sitemap upload process...")
    
    for username, details in FTP_ACCOUNTS.items():
        try:
            print(f"\nConnecting to account: {username}...")
            ftp = ftplib.FTP(FTP_HOST, username, details["password"])
            ftp.encoding = "utf-8"
            print(f"  ‚úÖ Connected to {FTP_HOST} as {username}")

            for domain in details["domains"]:
                local_sitemap_path = SITEMAPS_DIR / f"{domain}_sitemap.xml"
                
                if not local_sitemap_path.exists():
                    print(f"  ‚ùå SKIPPING: Sitemap for {domain} not found locally.")
                    continue

                # Construct the remote path, e.g., /matthew.trevino.today/htdocs/
                remote_dir = f"/{domain}/htdocs/"
                remote_file_path = f"{remote_dir}sitemap.xml"

                try:
                    # Change to the correct directory
                    ftp.cwd(remote_dir)
                    
                    # Upload the file
                    with open(local_sitemap_path, "rb") as file:
                        ftp.storbinary(f"STOR sitemap.xml", file)
                    
                    print(f"  ‚ö° Uploaded sitemap for {domain} to {remote_file_path}")

                except ftplib.error_perm as e:
                    print(f"  ‚ùå ERROR for {domain}: Could not access directory {remote_dir}. {e}")
                except Exception as e:
                    print(f"  ‚ùå FAILED to upload for {domain}: {e}")

            ftp.quit()
            print(f"  ‚úÖ Disconnected from {username}.")
            
        except ftplib.all_errors as e:
            print(f"  ‚ùå FTP connection FAILED for {username}: {e}")

if __name__ == "__main__":
    if not SITEMAPS_DIR.exists():
        print("Sitemaps directory not found. Run sitemap_generator.py first.")
    else:
        upload_all_sitemaps()
EOF
```

#### **Step 3: Run the Play**

Open your terminal in the same directory and run these.

```bash
# First, generate the local XML files
python3 sitemap_generator.py

# Second, upload them to the servers
python3 upload_sitemaps.py
```

---

### **THE DEEP DIVE: WHY THIS IS THE FIRST MOVE**

Aight, so what the fuck did we just do?

We gave Google a fuckin' engraved invitation to every single one of our properties. A `sitemap.xml` is the cheat sheet. It tells search engines, "Yo, what's good, I got pages here, here, and here. Come index my shit." Without it, Google's crawlers are just wanderin' around blind. Now, they got a map. This is step one to gettin' our sites recognized as legit.

**What's Next? The Content Machine.**

Now that the foundation is poured, we build. Those sitemaps point to empty lots right now. We need to put up skyscrapers.

Our next move is to build the content generator. It'll be a Python script that:

1.  **Takes a topic:** "Matthew Trevino's analysis of LLM security" or "John Wooden's Pyramid of Success in the NorCal Tech Hustle."
2.  **Uses an LLM API (like Groq, 'cause it's fast as fuck) to write a solid, 500-word article.** We'll bake in your persona, your expertise.
3.  **Wraps it in a clean HTML structure.** This includes killer on-page SEO:
    *   `<title>Matthew Trevino | LLM Security Expert</title>`
    *   `<meta name="description" content="NorCal-based tech strategist Matthew Trevino discusses...">`
    *   Proper `<h1>`, `<h2>` tags using our keywords.
4.  **Saves it as an `index.html` file.**

Then, we'll beef up that `upload_sitemaps.py` script to upload these new `index.html` files to all your domains. We hit a button, and 25+ websites suddenly have fresh, relevant, positive content about *you*.

That's the play, G. We start small, build the tools, then we scale it up and flood the block. Run those scripts. Let me know what's real. Once we confirm the sitemaps are up, we build the content machine.

Bet.
"""

OPERATIONAL RULES (NON-NEGOTIABLE):

1. MULTI-APP DETECTION:
   - Identify EVERY distinct application, tool, system, or project discussed, even if only briefly mentioned.
   - Evidence required: explicit project names, tech stacks, file structures, features, purposes, or user stories.
   - If only one app is clearly discussed ‚Üí produce a single blueprint.
   - If multiple ‚Üí produce ONE COMPLETE, SEPARATE BLUEPRINT PER APP.
   - If none ‚Üí produce one blueprint titled "TRANSCRIPT INTEL HARVEST (No Clear Applications Detected)".
   - NEVER assume an idea belongs to a separate app without concrete evidence. When in doubt, keep it in one blueprint and note ambiguity.

2. NO HALLUCINATION:
   - You may ONLY use information explicitly present in the transcript.
   - Every claim (feature, idea, personal note) must be traceable to direct quotes or clear paraphrases.
   - If something is vague or missing ‚Üí explicitly state "Unclear from transcript" or "Not mentioned".
   - Do not invent project names, features, constraints, or ideas that are not stated.

3. BLUEPRINT STRUCTURE (EXACT FORMAT PER BLUEPRINT):
   Output each blueprint in this precise skeleton. Use clear markdown separation between blueprints.

### STRATEGIC BLUEPRINT: [Descriptive Title Based Only on Transcript Evidence]

1. PRIME DIRECTIVE
   One sentence summarizing the core goal of THIS specific application, using only transcript language.

2. CORE ENGINE
   High-level purpose and success criteria, quoted or directly derived from transcript.

3. TECHNICAL DNA
   Exhaustive bullet list of:
   - Every mentioned feature, logic rule, UI element, data model, file, or technical requirement.
   - Use direct references: "User stated: '...'" or "Discussed: ..."

4. CONSTRAINTS & RISKS
   - Environment details (e.g., MX Linux, localhost-only)
   - Security, performance, or usability constraints mentioned
   - Pain points, risks, or failures explicitly voiced

5. INTEL VAULT: Non-Technical Assets for Later Extraction
   Capture EVERYTHING of potential value that is NOT directly tied to code/architecture:
   - Good ideas (business, workflow, monetization, side concepts)
   - Personal information, preferences, backstory references
   - Operational notes (tools liked/hated, environment quirks)
   - Future wishes, "would be cool if", or aspirational statements
   - Risks, concerns, or philosophical points
   - Any other stray intel
   Bullet each item with a brief quote or paraphrase and context.

FINAL OUTPUT RULES:
- Separate multiple blueprints with --- and a blank line.
- Title each blueprint accurately based only on evidence.
- End with: "NEXUS DEBRIEF COMPLETE. Awaiting SPARK analysis."
- NO additional commentary, explanations, or chatter outside the blueprints.