# HTML File Name and Content Extraction
**Date:** 2025-06-16 21:33:45
**UUID:** 9eb09b54-0764-4627-af6f-e042bc318b4b

============================================================
[USER ENTRY #001] - 2025-06-16 21:33:46
============================================================

okay i got 21 html files that look like this   i want to extract the name the first one is like this name stage3-eagle-implementation.md  and i want to extract the content   first one is liek this ""    and i want to save it with that same name except in .txt format...  they are all html files in this dir./  /home/flintx/basic-memory/peacock-nots

------------------------------------------------------------
[CLAUDE RESPONSE #001] - 2025-06-16 21:34:46
------------------------------------------------------------

 
```
This block is not supported on your current device yet.
```

Yo big dawg, I see what you need. You got 21 HTML files in that peacock-nots directory and you wanna extract the document name and content, then save 'em as .txt files. Based on the HTML structure I see in your examples, I got the perfect script to drop dimes on this whole operation.

Let me build you somethin' that'll handle this extraction like a pro:
```
Viewing artifacts created via the Analysis Tool web feature preview isn’t yet supported on mobile.
```

Now here's the commands to drop this script and run it, big dawg:

```bash
# Save the script
cat << 'EOF' > /home/flintx/html_extractor.py
#!/usr/bin/env python3
"""
HTML Document Extractor - Extract filename and content from HTML files
Converts HTML files with embedded document content to clean TXT files
"""

import os
import sys
import re
from pathlib import Path
from bs4 import BeautifulSoup

def extract_document_info(html_file_path):
    """
    Extract document name and content from HTML file
    Returns tuple: (document_name, document_content)
    """
    try:
        with open(html_file_path, 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Extract document name from the dialog header
        # Looking for pattern like: <h2 id="...">stage3-eagle-implementation.md</h2>
        document_name = None
        h2_element = soup.find('h2', {'id': re.compile(r'radix-.*')})
        if h2_element:
            document_name = h2_element.get_text().strip()
        
        # Fallback: try to find in dialog div structure
        if not document_name:
            dialog_div = soup.find('div', {'role': 'dialog'})
            if dialog_div:
                h2_in_dialog = dialog_div.find('h2')
                if h2_in_dialog:
                    document_name = h2_in_dialog.get_text().strip()
        
        # Extract content from the readability page
        # Looking for the main content in div with id="readability-page-1"
        content = None
        readability_div = soup.find('div', {'id': 'readability-page-1'})
        if readability_div:
            # Get all text content, preserving structure
            content = readability_div.get_text(separator='\n', strip=True)
            
            # Clean up the content
            lines = content.split('\n')
            cleaned_lines = []
            
            for line in lines:
                line = line.strip()
                if line and not line.startswith('•') and 'KB' not in line and 'lines' not in line:
                    # Skip metadata lines like "44.04 KB • 1,155 lines"
                    if not re.match(r'^\d+\.\d+\s*KB.*\d+\s*lines', line):
                        cleaned_lines.append(line)
            
            content = '\n'.join(cleaned_lines)
        
        # Fallback: get all text content if readability div not found
        if not content:
            # Remove script and style elements
            for script in soup(["script", "style"]):
                script.decompose()
            content = soup.get_text(separator='\n', strip=True)
        
        return document_name, content
        
    except Exception as e:
        print(f"Error processing {html_file_path}: {str(e)}")
        return None, None

def clean_filename(filename):
    """
    Clean filename to be filesystem-safe
    """
    if not filename:
        return "unknown_document"
    
    # Remove file extension if present
    name = os.path.splitext(filename)[0]
    
    # Replace problematic characters
    name = re.sub(r'[<>:"/\\|?*]', '_', name)
    name = re.sub(r'\s+', '_', name)
    
    return name

def process_html_files(input_dir, output_dir=None):
    """
    Process all HTML files in the input directory
    """
    input_path = Path(input_dir)
    
    if not input_path.exists():
        print(f"Error: Directory {input_dir} does not exist")
        return
    
    # Set output directory (same as input if not specified)
    if output_dir:
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
    else:
        output_path = input_path
    
    # Find all HTML files
    html_files = list(input_path.glob('*.html'))
    
    if not html_files:
        print(f"No HTML files found in {input_dir}")
        return
    
    print(f"Found {len(html_files)} HTML files to process...")
    
    processed_count = 0
    failed_count = 0
    
    for html_file in html_files:
        print(f"\nProcessing: {html_file.name}")
        
        document_name, content = extract_document_info(html_file)
        
        if document_name and content:
            # Create output filename
            clean_name = clean_filename(document_name)
            output_filename = f"{clean_name}.txt"
            output_file_path = output_path / output_filename
            
            # Write content to TXT file
            try:
                with open(output_file_path, 'w', encoding='utf-8') as f:
                    f.write(f"# {document_name}\n\n")
                    f.write(content)
                
                print(f"✓ Extracted: {document_name}")
                print(f"  → Saved as: {output_filename}")
                processed_count += 1
                
            except Exception as e:
                print(f"✗ Failed to write {output_filename}: {str(e)}")
                failed_count += 1
        else:
            print(f"✗ Failed to extract content from {html_file.name}")
            failed_count += 1
    
    print(f"\n{'='*50}")
    print(f"Processing complete!")
    print(f"Successfully processed: {processed_count} files")
    print(f"Failed: {failed_count} files")
    print(f"Output directory: {output_path}")

def main():
    """
    Main function - handle command line arguments
    """
    if len(sys.argv) < 2:
        print("Usage: python html_extractor.py <input_directory> [output_directory]")
        print("Example: python html_extractor.py /home/flintx/basic-memory/peacock-nots")
        sys.exit(1)
    
    input_dir = sys.argv[1]
    output_dir = sys.argv[2] if len(sys.argv) > 2 else None
    
    process_html_files(input_dir, output_dir)

if __name__ == "__main__":
    main()
EOF

# Make it executable
chmod +x /home/flintx/html_extractor.py

# Install required dependency if needed
pip install beautifulsoup4

# Run the extraction on your peacock-nots directory
python3 /home/flintx/html_extractor.py /home/flintx/basic-memory/peacock-nots
```

This script is gonna handle all that shit for you, my boy. Here's what it does:

**The Real Deal Breakdown:**
- Scans your peacock-nots directory for all HTML files
- Extracts the document name from the `<h2>` tags (like "stage3-eagle-implementation.md")
- Pulls the actual content from the readability div, filtering out metadata bullshit
- Cleans up filenames to be filesystem-safe
- Saves each document as a .txt file with the original name

**What's Real:**
- Handles 21 files in one shot - no manual bullshit
- Preserves document structure and formatting
- Filters out HTML cruft and metadata lines
- Creates clean, readable .txt files
- Gives you progress feedback so you know what's poppin'

**If You Need Different Output Location:**
```bash
# Save to a different directory
python3 /home/flintx/html_extractor.py /home/flintx/basic-memory/peacock-nots /home/flintx/extracted_docs
```

This script's built on that same foundation as your INTP logic - it breaks down the problem systematically, handles edge cases, and delivers exactly what you need. No bitch-made shortcuts, just solid extraction that gets the job done proper.

Run that command block and watch it transform all 21 HTML files into clean .txt documents, 4sho!

