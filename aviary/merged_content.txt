╭━─━─━─≪✠≫─━─━─━╮
--- File: eagle.py ---
╰━─━─━─≪✠≫─━─━─━╯

#!/usr/bin/env python3
"""
eagle.py - EAGLE Code Implementation Bird
The coding beast who transforms architecture into working code
"""

import json
import re
from typing import Dict, List, Any

class EagleImplementer:
    """EAGLE - The Code Generation Beast"""
    
    def __init__(self):
        self.stage_name = "EAGLE"
        self.icon = "🦅"
        self.specialty = "Code Implementation & Generation"
        self.optimal_model = "llama-3.1-8b-instant"  # Code generation beast
    
    def implement_code(self, falcon_architecture: Dict[str, Any]) -> Dict[str, Any]:
        """
        Main EAGLE function - generate working code based on FALCON architecture
        """
        print(f"🦅 EAGLE IMPLEMENTER: Generating working code...")
        
        # Extract architecture data
        architecture_text = falcon_architecture.get("raw_design", "")
        architecture_data = falcon_architecture.get("json_data", {})
        
        # Generate the EAGLE implementation prompt
        eagle_prompt = self._build_eagle_prompt(architecture_text, architecture_data)
        
        # Package the implementation for MCP processing
        eagle_implementation = {
            "stage": "EAGLE",
            "prompt": eagle_prompt,
            "falcon_input": falcon_architecture,
            "model": self.optimal_model,
            "temperature": 0.2,  # Lower for more consistent code
            "max_tokens": 2048,  # More tokens for code generation
            "implementation_type": "code_generation"
        }
        
        return eagle_implementation
    
    def _build_eagle_prompt(self, architecture_design: str, architecture_data: Dict[str, Any]) -> str:
        """Build the code implementation prompt"""
        
        prompt = f"""<thinking>
I need to implement the architecture designed by Falcon. Based on the technical design, I should create working, complete code files that match the specified structure and requirements.

Architecture: {architecture_design}

I need to generate:
- Complete, functional code files
- Proper file structure
- Working implementations of all components
- Clean, well-commented code
</thinking>

Act as Eagle, a senior developer. Implement the complete working code for this architecture.

Architecture Design:
{architecture_design}

Generate COMPLETE, WORKING code using this EXACT format:

**IMPLEMENTATION OVERVIEW:**
[Brief description of what you're building and key implementation decisions]

**CODE FILES:**

```filename: index.html
<!DOCTYPE html>
<html lang="en">
[Complete HTML code here]
</html>
```

```filename: styles.css
/* Complete CSS code here */
[All CSS styles]
```

```filename: script.js
// Complete JavaScript code here
[All JavaScript functionality]
```

**IMPLEMENTATION NOTES:**
- [Key implementation decision 1]
- [Key implementation decision 2]
- [How components interact]

**TESTING INSTRUCTIONS:**
1. [Step 1 to test the implementation]
2. [Step 2 to test the implementation]
3. [Step 3 to test the implementation]

Then provide structured data as JSON:
```json
{{
    "files_generated": ["list of filenames"],
    "technologies_used": ["list"],
    "implementation_complexity": "simple|moderate|complex",
    "estimated_lines_of_code": 100,
    "confidence_score": 9
}}
```

Generate COMPLETE, FUNCTIONAL code that works out of the box. No placeholders, no TODO comments."""
        
        return prompt
    
    def validate_eagle_response(self, response_text: str) -> Dict[str, Any]:
        """Validate that EAGLE response contains working code"""
        
        validation_result = {
            "valid": False,
            "has_overview": False,
            "has_code_files": False,
            "has_implementation_notes": False,
            "has_json": False,
            "file_count": 0,
            "character_count": len(response_text),
            "quality_score": 0
        }
        
        # Check for implementation overview
        if "IMPLEMENTATION OVERVIEW:" in response_text:
            validation_result["has_overview"] = True
            validation_result["quality_score"] += 1
        
        # Check for code files
        code_files = re.findall(r'```filename:\s*([^\n]+)\n(.*?)\n```', response_text, re.DOTALL)
        if code_files:
            validation_result["has_code_files"] = True
            validation_result["file_count"] = len(code_files)
            validation_result["quality_score"] += min(len(code_files), 3)  # Max 3 points for files
        
        # Check for implementation notes
        if "IMPLEMENTATION NOTES:" in response_text:
            validation_result["has_implementation_notes"] = True
            validation_result["quality_score"] += 1
        
        # Check for JSON data
        json_pattern = r'```json\s*\n(.*?)\n```'
        json_match = re.search(json_pattern, response_text, re.DOTALL)
        if json_match:
            try:
                json.loads(json_match.group(1))
                validation_result["has_json"] = True
                validation_result["quality_score"] += 2
            except json.JSONDecodeError:
                pass
        
        # Determine if valid
        validation_result["valid"] = (
            validation_result["has_code_files"] and 
            validation_result["file_count"] >= 1 and
            validation_result["character_count"] > 500
        )
        
        return validation_result
    
    def extract_code_files(self, response_text: str) -> List[Dict[str, Any]]:
        """Extract all code files from EAGLE response"""
        
        code_files = []
        
        # Pattern for filename-based code blocks
        filename_pattern = r'```filename:\s*([^\n]+)\n(.*?)\n```'
        filename_matches = re.findall(filename_pattern, response_text, re.DOTALL)
        
        for filename, code in filename_matches:
            file_data = {
                "filename": filename.strip(),
                "code": code.strip(),
                "language": self._detect_language(filename.strip()),
                "size": len(code.strip()),
                "lines": len(code.strip().split('\n'))
            }
            code_files.append(file_data)
        
        return code_files
    
    def extract_implementation_data(self, response_text: str) -> Dict[str, Any]:
        """Extract structured implementation data from EAGLE response"""
        
        implementation = {
            "overview": "",
            "code_files": [],
            "implementation_notes": [],
            "testing_instructions": [],
            "json_data": {},
            "raw_implementation": response_text
        }
        
        # Extract implementation overview
        overview_match = re.search(r'\*\*IMPLEMENTATION OVERVIEW:\*\*\s*\n([^\n*]+(?:\n[^\n*]+)*)', response_text)
        if overview_match:
            implementation["overview"] = overview_match.group(1).strip()
        
        # Extract code files
        implementation["code_files"] = self.extract_code_files(response_text)
        
        # Extract implementation notes
        notes_section = re.search(r'\*\*IMPLEMENTATION NOTES:\*\*\s*\n((?:- [^\n]+\n?)+)', response_text)
        if notes_section:
            notes = re.findall(r'- ([^\n]+)', notes_section.group(1))
            implementation["implementation_notes"] = [note.strip() for note in notes]
        
        # Extract testing instructions
        test_section = re.search(r'\*\*TESTING INSTRUCTIONS:\*\*\s*\n((?:\d+\. [^\n]+\n?)+)', response_text)
        if test_section:
            instructions = re.findall(r'\d+\. ([^\n]+)', test_section.group(1))
            implementation["testing_instructions"] = [instruction.strip() for instruction in instructions]
        
        # Extract JSON data
        json_pattern = r'```json\s*\n(.*?)\n```'
        json_match = re.search(json_pattern, response_text, re.DOTALL)
        if json_match:
            try:
                implementation["json_data"] = json.loads(json_match.group(1))
            except json.JSONDecodeError:
                implementation["json_data"] = {}
        
        return implementation
    
    def _detect_language(self, filename: str) -> str:
        """Detect programming language from filename"""
        ext_map = {
            '.html': 'html',
            '.css': 'css', 
            '.js': 'javascript',
            '.py': 'python',
            '.java': 'java',
            '.cpp': 'cpp',
            '.c': 'c',
            '.php': 'php',
            '.rb': 'ruby',
            '.go': 'go',
            '.rs': 'rust',
            '.ts': 'typescript',
            '.jsx': 'jsx',
            '.tsx': 'tsx'
        }
        
        for ext, lang in ext_map.items():
            if filename.lower().endswith(ext):
                return lang
        
        return 'text'
    
    def generate_project_structure(self, code_files: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Generate project structure and file organization"""
        
        structure = {
            "total_files": len(code_files),
            "total_lines": sum(file_data["lines"] for file_data in code_files),
            "total_size": sum(file_data["size"] for file_data in code_files),
            "languages": list(set(file_data["language"] for file_data in code_files)),
            "file_breakdown": {}
        }
        
        # Categorize files by type
        for file_data in code_files:
            lang = file_data["language"]
            if lang not in structure["file_breakdown"]:
                structure["file_breakdown"][lang] = {
                    "count": 0,
                    "total_lines": 0,
                    "files": []
                }
            
            structure["file_breakdown"][lang]["count"] += 1
            structure["file_breakdown"][lang]["total_lines"] += file_data["lines"]
            structure["file_breakdown"][lang]["files"].append(file_data["filename"])
        
        return structure
    
    def optimize_code_structure(self, implementation_data: Dict[str, Any]) -> Dict[str, Any]:
        """Optimize code structure for better organization"""
        
        optimization_suggestions = {
            "structure_improvements": [],
            "performance_tips": [],
            "maintainability_suggestions": [],
            "scalability_considerations": []
        }
        
        code_files = implementation_data.get("code_files", [])
        
        # Analyze structure
        if len(code_files) > 5:
            optimization_suggestions["structure_improvements"].append(
                "Consider organizing files into folders (src/, assets/, components/)"
            )
        
        # Check for large files
        for file_data in code_files:
            if file_data["lines"] > 200:
                optimization_suggestions["maintainability_suggestions"].append(
                    f"Consider breaking down {file_data['filename']} - {file_data['lines']} lines is quite large"
                )
        
        # Performance suggestions based on file types
        languages = [file_data["language"] for file_data in code_files]
        if "javascript" in languages:
            optimization_suggestions["performance_tips"].extend([
                "Consider code splitting for large JavaScript files",
                "Implement lazy loading for better performance",
                "Minify JavaScript for production"
            ])
        
        if "css" in languages:
            optimization_suggestions["performance_tips"].extend([
                "Consider CSS minification and compression",
                "Use CSS custom properties for better maintainability"
            ])
        
        # Scalability considerations
        if len(code_files) >= 3:
            optimization_suggestions["scalability_considerations"].extend([
                "Consider implementing a build system (webpack, vite, etc.)",
                "Set up testing framework for future development",
                "Consider version control and deployment strategy"
            ])
        
        return optimization_suggestions

# Factory function for EAGLE bird
def create_eagle_implementer() -> EagleImplementer:
    """Factory function to create EAGLE implementer instance"""
    return EagleImplementer()

# Test function for EAGLE bird
def test_eagle_bird():
    """Test the EAGLE bird with sample FALCON input"""
    eagle = create_eagle_implementer()
    
    # Mock FALCON architecture
    falcon_architecture = {
        "raw_design": """
TECHNOLOGY STACK:
- Frontend: HTML, CSS, JavaScript
- Backend: None (client-side only)
- Database: LocalStorage

CORE COMPONENTS:
1. Game Engine - Handles snake movement and collision detection
2. Renderer - Draws game elements on canvas
3. Input Handler - Processes user keyboard input

FILE STRUCTURE:
```
snake_game/
├── index.html
├── styles.css
└── script.js
```
        """,
        "json_data": {
            "tech_stack": {
                "frontend": "HTML, CSS, JavaScript",
                "backend": "None",
                "database": "LocalStorage"
            },
            "complexity": "simple"
        }
    }
    
    implementation = eagle.implement_code(falcon_architecture)
    
    print("🧪 TESTING EAGLE BIRD")
    print(f"🦅 Stage: {implementation['stage']}")
    print(f"🤖 Model: {implementation['model']}")
    print(f"💻 Implementation Type: {implementation['implementation_type']}")
    print(f"📏 Prompt Length: {len(implementation['prompt'])} characters")
    print(f"🔥 Temperature: {implementation['temperature']}")
    print(f"📊 Max Tokens: {implementation['max_tokens']}")
    
    return implementation

if __name__ == "__main__":
    # Test EAGLE bird independently
    test_eagle_bird()

┎━─━─━─━─━─━─━─━─━┒
--- File: falcon.py ---
┖━─━─━─━─━─━─━─━─━┚

#!/usr/bin/env python3
"""
falcon.py - FALCON Architecture Design Bird
The senior architect who designs technical systems and component structures
"""

import json
import re
from typing import Dict, List, Any

class FalconArchitect:
    """FALCON - The System Architect"""
    
    def __init__(self):
        self.stage_name = "FALCON"
        self.icon = "🦅"
        self.specialty = "Technical Architecture Design"
        self.optimal_model = "gemma2-9b-it"  # Structure champion
    
    def design_architecture(self, spark_requirements: Dict[str, Any]) -> Dict[str, Any]:
        """
        Main FALCON function - design technical architecture based on SPARK requirements
        """
        print(f"🦅 FALCON ARCHITECT: Designing system architecture...")
        
        # Extract key data from SPARK analysis
        spark_text = spark_requirements.get("raw_analysis", "")
        requirements_data = spark_requirements.get("json_data", {})
        
        # Generate the FALCON architecture prompt
        falcon_prompt = self._build_falcon_prompt(spark_text, requirements_data)
        
        # Package the architecture design for MCP processing
        falcon_design = {
            "stage": "FALCON",
            "prompt": falcon_prompt,
            "spark_input": spark_requirements,
            "model": self.optimal_model,
            "temperature": 0.3,
            "max_tokens": 1024,
            "design_type": "technical_architecture"
        }
        
        return falcon_design
    
    def _build_falcon_prompt(self, spark_analysis: str, requirements_data: Dict[str, Any]) -> str:
        """Build the technical architecture design prompt"""
        
        prompt = f"""<thinking>
Based on the requirements from Spark, I need to design a technical architecture.

Requirements: {spark_analysis}

I should think about:
- What technologies would work best
- How to structure the codebase
- What components are needed
- How they interact
</thinking>

Act as Falcon, a senior software architect. Design the technical architecture for this project.

Requirements Analysis:
{spark_analysis}

Provide architecture design in this EXACT format:

**TECHNOLOGY STACK:**
- Frontend: [Technology choices]
- Backend: [Technology choices]  
- Database: [Technology choices]
- Additional: [Other technologies]

**CORE COMPONENTS:**
1. [Component Name] - [Purpose and functionality]
2. [Component Name] - [Purpose and functionality]
3. [Component Name] - [Purpose and functionality]

**FILE STRUCTURE:**
```
project_root/
├── [folder1]/
│   ├── [file1.ext]
│   └── [file2.ext]
├── [folder2]/
└── [file3.ext]
```

**COMPONENT INTERACTIONS:**
[Describe how components communicate and data flows]

Then provide the structured data as JSON:
```json
{{
    "tech_stack": {{
        "frontend": "string",
        "backend": "string",
        "database": "string"
    }},
    "components": ["list"],
    "complexity": "simple|moderate|complex",
    "confidence_score": 8
}}
```

Focus on practical, implementable architecture."""
        
        return prompt
    
    def validate_falcon_response(self, response_text: str) -> Dict[str, Any]:
        """Validate that FALCON response contains required architecture elements"""
        
        validation_result = {
            "valid": False,
            "has_tech_stack": False,
            "has_components": False,
            "has_file_structure": False,
            "has_json": False,
            "character_count": len(response_text),
            "quality_score": 0
        }
        
        # Check for technology stack
        if "TECHNOLOGY STACK:" in response_text:
            validation_result["has_tech_stack"] = True
            validation_result["quality_score"] += 2
        
        # Check for core components
        if "CORE COMPONENTS:" in response_text:
            validation_result["has_components"] = True
            validation_result["quality_score"] += 2
        
        # Check for file structure
        if "FILE STRUCTURE:" in response_text and "project_root/" in response_text:
            validation_result["has_file_structure"] = True
            validation_result["quality_score"] += 2
        
        # Check for JSON data
        json_pattern = r'```json\s*\n(.*?)\n```'
        json_match = re.search(json_pattern, response_text, re.DOTALL)
        if json_match:
            try:
                json.loads(json_match.group(1))
                validation_result["has_json"] = True
                validation_result["quality_score"] += 3
            except json.JSONDecodeError:
                pass
        
        # Determine if valid
        validation_result["valid"] = (
            validation_result["has_tech_stack"] and 
            validation_result["has_components"] and
            validation_result["character_count"] > 300
        )
        
        return validation_result
    
    def extract_architecture_data(self, response_text: str) -> Dict[str, Any]:
        """Extract structured architecture data from FALCON response"""
        
        architecture = {
            "tech_stack": {},
            "components": [],
            "file_structure": "",
            "component_interactions": "",
            "json_data": {},
            "raw_design": response_text
        }
        
        # Extract technology stack
        tech_section = re.search(r'\*\*TECHNOLOGY STACK:\*\*\s*\n((?:- [^\n]+\n?)+)', response_text)
        if tech_section:
            tech_items = re.findall(r'- ([^:]+): ([^\n]+)', tech_section.group(1))
            for category, tech in tech_items:
                architecture["tech_stack"][category.strip().lower()] = tech.strip()
        
        # Extract core components
        comp_section = re.search(r'\*\*CORE COMPONENTS:\*\*\s*\n((?:\d+\. [^\n]+\n?)+)', response_text)
        if comp_section:
            components = re.findall(r'\d+\. ([^-]+) - ([^\n]+)', comp_section.group(1))
            for name, purpose in components:
                architecture["components"].append({
                    "name": name.strip(),
                    "purpose": purpose.strip()
                })
        
        # Extract file structure
        file_match = re.search(r'\*\*FILE STRUCTURE:\*\*\s*\n```\s*\n(.*?)\n```', response_text, re.DOTALL)
        if file_match:
            architecture["file_structure"] = file_match.group(1).strip()
        
        # Extract component interactions
        interact_match = re.search(r'\*\*COMPONENT INTERACTIONS:\*\*\s*\n([^\n*]+(?:\n[^\n*]+)*)', response_text)
        if interact_match:
            architecture["component_interactions"] = interact_match.group(1).strip()
        
        # Extract JSON data
        json_pattern = r'```json\s*\n(.*?)\n```'
        json_match = re.search(json_pattern, response_text, re.DOTALL)
        if json_match:
            try:
                architecture["json_data"] = json.loads(json_match.group(1))
            except json.JSONDecodeError:
                architecture["json_data"] = {}
        
        return architecture
    
    def generate_component_specs(self, architecture_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Generate detailed specifications for each component"""
        
        component_specs = []
        
        for component in architecture_data.get("components", []):
            spec = {
                "name": component["name"],
                "purpose": component["purpose"],
                "technologies": self._suggest_technologies_for_component(component["name"]),
                "interfaces": self._define_component_interfaces(component["name"]),
                "dependencies": self._identify_dependencies(component["name"], architecture_data)
            }
            component_specs.append(spec)
        
        return component_specs
    
    def _suggest_technologies_for_component(self, component_name: str) -> List[str]:
        """Suggest appropriate technologies for a component"""
        name_lower = component_name.lower()
        
        if any(term in name_lower for term in ['ui', 'interface', 'frontend', 'view']):
            return ['HTML', 'CSS', 'JavaScript']
        elif any(term in name_lower for term in ['api', 'server', 'backend', 'service']):
            return ['Python', 'Node.js', 'Express']
        elif any(term in name_lower for term in ['database', 'storage', 'data']):
            return ['SQLite', 'PostgreSQL', 'MongoDB']
        elif any(term in name_lower for term in ['auth', 'security', 'login']):
            return ['JWT', 'OAuth', 'bcrypt']
        else:
            return ['JavaScript', 'Python']
    
    def _define_component_interfaces(self, component_name: str) -> Dict[str, List[str]]:
        """Define interfaces for component communication"""
        return {
            "inputs": ["data", "user_actions", "events"],
            "outputs": ["responses", "updates", "notifications"],
            "methods": ["initialize", "process", "validate", "cleanup"]
        }
    
    def _identify_dependencies(self, component_name: str, architecture_data: Dict[str, Any]) -> List[str]:
        """Identify dependencies between components"""
        all_components = [comp["name"] for comp in architecture_data.get("components", [])]
        # Simple dependency logic - can be enhanced
        return [comp for comp in all_components if comp != component_name]

# Factory function for FALCON bird
def create_falcon_architect() -> FalconArchitect:
    """Factory function to create FALCON architect instance"""
    return FalconArchitect()

# Test function for FALCON bird
def test_falcon_bird():
    """Test the FALCON bird with sample SPARK input"""
    falcon = create_falcon_architect()
    
    # Mock SPARK requirements
    spark_requirements = {
        "raw_analysis": "Build a snake game with HTML, CSS, and JavaScript",
        "json_data": {
            "core_objective": "Create an interactive snake game",
            "in_scope": ["Game mechanics", "Score tracking", "Visual interface"],
            "complexity": "simple"
        }
    }
    
    design = falcon.design_architecture(spark_requirements)
    
    print("🧪 TESTING FALCON BIRD")
    print(f"🦅 Stage: {design['stage']}")
    print(f"🤖 Model: {design['model']}")
    print(f"🏗️ Design Type: {design['design_type']}")
    print(f"📏 Prompt Length: {len(design['prompt'])} characters")
    
    return design

if __name__ == "__main__":
    # Test FALCON bird independently
    test_falcon_bird()

┍──━──━──┙◆┕──━──━──┑
--- File: hawk.py ---
┕──━──━──┑◆┍──━──━──┙

#!/usr/bin/env python3
"""
hawk.py - HAWK Quality Assurance Bird
The QA specialist who ensures code quality and creates comprehensive testing strategies
"""

import json
import re
from typing import Dict, List, Any

class HawkQASpecialist:
    """HAWK - The Quality Assurance Master"""
    
    def __init__(self):
        self.stage_name = "HAWK"
        self.icon = "🦅"
        self.specialty = "Quality Assurance & Testing Strategy"
        self.optimal_model = "gemma2-9b-it"  # QA structure specialist
    
    def analyze_implementation(self, eagle_implementation: Dict[str, Any]) -> Dict[str, Any]:
        """
        Main HAWK function - analyze code quality and create testing strategy
        """
        print(f"🦅 HAWK QA SPECIALIST: Analyzing code quality and creating test strategy...")
        
        # Extract implementation data
        implementation_text = eagle_implementation.get("raw_implementation", "")
        code_files = eagle_implementation.get("code_files", [])
        
        # Generate the HAWK QA prompt
        hawk_prompt = self._build_hawk_prompt(implementation_text, code_files)
        
        # Package the QA analysis for MCP processing
        hawk_analysis = {
            "stage": "HAWK",
            "prompt": hawk_prompt,
            "eagle_input": eagle_implementation,
            "model": self.optimal_model,
            "temperature": 0.3,
            "max_tokens": 1024,
            "analysis_type": "quality_assurance"
        }
        
        return hawk_analysis
    
    def _build_hawk_prompt(self, implementation_text: str, code_files: List[Dict[str, Any]]) -> str:
        """Build the QA analysis and testing strategy prompt"""
        
        files_summary = self._generate_files_summary(code_files)
        
        prompt = f"""<thinking>
I need to analyze the implementation from Eagle and create a comprehensive QA strategy. I should look at:
- Code quality and best practices
- Security considerations
- Performance implications
- Testing requirements
- Deployment readiness

Implementation: {implementation_text[:500]}...
Files: {files_summary}
</thinking>

Act as Hawk, a senior QA engineer. Create comprehensive QA strategy for this implementation.

Implementation Details:
{implementation_text}

Provide QA strategy in this EXACT format:

**1. Test Cases:**
- Functional tests for core features
- Edge cases and error scenarios
- Integration test requirements

**2. Security Validation:**
- Authentication/authorization checks
- Input validation requirements
- Data protection measures

**3. Performance Considerations:**
- Load testing requirements
- Scalability checkpoints
- Resource optimization

**4. Error Handling Scenarios:**
- Network failure handling
- Data corruption recovery
- User error management

**5. Production Readiness Checklist:**
- Deployment requirements
- Monitoring setup
- Backup strategies

Then provide the structured data as JSON:
```json
{{
    "test_coverage": 85,
    "security_score": 9,
    "performance_rating": "good",
    "production_ready": true,
    "confidence_score": 8
}}
```

Be specific and actionable for each area."""
        
        return prompt
    
    def _generate_files_summary(self, code_files: List[Dict[str, Any]]) -> str:
        """Generate a summary of code files for the prompt"""
        if not code_files:
            return "No code files provided"
        
        summary_parts = []
        for file_data in code_files:
            summary_parts.append(f"{file_data['filename']} ({file_data['language']}, {file_data['lines']} lines)")
        
        return ", ".join(summary_parts)
    
    def validate_hawk_response(self, response_text: str) -> Dict[str, Any]:
        """Validate that HAWK response contains comprehensive QA analysis"""
        
        validation_result = {
            "valid": False,
            "has_test_cases": False,
            "has_security": False,
            "has_performance": False,
            "has_error_handling": False,
            "has_production_checklist": False,
            "has_json": False,
            "character_count": len(response_text),
            "quality_score": 0
        }
        
        # Check for test cases
        if "1. Test Cases:" in response_text:
            validation_result["has_test_cases"] = True
            validation_result["quality_score"] += 2
        
        # Check for security validation
        if "2. Security Validation:" in response_text:
            validation_result["has_security"] = True
            validation_result["quality_score"] += 2
        
        # Check for performance considerations
        if "3. Performance Considerations:" in response_text:
            validation_result["has_performance"] = True
            validation_result["quality_score"] += 2
        
        # Check for error handling
        if "4. Error Handling Scenarios:" in response_text:
            validation_result["has_error_handling"] = True
            validation_result["quality_score"] += 1
        
        # Check for production readiness
        if "5. Production Readiness Checklist:" in response_text:
            validation_result["has_production_checklist"] = True
            validation_result["quality_score"] += 2
        
        # Check for JSON data
        json_pattern = r'```json\s*\n(.*?)\n```'
        json_match = re.search(json_pattern, response_text, re.DOTALL)
        if json_match:
            try:
                json.loads(json_match.group(1))
                validation_result["has_json"] = True
                validation_result["quality_score"] += 2
            except json.JSONDecodeError:
                pass
        
        # Determine if valid
        validation_result["valid"] = (
            validation_result["has_test_cases"] and 
            validation_result["has_security"] and
            validation_result["has_performance"] and
            validation_result["character_count"] > 400
        )
        
        return validation_result
    
    def extract_qa_data(self, response_text: str) -> Dict[str, Any]:
        """Extract structured QA data from HAWK response"""
        
        qa_analysis = {
            "test_cases": [],
            "security_validation": [],
            "performance_considerations": [],
            "error_handling": [],
            "production_checklist": [],
            "json_data": {},
            "raw_analysis": response_text
        }
        
        # Extract test cases
        test_section = re.search(r'\*\*1\. Test Cases:\*\*\s*\n((?:- [^\n]+\n?)+)', response_text)
        if test_section:
            tests = re.findall(r'- ([^\n]+)', test_section.group(1))
            qa_analysis["test_cases"] = [test.strip() for test in tests]
        
        # Extract security validation
        security_section = re.search(r'\*\*2\. Security Validation:\*\*\s*\n((?:- [^\n]+\n?)+)', response_text)
        if security_section:
            security_items = re.findall(r'- ([^\n]+)', security_section.group(1))
            qa_analysis["security_validation"] = [item.strip() for item in security_items]
        
        # Extract performance considerations
        perf_section = re.search(r'\*\*3\. Performance Considerations:\*\*\s*\n((?:- [^\n]+\n?)+)', response_text)
        if perf_section:
            perf_items = re.findall(r'- ([^\n]+)', perf_section.group(1))
            qa_analysis["performance_considerations"] = [item.strip() for item in perf_items]
        
        # Extract error handling
        error_section = re.search(r'\*\*4\. Error Handling Scenarios:\*\*\s*\n((?:- [^\n]+\n?)+)', response_text)
        if error_section:
            error_items = re.findall(r'- ([^\n]+)', error_section.group(1))
            qa_analysis["error_handling"] = [item.strip() for item in error_items]
        
        # Extract production checklist
        prod_section = re.search(r'\*\*5\. Production Readiness Checklist:\*\*\s*\n((?:- [^\n]+\n?)+)', response_text)
        if prod_section:
            prod_items = re.findall(r'- ([^\n]+)', prod_section.group(1))
            qa_analysis["production_checklist"] = [item.strip() for item in prod_items]
        
        # Extract JSON data
        json_pattern = r'```json\s*\n(.*?)\n```'
        json_match = re.search(json_pattern, response_text, re.DOTALL)
        if json_match:
            try:
                qa_analysis["json_data"] = json.loads(json_match.group(1))
            except json.JSONDecodeError:
                qa_analysis["json_data"] = {}
        
        return qa_analysis
    
    def generate_test_suite(self, qa_data: Dict[str, Any], code_files: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Generate automated test suite based on QA analysis"""
        
        test_suite = {
            "unit_tests": [],
            "integration_tests": [],
            "e2e_tests": [],
            "performance_tests": [],
            "security_tests": []
        }
        
        # Generate unit tests based on code files
        for file_data in code_files:
            if file_data["language"] == "javascript":
                test_suite["unit_tests"].extend(
                    self._generate_js_unit_tests(file_data)
                )
            elif file_data["language"] == "python":
                test_suite["unit_tests"].extend(
                    self._generate_python_unit_tests(file_data)
                )
        
        # Generate integration tests
        if len(code_files) > 1:
            test_suite["integration_tests"] = [
                "Test component communication",
                "Test data flow between modules",
                "Test API integration points"
            ]
        
        # Generate E2E tests for web applications
        if any(file_data["language"] == "html" for file_data in code_files):
            test_suite["e2e_tests"] = [
                "Test complete user workflows",
                "Test cross-browser compatibility",
                "Test responsive design on different devices"
            ]
        
        # Generate performance tests
        test_suite["performance_tests"] = [
            "Load testing with simulated users",
            "Memory usage profiling",
            "Response time benchmarking"
        ]
        
        # Generate security tests
        test_suite["security_tests"] = qa_data.get("security_validation", [])
        
        return test_suite
    
    def _generate_js_unit_tests(self, file_data: Dict[str, Any]) -> List[str]:
        """Generate JavaScript unit test suggestions"""
        return [
            f"Test {file_data['filename']} function exports",
            f"Test {file_data['filename']} error handling",
            f"Test {file_data['filename']} input validation"
        ]
    
    def _generate_python_unit_tests(self, file_data: Dict[str, Any]) -> List[str]:
        """Generate Python unit test suggestions"""
        return [
            f"Test {file_data['filename']} class methods",
            f"Test {file_data['filename']} exception handling",
            f"Test {file_data['filename']} edge cases"
        ]
    
    def calculate_quality_metrics(self, qa_data: Dict[str, Any], code_files: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Calculate overall quality metrics for the implementation"""
        
        metrics = {
            "overall_score": 0,
            "test_coverage_estimate": 0,
            "security_rating": "unknown",
            "maintainability_score": 0,
            "performance_rating": "unknown",
            "production_readiness": False
        }
        
        # Calculate based on QA analysis completeness
        json_data = qa_data.get("json_data", {})
        
        if "test_coverage" in json_data:
            metrics["test_coverage_estimate"] = json_data["test_coverage"]
        
        if "security_score" in json_data:
            score = json_data["security_score"]
            if score >= 8:
                metrics["security_rating"] = "excellent"
            elif score >= 6:
                metrics["security_rating"] = "good"
            elif score >= 4:
                metrics["security_rating"] = "fair"
            else:
                metrics["security_rating"] = "poor"
        
        if "performance_rating" in json_data:
            metrics["performance_rating"] = json_data["performance_rating"]
        
        if "production_ready" in json_data:
            metrics["production_readiness"] = json_data["production_ready"]
        
        # Calculate maintainability based on code structure
        total_lines = sum(file_data["lines"] for file_data in code_files)
        file_count = len(code_files)
        
        if file_count > 0:
            avg_lines_per_file = total_lines / file_count
            if avg_lines_per_file < 100:
                metrics["maintainability_score"] = 9
            elif avg_lines_per_file < 200:
                metrics["maintainability_score"] = 7
            elif avg_lines_per_file < 300:
                metrics["maintainability_score"] = 5
            else:
                metrics["maintainability_score"] = 3
        
        # Calculate overall score
        scores = [
            metrics["test_coverage_estimate"] / 10,  # Convert to 0-10 scale
            json_data.get("security_score", 5),
            metrics["maintainability_score"],
            8 if metrics["performance_rating"] == "excellent" else 
            6 if metrics["performance_rating"] == "good" else 4
        ]
        
        metrics["overall_score"] = sum(scores) / len(scores)
        
        return metrics

# Factory function for HAWK bird
def create_hawk_qa_specialist() -> HawkQASpecialist:
    """Factory function to create HAWK QA specialist instance"""
    return HawkQASpecialist()

# Test function for HAWK bird
def test_hawk_bird():
    """Test the HAWK bird with sample EAGLE input"""
    hawk = create_hawk_qa_specialist()
    
    # Mock EAGLE implementation
    eagle_implementation = {
        "raw_implementation": """
IMPLEMENTATION OVERVIEW:
Complete snake game with HTML5 canvas, CSS styling, and JavaScript game logic.

CODE FILES:
- index.html (50 lines)
- styles.css (75 lines) 
- script.js (150 lines)

IMPLEMENTATION NOTES:
- Used HTML5 Canvas for game rendering
- Implemented collision detection
- Added score tracking system
        """,
        "code_files": [
            {"filename": "index.html", "language": "html", "lines": 50, "size": 1200},
            {"filename": "styles.css", "language": "css", "lines": 75, "size": 1800},
            {"filename": "script.js", "language": "javascript", "lines": 150, "size": 4500}
        ]
    }
    
    analysis = hawk.analyze_implementation(eagle_implementation)
    
    print("🧪 TESTING HAWK BIRD")
    print(f"🦅 Stage: {analysis['stage']}")
    print(f"🤖 Model: {analysis['model']}")
    print(f"🔍 Analysis Type: {analysis['analysis_type']}")
    print(f"📏 Prompt Length: {len(analysis['prompt'])} characters")
    
    return analysis

if __name__ == "__main__":
    # Test HAWK bird independently
    test_hawk_bird()

╔═══━━━─── • ───━━━═══╗
--- File: in_homing.py ---
╚═══━━━─── • ───━━━═══╝

#!/usr/bin/env python3
"""
in-homing.py - IN-HOMING Response Processing & XEdit Generation Bird
Handles LLM2 responses coming back IN and creates the final XEdit interface
"""

import json
import re
import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

class InHomingProcessor:
    """IN-HOMING - The Response Handler & XEdit Generator"""
    
    def __init__(self):
        self.stage_name = "IN-HOMING"
        self.icon = "🔄"
        self.specialty = "LLM2 Response Processing & XEdit Generation"
        self.session_timestamp = self._generate_session_timestamp()
    
    def process_llm2_response(self, llm2_response: str, pipeline_metadata: Dict[str, Any]) -> Dict[str, Any]:
        """
        Main IN-HOMING function - process LLM2 response and generate XEdit interface
        """
        print(f"🔄 IN-HOMING: Processing LLM2 response and generating XEdit...")
        
        processing_result = {
            "success": False,
            "llm2_response": llm2_response,
            "pipeline_metadata": pipeline_metadata,
            "parsed_data": {},
            "xedit_interface": None,
            "xedit_paths": {},
            "project_files": [],
            "session_timestamp": self.session_timestamp,
            "processing_timestamp": datetime.datetime.now().isoformat(),
            "error": None
        }
        
        try:
            # Parse the LLM2 response
            processing_result["parsed_data"] = self._parse_llm2_response(llm2_response)
            
            # Extract code files
            processing_result["project_files"] = self._extract_project_files(processing_result["parsed_data"])
            
            # Generate XEdit paths
            processing_result["xedit_paths"] = self._generate_xedit_paths(processing_result["project_files"])
            
            # Generate XEdit interface
            processing_result["xedit_interface"] = self._generate_xedit_interface(
                processing_result["parsed_data"],
                processing_result["xedit_paths"],
                pipeline_metadata
            )
            
            # Save XEdit interface to file
            xedit_file_path = self._save_xedit_interface(
                processing_result["xedit_interface"],
                pipeline_metadata.get("project_name", "project")
            )
            
            processing_result["xedit_file_path"] = str(xedit_file_path)
            processing_result["success"] = True
            
            print(f"✅ IN-HOMING: Processing completed successfully!")
            print(f"📁 Generated: {len(processing_result['project_files'])} files")
            print(f"🎯 XEdit Paths: {len(processing_result['xedit_paths'])}")
            print(f"💾 Saved: {xedit_file_path}")
            
        except Exception as e:
            processing_result["error"] = str(e)
            processing_result["success"] = False
            print(f"❌ IN-HOMING: Processing failed - {e}")
        
        return processing_result
    
    def _parse_llm2_response(self, response_text: str) -> Dict[str, Any]:
        """Parse the LLM2 response into structured data"""
        
        parsed_data = {
            "project_overview": "",
            "code_files": [],
            "implementation_notes": [],
            "testing_checklist": [],
            "raw_response": response_text
        }
        
        # Extract project overview
        overview_match = re.search(r'\*\*PROJECT OVERVIEW:\*\*\s*\n([^\n*]+(?:\n[^\n*]+)*)', response_text)
        if overview_match:
            parsed_data["project_overview"] = overview_match.group(1).strip()
        
        # Extract code files
        code_files = self._extract_code_blocks_with_filenames(response_text)
        parsed_data["code_files"] = code_files
        
        # Extract implementation notes
        notes_match = re.search(r'\*\*IMPLEMENTATION NOTES:\*\*\s*\n((?:- [^\n]+\n?)+)', response_text)
        if notes_match:
            notes = re.findall(r'- ([^\n]+)', notes_match.group(1))
            parsed_data["implementation_notes"] = [note.strip() for note in notes]
        
        # Extract testing checklist
        testing_match = re.search(r'\*\*TESTING CHECKLIST:\*\*\s*\n((?:- [^\n]+\n?)+)', response_text)
        if testing_match:
            tests = re.findall(r'- ([^\n]+)', testing_match.group(1))
            parsed_data["testing_checklist"] = [test.strip() for test in tests]
        
        print(f"📝 Parsed: {len(parsed_data['code_files'])} files, {len(parsed_data['implementation_notes'])} notes")
        return parsed_data
    
    def _extract_code_blocks_with_filenames(self, response_text: str) -> List[Dict[str, Any]]:
        """Extract code blocks with filenames from response"""
        
        code_files = []
        
        # Pattern for filename-based code blocks
        filename_pattern = r'```filename:\s*([^\n]+)\n(.*?)\n```'
        filename_matches = re.findall(filename_pattern, response_text, re.DOTALL)
        
        for filename, code in filename_matches:
            file_data = {
                "filename": filename.strip(),
                "code": code.strip(),
                "language": self._detect_language(filename.strip()),
                "size": len(code.strip()),
                "lines": len(code.strip().split('\n'))
            }
            code_files.append(file_data)
            print(f"📄 Found: {file_data['filename']} ({file_data['language']}, {file_data['lines']} lines)")
        
        return code_files
    
    def _extract_project_files(self, parsed_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Convert parsed data to project files format"""
        return parsed_data.get("code_files", [])
    
    def _generate_xedit_paths(self, project_files: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
        """Generate 7x001 style XEdit paths for all code elements"""
        
        xedit_paths = {}
        path_counter = 1
        
        for file_data in project_files:
            filename = file_data["filename"]
            language = file_data["language"]
            code = file_data["code"]
            
            print(f"🔍 Analyzing {filename} ({language}) for XEdit paths...")
            
            # Parse code elements in this file
            code_elements = self._parse_code_elements(code, language, filename)
            
            for element in code_elements:
                xedit_id = f"7x{path_counter:03d}"
                
                xedit_paths[xedit_id] = {
                    "display_name": element["name"],
                    "type": element["type"],
                    "filename": filename,
                    "language": language,
                    "line_start": element["line_start"],
                    "line_end": element["line_end"],
                    "lines_display": f"{element['line_start']}-{element['line_end']}",
                    "technical_path": f"{filename}::{element['type']}.{element['name']}/lines[{element['line_start']}-{element['line_end']}]",
                    "optimal_model": self._select_optimal_model(element["type"], language)
                }
                
                path_counter += 1
                print(f"  🎯 {xedit_id}: {element['name']} ({element['type']})")
        
        print(f"✅ Generated {len(xedit_paths)} XEdit paths")
        return xedit_paths
    
    def _parse_code_elements(self, code: str, language: str, filename: str) -> List[Dict[str, Any]]:
        """Parse functions, classes, and other code elements"""
        elements = []
        lines = code.split('\n')
        
        if language == 'python':
            elements.extend(self._parse_python_elements(lines))
        elif language in ['javascript', 'js']:
            elements.extend(self._parse_javascript_elements(lines))
        elif language == 'html':
            elements.extend(self._parse_html_elements(lines))
        elif language == 'css':
            elements.extend(self._parse_css_elements(lines))
        else:
            # Generic parsing for other languages
            elements.extend(self._parse_generic_elements(lines))
        
        return elements
    
    def _parse_python_elements(self, lines: List[str]) -> List[Dict[str, Any]]:
        """Parse Python functions and classes"""
        elements = []
        
        for i, line in enumerate(lines, 1):
            # Function definitions
            func_match = re.match(r'^(\s*)def\s+(\w+)\s*\(', line)
            if func_match:
                elements.append({
                    "name": func_match.group(2),
                    "type": "function",
                    "line_start": i,
                    "line_end": min(i + 20, len(lines))
                })
            
            # Class definitions
            class_match = re.match(r'^(\s*)class\s+(\w+)', line)
            if class_match:
                elements.append({
                    "name": class_match.group(2),
                    "type": "class",
                    "line_start": i,
                    "line_end": min(i + 50, len(lines))
                })
        
        return elements
    
    def _parse_javascript_elements(self, lines: List[str]) -> List[Dict[str, Any]]:
        """Parse JavaScript functions and classes"""
        elements = []
        
        for i, line in enumerate(lines, 1):
            # Function declarations
            func_match = re.search(r'function\s+(\w+)\s*\(', line)
            if func_match:
                elements.append({
                    "name": func_match.group(1),
                    "type": "function",
                    "line_start": i,
                    "line_end": min(i + 15, len(lines))
                })
            
            # Arrow functions and const assignments
            arrow_match = re.search(r'(?:const|let|var)\s+(\w+)\s*=\s*(?:\([^)]*\)\s*=>|function)', line)
            if arrow_match:
                elements.append({
                    "name": arrow_match.group(1),
                    "type": "function",
                    "line_start": i,
                    "line_end": min(i + 10, len(lines))
                })
            
            # Class definitions
            class_match = re.search(r'class\s+(\w+)', line)
            if class_match:
                elements.append({
                    "name": class_match.group(1),
                    "type": "class",
                    "line_start": i,
                    "line_end": min(i + 30, len(lines))
                })
            
            # Method definitions (inside classes)
            method_match = re.search(r'^\s+(\w+)\s*\([^)]*\)\s*\{', line)
            if method_match and not line.strip().startswith('//'):
                elements.append({
                    "name": method_match.group(1),
                    "type": "method",
                    "line_start": i,
                    "line_end": min(i + 12, len(lines))
                })
        
        return elements
    
    def _parse_html_elements(self, lines: List[str]) -> List[Dict[str, Any]]:
        """Parse HTML elements"""
        elements = []
        
        for i, line in enumerate(lines, 1):
            # Major HTML tags with IDs
            id_match = re.search(r'<(div|section|header|footer|main|nav|article)\s*[^>]*id=["\']([^"\']+)["\']', line)
            if id_match:
                elements.append({
                    "name": id_match.group(2),
                    "type": f"html_{id_match.group(1)}",
                    "line_start": i,
                    "line_end": min(i + 5, len(lines))
                })
            
            # Major HTML tags with classes
            class_match = re.search(r'<(div|section|header|footer|main|nav)\s*[^>]*class=["\']([^"\']+)["\']', line)
            if class_match:
                elements.append({
                    "name": class_match.group(2).split()[0],  # First class name
                    "type": f"html_{class_match.group(1)}",
                    "line_start": i,
                    "line_end": min(i + 5, len(lines))
                })
        
        return elements
    
    def _parse_css_elements(self, lines: List[str]) -> List[Dict[str, Any]]:
        """Parse CSS classes and IDs"""
        elements = []
        
        for i, line in enumerate(lines, 1):
            # CSS classes
            class_match = re.search(r'\.([a-zA-Z][\w-]*)\s*\{', line)
            if class_match:
                elements.append({
                    "name": class_match.group(1),
                    "type": "css_class",
                    "line_start": i,
                    "line_end": min(i + 10, len(lines))
                })
            
            # CSS IDs
            id_match = re.search(r'#([a-zA-Z][\w-]*)\s*\{', line)
            if id_match:
                elements.append({
                    "name": id_match.group(1),
                    "type": "css_id",
                    "line_start": i,
                    "line_end": min(i + 10, len(lines))
                })
        
        return elements
    
    def _parse_generic_elements(self, lines: List[str]) -> List[Dict[str, Any]]:
        """Generic parsing for unknown languages"""
        elements = []
        
        for i, line in enumerate(lines, 1):
            # Generic function-like patterns
            if re.search(r'\w+\s*\([^)]*\)\s*\{', line):
                func_match = re.search(r'(\w+)\s*\(', line)
                if func_match:
                    elements.append({
                        "name": func_match.group(1),
                        "type": "function",
                        "line_start": i,
                        "line_end": min(i + 10, len(lines))
                    })
        
        return elements
    
    def _generate_xedit_interface(self, parsed_data: Dict[str, Any], xedit_paths: Dict[str, Dict[str, Any]], pipeline_metadata: Dict[str, Any]) -> str:
        """Generate complete XEdit HTML interface"""
        
        project_name = pipeline_metadata.get("project_name", "Unknown Project")
        
        # Combine all code for display
        combined_code = self._combine_code_for_display(parsed_data["code_files"])
        
        # Generate functions list HTML
        functions_html = self._generate_functions_html(xedit_paths)
        
        # Generate code display HTML with line numbers
        code_html = self._generate_code_html(combined_code)
        
        html_content = f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>🦚 Peacock XEdit Interface - {project_name}</title>
    <style>
        * {{ margin: 0; padding: 0; box-sizing: border-box; }}
        body {{ font-family: 'SF Mono', monospace; background: #0d1117; color: #e6edf3; height: 100vh; overflow: hidden; }}
        
        .header {{ background: #161b22; border-bottom: 1px solid #30363d; padding: 12px 20px; display: flex; justify-content: space-between; align-items: center; }}
        .peacock-logo {{ font-size: 18px; font-weight: bold; color: #ff6b35; }}
        .project-info {{ color: #8b949e; font-size: 14px; }}
        .session-info {{ background: rgba(0, 255, 136, 0.1); border: 1px solid #00ff88; border-radius: 6px; padding: 4px 8px; font-size: 12px; color: #00ff88; }}
        
        .main-container {{ display: flex; height: calc(100vh - 60px); }}
        
        .left-panel {{ width: 320px; background: #161b22; border-right: 1px solid #30363d; display: flex; flex-direction: column; }}
        .panel-header {{ background: #21262d; padding: 12px 16px; border-bottom: 1px solid #30363d; font-weight: 600; font-size: 13px; color: #7c3aed; }}
        
        .functions-list {{ flex: 1; overflow-y: auto; padding: 8px; }}
        .function-item {{ background: #21262d; border: 1px solid #30363d; border-radius: 6px; padding: 12px; margin-bottom: 8px; cursor: pointer; transition: all 0.2s; position: relative; }}
        .function-item:hover {{ border-color: #ff6b35; background: #2d333b; transform: translateX(3px); }}
        .function-item.selected {{ border-color: #ff6b35; background: #2d333b; box-shadow: 0 0 0 1px #ff6b35; }}
        
        .function-info {{ display: flex; align-items: center; gap: 8px; flex-wrap: wrap; }}
        .function-name {{ font-weight: 600; color: #79c0ff; }}
        .function-type {{ background: #30363d; color: #8b949e; padding: 2px 6px; border-radius: 4px; font-size: 10px; }}
        .xedit-id {{ background: #238636; color: white; padding: 2px 6px; border-radius: 4px; font-size: 10px; font-weight: 600; }}
        .function-lines {{ color: #6e7681; font-size: 10px; }}
        
        .add-btn {{ position: absolute; right: 8px; top: 50%; transform: translateY(-50%); background: #238636; color: white; border: none; border-radius: 4px; width: 24px; height: 24px; cursor: pointer; font-weight: bold; }}
        .add-btn:hover {{ background: #2ea043; }}
        
        .middle-panel {{ width: 280px; background: #0d1117; border-right: 1px solid #30363d; display: flex; flex-direction: column; }}
        .payload-header {{ background: #21262d; padding: 12px 16px; border-bottom: 1px solid #30363d; font-weight: 600; font-size: 13px; color: #ff6b35; }}
        
        .payload-list {{ flex: 1; overflow-y: auto; padding: 8px; }}
        .payload-item {{ background: rgba(255, 107, 53, 0.1); border: 1px solid #ff6b35; border-radius: 6px; padding: 8px; margin-bottom: 6px; font-size: 12px; }}
        .payload-item .remove-btn {{ float: right; background: #da3633; color: white; border: none; border-radius: 3px; width: 18px; height: 18px; cursor: pointer; font-size: 10px; }}
        
        .send-btn {{ margin: 8px; padding: 12px; background: linear-gradient(45deg, #238636, #2ea043); color: white; border: none; border-radius: 6px; font-weight: 600; cursor: pointer; }}
        .send-btn:hover {{ background: linear-gradient(45deg, #2ea043, #238636); }}
        
        .right-panel {{ flex: 1; background: #0d1117; display: flex; flex-direction: column; }}
        .code-header {{ background: #21262d; padding: 12px 16px; border-bottom: 1px solid #30363d; font-weight: 600; font-size: 13px; color: #79c0ff; }}
        
        .code-container {{ flex: 1; overflow: auto; }}
        .code-line {{ display: flex; font-family: 'SF Mono', monospace; font-size: 13px; line-height: 1.4; }}
        .code-line:hover {{ background: rgba(255, 255, 255, 0.05); }}
        .line-number {{ width: 60px; padding: 4px 8px; background: #161b22; border-right: 1px solid #30363d; color: #6e7681; text-align: right; flex-shrink: 0; }}
        .line-content {{ padding: 4px 12px; flex: 1; white-space: pre; }}
        
        .highlighted {{ background: rgba(255, 107, 53, 0.2) !important; }}
    </style>
</head>
<body>
    <div class="header">
        <div class="peacock-logo">🦚 Peacock XEdit Interface</div>
        <div class="project-info">{project_name}</div>
        <div class="session-info">Session: {self.session_timestamp}</div>
    </div>
    
    <div class="main-container">
        <!-- Left Panel: Functions List -->
        <div class="left-panel">
            <div class="panel-header">📋 Functions & Classes ({len(xedit_paths)})</div>
            <div class="functions-list">
                {functions_html}
            </div>
        </div>
        
        <!-- Middle Panel: Payload -->
        <div class="middle-panel">
            <div class="payload-header">🎯 Payload</div>
            <div class="payload-list" id="payloadList">
                <div style="color: #6e7681; text-align: center; padding: 20px; font-size: 12px;">
                    Click functions to add XEdit-Paths
                </div>
            </div>
            <button class="send-btn" onclick="sendToLLM2()">🚀 Send to LLM2</button>
            <button class="deploy-btn" onclick="deployProject()" style="margin: 8px; padding: 12px; background: linear-gradient(45deg, #0969da, #1f6feb); color: white; border: none; border-radius: 6px; font-weight: 600; cursor: pointer;">🦚 PCOCK Deploy</button>
        </div>
        
        <!-- Right Panel: Code Display -->
        <div class="right-panel">
            <div class="code-header">💻 Generated Code ({len(parsed_data["code_files"])} files)</div>
            <div class="code-container">
                {code_html}
            </div>
        </div>
    </div>
    
    <script>
        let payloadItems = [];
        const xeditPaths = {json.dumps(xedit_paths)};
        
        function highlightFunction(xeditId) {{
            // Remove previous highlights
            document.querySelectorAll('.highlighted').forEach(el => {{
                el.classList.remove('highlighted');
            }});
            
            // Remove previous selection
            document.querySelectorAll('.function-item.selected').forEach(el => {{
                el.classList.remove('selected');
            }});
            
            // Add selection to clicked item
            event.currentTarget.classList.add('selected');
            
            // Highlight lines based on XEdit path data
            const pathData = xeditPaths[xeditId];
            if (pathData) {{
                for (let lineNum = pathData.line_start; lineNum <= pathData.line_end; lineNum++) {{
                    const lineElement = document.querySelector(`[data-line="${{lineNum}}"]`);
                    if (lineElement) {{
                        lineElement.classList.add('highlighted');
                    }}
                }}
                
                // Scroll to the highlighted section
                const firstHighlighted = document.querySelector('.highlighted');
                if (firstHighlighted) {{
                    firstHighlighted.scrollIntoView({{ behavior: 'smooth', block: 'center' }});
                }}
            }}
            
            console.log('Highlighted function:', xeditId, pathData);
        }}
        
        function addToPayload(xeditId) {{
            event.stopPropagation(); // Prevent highlighting when clicking add button
            
            if (payloadItems.includes(xeditId)) return;
            
            payloadItems.push(xeditId);
            updatePayloadDisplay();
            console.log('Added to payload:', xeditId);
        }}
        
        function removeFromPayload(xeditId) {{
            payloadItems = payloadItems.filter(item => item !== xeditId);
            updatePayloadDisplay();
            console.log('Removed from payload:', xeditId);
        }}
        
        function updatePayloadDisplay() {{
            const payloadList = document.getElementById('payloadList');
            
            if (payloadItems.length === 0) {{
                payloadList.innerHTML = '<div style="color: #6e7681; text-align: center; padding: 20px; font-size: 12px;">Click functions to add XEdit-Paths</div>';
                return;
            }}
            
            payloadList.innerHTML = payloadItems.map(xeditId => {{
                const pathData = xeditPaths[xeditId];
                const displayName = pathData ? pathData.display_name : xeditId;
                return `<div class="payload-item">
                    <strong>${{xeditId}}</strong><br>
                    ${{displayName}} (${{pathData ? pathData.type : 'unknown'}})
                    <button class="remove-btn" onclick="removeFromPayload('${{xeditId}}')">&times;</button>
                </div>`;
            }}).join('');
        }}
        
        function deployProject() {{
            console.log('🦚 PCOCK DEPLOY: Starting deployment...');
            
            // Send deploy request to MCP
            fetch('http://127.0.0.1:8000/deploy', {{
                method: 'POST',
                headers: {{
                    'Content-Type': 'application/json',
                }},
                body: JSON.stringify({{
                    project_name: '{project_name}',
                    action: 'deploy_and_run'
                }})
            }})
            .then(response => response.json())
            .then(data => {{
                if (data.success) {{
                    alert(`🦚 PCOCK DEPLOY SUCCESS!\\n\\n` +
                          `📁 Project: ${{data.project_name}}\\n` +
                          `📄 Files: ${{data.files_deployed}}\\n` +
                          `🌐 Running: ${{data.server_url}}\\n\\n` +
                          `Browser should open automatically!`);
                }} else {{
                    alert(`❌ PCOCK DEPLOY FAILED:\\n${{data.error}}`);
                }}
            }})
            .catch(error => {{
                console.error('Deploy error:', error);
                alert(`❌ Deploy request failed: ${{error.message}}`);
            }});
        }}
        
        function sendToLLM2() {{
            if (payloadItems.length === 0) {{
                alert('Please add some XEdit-Paths to the payload first');
                return;
            }}
            
            const payloadData = payloadItems.map(xeditId => xeditPaths[xeditId]);
            console.log('Sending to LLM2:', payloadData);
            
            alert(`Sending ${{payloadItems.length}} XEdit-Paths to LLM2 for optimization:\n\n${{payloadItems.join(', ')}}`);
            
            // In real implementation, would send to MCP server for LLM2 processing
        }}
        
        console.log('🦚 Peacock XEdit Interface Loaded');
        console.log('📁 Project:', '{project_name}');
        console.log('🔢 XEdit Paths:', {len(xedit_paths)});
        console.log('📄 Code Files:', {len(parsed_data["code_files"])});
        console.log('🎯 XEdit Paths Data:', xeditPaths);
    </script>
</body>
</html>"""
        
        return html_content
    
    def _combine_code_for_display(self, code_files: List[Dict[str, Any]]) -> str:
        """Combine all code files for display in interface"""
        combined = []
        
        for file_data in code_files:
            combined.append(f"// File: {file_data['filename']}")
            combined.append(f"// Language: {file_data['language']}")
            combined.append(f"// Size: {file_data['size']} characters")
            combined.append("")
            combined.append(file_data['code'])
            combined.append("")
            combined.append("// " + "="*60)
            combined.append("")
        
        return "\n".join(combined)
    
    def _generate_functions_html(self, xedit_paths: Dict[str, Dict[str, Any]]) -> str:
        """Generate HTML for functions list"""
        functions_html = ""
        
        for xedit_id, path_data in xedit_paths.items():
            icon = "🏗️" if path_data["type"] == "class" else "⚡"
            if "method" in path_data["type"]:
                icon = "🔧"
            elif "html" in path_data["type"]:
                icon = "🌐"
            elif "css" in path_data["type"]:
                icon = "🎨"
            
            functions_html += f"""
            <div class="function-item" onclick="highlightFunction('{xedit_id}')">
                <div class="function-info">
                    <span>{icon}</span>
                    <span class="function-name">{path_data['display_name']}</span>
                    <span class="function-type">{path_data['type']}</span>
                    <span class="xedit-id">{xedit_id}</span>
                    <div class="function-lines">Lines {path_data['lines_display']}</div>
                </div>
                <button class="add-btn" onclick="addToPayload('{xedit_id}')" title="Add to payload">+</button>
            </div>"""
        
        if not xedit_paths:
            functions_html = '<div style="color: #6e7681; text-align: center; padding: 20px;">No functions or classes found</div>'
        
        return functions_html
    
    def _generate_code_html(self, combined_code: str) -> str:
        """Generate HTML for code display with line numbers"""
        lines = combined_code.split('\n')
        code_html = ""
        
        for i, line in enumerate(lines, 1):
            escaped_line = line.replace('<', '&lt;').replace('>', '&gt;')
            code_html += f'<div class="code-line" data-line="{i}"><span class="line-number">{i:3d}</span><span class="line-content">{escaped_line}</span></div>\n'
        
        return code_html
    
    def _save_xedit_interface(self, html_content: str, project_name: str) -> Path:
        """Save XEdit interface to file"""
        output_dir = Path("/home/flintx/peacock/html")
        output_dir.mkdir(exist_ok=True)
        
        file_path = output_dir / f"xedit-{self.session_timestamp}.html"
        
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(html_content)
        
        print(f"💾 XEdit interface saved: {file_path}")
        return file_path
    
    def _detect_language(self, filename: str) -> str:
        """Detect programming language from filename"""
        ext_map = {
            '.html': 'html',
            '.css': 'css', 
            '.js': 'javascript',
            '.py': 'python',
            '.java': 'java',
            '.cpp': 'cpp',
            '.c': 'c',
            '.php': 'php',
            '.rb': 'ruby',
            '.go': 'go',
            '.rs': 'rust',
            '.ts': 'typescript',
            '.jsx': 'jsx',
            '.tsx': 'tsx'
        }
        
        for ext, lang in ext_map.items():
            if filename.lower().endswith(ext):
                return lang
        
        return 'text'
    
    def _select_optimal_model(self, element_type: str, language: str) -> str:
        """Select optimal model based on element type and language"""
        # Based on testing results
        if element_type == "class" or language in ["html", "css"]:
            return "gemma2-9b-it"  # Better structure handling
        else:
            return "llama-3.1-8b-instant"  # Better code analysis
    
    def _generate_session_timestamp(self) -> str:
        """Generate session timestamp matching MCP format - MILITARY TIME"""
        now = datetime.datetime.now()
        week = now.isocalendar()[1]
        day = now.day
        hour = now.hour  # Already 24-hour format
        minute = now.minute
        return f"{week:02d}-{day:02d}-{hour:02d}{minute:02d}"
    
    def generate_project_summary(self, processing_result: Dict[str, Any]) -> Dict[str, Any]:
        """Generate complete project summary"""
        
        summary = {
            "project_overview": processing_result["parsed_data"].get("project_overview", ""),
            "files_generated": len(processing_result["project_files"]),
            "xedit_paths_created": len(processing_result["xedit_paths"]),
            "total_lines_of_code": sum(file_data["lines"] for file_data in processing_result["project_files"]),
            "languages_used": list(set(file_data["language"] for file_data in processing_result["project_files"])),
            "implementation_notes": processing_result["parsed_data"].get("implementation_notes", []),
            "testing_checklist": processing_result["parsed_data"].get("testing_checklist", []),
            "xedit_interface_path": processing_result.get("xedit_file_path", ""),
            "session_info": {
                "timestamp": self.session_timestamp,
                "processing_time": processing_result["processing_timestamp"]
            }
        }
        
        return summary
    
    def deploy_project_files(self, project_files: List[Dict[str, Any]], project_name: str) -> Dict[str, Any]:
        """Deploy project files to local apps directory and start server"""
        print(f"🚀 IN-HOMING: Deploying {project_name}...")
        
        deploy_result = {
            "success": False,
            "project_path": "",
            "server_url": "",
            "files_deployed": 0,
            "error": None
        }
        
        try:
            # Create apps directory structure
            apps_dir = Path("/home/flintx/peacock/apps")
            apps_dir.mkdir(exist_ok=True)
            
            project_dir = apps_dir / project_name
            project_dir.mkdir(exist_ok=True)
            
            # Deploy all project files
            for file_data in project_files:
                file_path = project_dir / file_data["filename"]
                
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(file_data["code"])
                
                print(f"📄 Deployed: {file_data['filename']} ({file_data['size']} chars)")
                deploy_result["files_deployed"] += 1
            
            # Create project manifest
            manifest = {
                "name": project_name,
                "created": datetime.datetime.now().isoformat(),
                "files": [f["filename"] for f in project_files],
                "languages": list(set(f["language"] for f in project_files)),
                "total_lines": sum(f["lines"] for f in project_files),
                "session": self.session_timestamp
            }
            
            manifest_path = project_dir / "peacock.json"
            with open(manifest_path, "w", encoding="utf-8") as f:
                json.dump(manifest, f, indent=2)
            
            deploy_result["project_path"] = str(project_dir)
            deploy_result["server_url"] = f"http://localhost:8080"
            deploy_result["success"] = True
            
            print(f"✅ Deployed {deploy_result['files_deployed']} files to: {project_dir}")
            print(f"🌐 Ready to serve at: {deploy_result['server_url']}")
            
        except Exception as e:
            deploy_result["error"] = str(e)
            print(f"❌ Deploy failed: {e}")
        
        return deploy_result
    
    def start_local_server(self, project_name: str, port: int = 8080) -> Dict[str, Any]:
        """Start local HTTP server for deployed project"""
        import subprocess
        import webbrowser
        import time
        
        server_result = {
            "success": False,
            "server_url": "",
            "process_id": None,
            "error": None
        }
        
        try:
            project_dir = Path(f"/home/flintx/peacock/apps/{project_name}")
            
            if not project_dir.exists():
                server_result["error"] = f"Project {project_name} not found"
                return server_result
            
            # Start HTTP server in project directory
            server_cmd = [
                "python", "-m", "http.server", str(port), 
                "--directory", str(project_dir)
            ]
            
            print(f"🌐 Starting server: {' '.join(server_cmd)}")
            
            # Start server process (non-blocking)
            process = subprocess.Popen(
                server_cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                cwd=str(project_dir)
            )
            
            # Give server a moment to start
            time.sleep(1)
            
            server_url = f"http://localhost:{port}"
            
            # Open in browser
            print(f"🚀 Opening browser: {server_url}")
            webbrowser.open(server_url)
            
            server_result["success"] = True
            server_result["server_url"] = server_url
            server_result["process_id"] = process.pid
            
            print(f"✅ Server started successfully!")
            print(f"   🌐 URL: {server_url}")
            print(f"   🔢 PID: {process.pid}")
            print(f"   📁 Directory: {project_dir}")
            
        except Exception as e:
            server_result["error"] = str(e)
            print(f"❌ Server start failed: {e}")
        
        return server_result
    
    def deploy_and_run(self, project_files: List[Dict[str, Any]], project_name: str) -> Dict[str, Any]:
        """Complete deploy and run workflow"""
        print(f"🦚 PCOCK DEPLOY: {project_name}")
        
        # Deploy files
        deploy_result = self.deploy_project_files(project_files, project_name)
        
        if not deploy_result["success"]:
            return {
                "success": False,
                "error": f"Deploy failed: {deploy_result['error']}",
                "deploy_result": deploy_result
            }
        
        # Start server and open browser
        server_result = self.start_local_server(project_name)
        
        complete_result = {
            "success": server_result["success"],
            "project_name": project_name,
            "project_path": deploy_result["project_path"],
            "files_deployed": deploy_result["files_deployed"],
            "server_url": server_result["server_url"],
            "process_id": server_result.get("process_id"),
            "deploy_result": deploy_result,
            "server_result": server_result
        }
        
        if complete_result["success"]:
            print(f"🎉 PCOCK DEPLOY COMPLETE!")
            print(f"   📁 Project: {project_name}")
            print(f"   📄 Files: {deploy_result['files_deployed']}")
            print(f"   🌐 Running: {server_result['server_url']}")
        else:
            complete_result["error"] = server_result.get("error", "Unknown server error")
        
        return complete_result
        """Validate the quality of processing results"""
        
        validation = {
            "overall_quality": "unknown",
            "code_files_valid": False,
            "xedit_paths_valid": False,
            "interface_generated": False,
            "recommendations": []
        }
        
        # Check code files
        if processing_result["project_files"] and len(processing_result["project_files"]) > 0:
            validation["code_files_valid"] = True
            
            # Check if files have content
            total_lines = sum(file_data["lines"] for file_data in processing_result["project_files"])
            if total_lines < 50:
                validation["recommendations"].append("Generated code seems very short - may need more implementation")
        
        # Check XEdit paths
        if processing_result["xedit_paths"] and len(processing_result["xedit_paths"]) > 0:
            validation["xedit_paths_valid"] = True
        else:
            validation["recommendations"].append("No XEdit paths generated - code may lack functions/classes")
        
        # Check interface generation
        if processing_result["xedit_interface"] and len(processing_result["xedit_interface"]) > 1000:
            validation["interface_generated"] = True
        else:
            validation["recommendations"].append("XEdit interface generation may have failed")
        
        # Overall quality assessment
        quality_score = 0
        if validation["code_files_valid"]:
            quality_score += 3
        if validation["xedit_paths_valid"]:
            quality_score += 2
        if validation["interface_generated"]:
            quality_score += 3
        
        if quality_score >= 7:
            validation["overall_quality"] = "excellent"
        elif quality_score >= 5:
            validation["overall_quality"] = "good"
        elif quality_score >= 3:
            validation["overall_quality"] = "fair"
        else:
            validation["overall_quality"] = "poor"
        
        return validation

# Factory function for IN-HOMING processor
def create_return_homing_processor() -> InHomingProcessor:
    """Factory function to create IN-HOMING processor instance"""
    return InHomingProcessor()

# Test function for IN-HOMING processor
def test_in_homing_processor():
    """Test the IN-HOMING processor with sample LLM2 response"""
    processor = create_return_homing_processor()
    
    # Mock LLM2 response
    sample_llm2_response = """**PROJECT OVERVIEW:**
Complete snake game implementation with HTML5 canvas, CSS styling, and JavaScript game logic.

**CODE FILES:**

```filename: index.html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Snake Game</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div id="gameContainer">
        <canvas id="gameCanvas" width="400" height="400"></canvas>
        <div id="score">Score: 0</div>
    </div>
    <script src="script.js"></script>
</body>
</html>
```

```filename: styles.css
body {
    margin: 0;
    padding: 0;
    display: flex;
    justify-content: center;
    align-items: center;
    min-height: 100vh;
    background-color: #2c3e50;
    font-family: Arial, sans-serif;
}

#gameContainer {
    text-align: center;
}

#gameCanvas {
    border: 2px solid #fff;
    background-color: #34495e;
}

#score {
    color: white;
    font-size: 24px;
    margin-top: 10px;
}
```

```filename: script.js
const canvas = document.getElementById('gameCanvas');
const ctx = canvas.getContext('2d');
const scoreElement = document.getElementById('score');

class SnakeGame {
    constructor() {
        this.snake = [{x: 200, y: 200}];
        this.food = this.generateFood();
        this.direction = {x: 0, y: 0};
        this.score = 0;
    }
    
    generateFood() {
        return {
            x: Math.floor(Math.random() * (canvas.width / 20)) * 20,
            y: Math.floor(Math.random() * (canvas.height / 20)) * 20
        };
    }
    
    update() {
        const head = {x: this.snake[0].x + this.direction.x, y: this.snake[0].y + this.direction.y};
        this.snake.unshift(head);
        
        if (head.x === this.food.x && head.y === this.food.y) {
            this.score++;
            this.food = this.generateFood();
        } else {
            this.snake.pop();
        }
        
        this.checkCollision();
    }
    
    checkCollision() {
        const head = this.snake[0];
        if (head.x < 0 || head.x >= canvas.width || head.y < 0 || head.y >= canvas.height) {
            this.gameOver();
        }
        
        for (let i = 1; i < this.snake.length; i++) {
            if (head.x === this.snake[i].x && head.y === this.snake[i].y) {
                this.gameOver();
            }
        }
    }
    
    gameOver() {
        alert('Game Over! Score: ' + this.score);
        this.snake = [{x: 200, y: 200}];
        this.direction = {x: 0, y: 0};
        this.score = 0;
        this.food = this.generateFood();
    }
    
    render() {
        ctx.clearRect(0, 0, canvas.width, canvas.height);
        
        // Draw snake
        ctx.fillStyle = '#27ae60';
        this.snake.forEach(segment => {
            ctx.fillRect(segment.x, segment.y, 20, 20);
        });
        
        // Draw food
        ctx.fillStyle = '#e74c3c';
        ctx.fillRect(this.food.x, this.food.y, 20, 20);
        
        // Update score
        scoreElement.textContent = 'Score: ' + this.score;
    }
}

const game = new SnakeGame();

function gameLoop() {
    game.update();
    game.render();
}

document.addEventListener('keydown', (e) => {
    switch(e.key) {
        case 'ArrowUp':
            if (game.direction.y === 0) game.direction = {x: 0, y: -20};
            break;
        case 'ArrowDown':
            if (game.direction.y === 0) game.direction = {x: 0, y: 20};
            break;
        case 'ArrowLeft':
            if (game.direction.x === 0) game.direction = {x: -20, y: 0};
            break;
        case 'ArrowRight':
            if (game.direction.x === 0) game.direction = {x: 20, y: 0};
            break;
    }
});

setInterval(gameLoop, 100);
```

**IMPLEMENTATION NOTES:**
- Used HTML5 Canvas for smooth game rendering
- Implemented collision detection for walls and self-collision
- Added keyboard controls for snake movement
- Simple scoring system with food consumption

**TESTING CHECKLIST:**
- Test arrow key controls for snake movement
- Verify collision detection works properly
- Check food generation and scoring
- Test game over and restart functionality"""

    # Mock pipeline metadata
    pipeline_metadata = {
        "project_name": "snake_game",
        "total_stages": 4,
        "pipeline_duration": "15.3 seconds"
    }
    
    print("🧪 TESTING IN-HOMING PROCESSOR")
    print("="*70)
    
    # Process the LLM2 response
    processing_result = processor.process_llm2_response(sample_llm2_response, pipeline_metadata)
    
    print("\n📊 PROCESSING RESULTS:")
    print(f"✅ Success: {processing_result['success']}")
    print(f"📁 Project Files: {len(processing_result['project_files'])}")
    print(f"🎯 XEdit Paths: {len(processing_result['xedit_paths'])}")
    
    if processing_result["xedit_interface"]:
        print(f"🌐 XEdit Interface: Generated ({len(processing_result['xedit_interface'])} characters)")
    
    if processing_result.get("xedit_file_path"):
        print(f"💾 Saved to: {processing_result['xedit_file_path']}")
    
    # Show XEdit paths generated
    if processing_result["xedit_paths"]:
        print(f"\n🎯 XEDIT PATHS GENERATED:")
        for xedit_id, path_data in processing_result["xedit_paths"].items():
            print(f"   {xedit_id}: {path_data['display_name']} ({path_data['type']}) - Lines {path_data['lines_display']}")
    
    # Generate project summary
    summary = processor.generate_project_summary(processing_result)
    print(f"\n📋 PROJECT SUMMARY:")
    print(f"   📄 Files: {summary['files_generated']}")
    print(f"   📊 Lines of Code: {summary['total_lines_of_code']}")
    print(f"   🔤 Languages: {', '.join(summary['languages_used'])}")
    print(f"   🎯 XEdit Paths: {summary['xedit_paths_created']}")
    
    # Validate quality
    validation = processor.validate_processing_quality(processing_result)
    print(f"\n🔍 QUALITY VALIDATION:")
    print(f"   📈 Overall Quality: {validation['overall_quality']}")
    print(f"   📝 Code Files Valid: {validation['code_files_valid']}")
    print(f"   🎯 XEdit Paths Valid: {validation['xedit_paths_valid']}")
    print(f"   🌐 Interface Generated: {validation['interface_generated']}")
    
    if validation["recommendations"]:
        print(f"   💡 Recommendations:")
        for rec in validation["recommendations"]:
            print(f"      • {rec}")
    
    if processing_result["error"]:
        print(f"❌ Error: {processing_result['error']}")
    
    return processing_result

if __name__ == "__main__":
    # Test IN-HOMING processor
    test_in_homing_processor()

╔══════════•⊱✦⊰•══════════╗
--- File: out_homing.py ---
╚══════════•⊱✦⊰•══════════╝

#!/usr/bin/env python3
"""
out-homing.py - OUT-HOMING Pipeline Orchestration Bird
The conductor who coordinates all birds and sends the complete work OUT to LLM2
"""

import json
import datetime
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional

# Import all the bird modules (same directory)
from spark import create_spark_analyst
from falcon import create_falcon_architect  
from eagle import create_eagle_implementer
from hawk import create_hawk_qa_specialist

class OutHomingOrchestrator:
    """OUT-HOMING - The Pipeline Conductor & LLM2 Outbound Handler"""
    
    def __init__(self):
        self.stage_name = "OUT-HOMING"
        self.icon = "🏠"
        self.specialty = "Pipeline Orchestration & LLM2 Preparation"
        
        # Initialize all birds
        self.spark = create_spark_analyst()
        self.falcon = create_falcon_architect()
        self.eagle = create_eagle_implementer()
        self.hawk = create_hawk_qa_specialist()
        
        # Pipeline state
        self.pipeline_results = {}
        self.session_timestamp = self._generate_session_timestamp()
    
    def orchestrate_full_pipeline(self, user_request: str, project_name: Optional[str] = None) -> Dict[str, Any]:
        """
        Main OUT-HOMING function - orchestrate complete pipeline and prepare for LLM2
        """
        print(f"🏠 OUT-HOMING ORCHESTRATOR: Starting complete pipeline...")
        
        if not project_name:
            project_name = self._generate_project_name(user_request)
        
        pipeline_execution = {
            "project_name": project_name,
            "user_request": user_request,
            "session_timestamp": self.session_timestamp,
            "pipeline_start": datetime.datetime.now().isoformat(),
            "stages_completed": [],
            "stage_results": {},
            "llm2_payload": None,
            "success": False,
            "error": None
        }
        
        try:
            print(f"🎯 Project: {project_name}")
            print(f"📝 Request: {user_request}")
            print(f"🕐 Session: {self.session_timestamp}")
            print("="*70)
            
            # Execute pipeline stages in order
            pipeline_execution = self._execute_spark_stage(pipeline_execution)
            pipeline_execution = self._execute_falcon_stage(pipeline_execution)
            pipeline_execution = self._execute_eagle_stage(pipeline_execution)
            pipeline_execution = self._execute_hawk_stage(pipeline_execution)
            
            # Prepare LLM2 payload
            pipeline_execution = self._prepare_llm2_payload(pipeline_execution)
            
            pipeline_execution["success"] = True
            pipeline_execution["pipeline_end"] = datetime.datetime.now().isoformat()
            
            print(f"✅ OUT-HOMING: Pipeline completed successfully!")
            print(f"📦 LLM2 Payload: {len(pipeline_execution['llm2_payload']['combined_prompt'])} characters")
            
        except Exception as e:
            pipeline_execution["error"] = str(e)
            pipeline_execution["success"] = False
            print(f"❌ OUT-HOMING: Pipeline failed - {e}")
        
        return pipeline_execution
    
    def _execute_spark_stage(self, pipeline_execution: Dict[str, Any]) -> Dict[str, Any]:
        """Execute SPARK requirements analysis stage"""
        print(f"⚡ OUT-HOMING → SPARK: Analyzing requirements...")
        
        user_request = pipeline_execution["user_request"]
        spark_analysis = self.spark.analyze_project_request(user_request)
        
        # Store the prompt for LLM2 combination
        pipeline_execution["stage_results"]["spark"] = {
            "prompt": spark_analysis["prompt"],
            "model": spark_analysis["model"],
            "stage": spark_analysis["stage"],
            "user_request": user_request
        }
        
        pipeline_execution["stages_completed"].append("SPARK")
        
        print(f"✅ SPARK stage completed - {len(spark_analysis['prompt'])} chars")
        return pipeline_execution
    
    def _execute_falcon_stage(self, pipeline_execution: Dict[str, Any]) -> Dict[str, Any]:
        """Execute FALCON architecture design stage"""
        print(f"🦅 OUT-HOMING → FALCON: Designing architecture...")
        
        # For pipeline orchestration, we create a requirements summary for FALCON
        spark_data = pipeline_execution["stage_results"]["spark"]
        
        # Create mock requirements for FALCON (in real flow, this comes from SPARK LLM response)
        mock_spark_requirements = {
            "raw_analysis": f"Requirements analysis for {pipeline_execution['project_name']}",
            "json_data": {
                "core_objective": f"Build {pipeline_execution['user_request']}",
                "in_scope": ["Core functionality", "User interface", "Basic features"],
                "complexity": "moderate"
            }
        }
        
        falcon_design = self.falcon.design_architecture(mock_spark_requirements)
        
        # Store the prompt for LLM2 combination
        pipeline_execution["stage_results"]["falcon"] = {
            "prompt": falcon_design["prompt"],
            "model": falcon_design["model"],
            "stage": falcon_design["stage"],
            "spark_input": mock_spark_requirements
        }
        
        pipeline_execution["stages_completed"].append("FALCON")
        
        print(f"✅ FALCON stage completed - {len(falcon_design['prompt'])} chars")
        return pipeline_execution
    
    def _execute_eagle_stage(self, pipeline_execution: Dict[str, Any]) -> Dict[str, Any]:
        """Execute EAGLE code implementation stage"""
        print(f"🦅 OUT-HOMING → EAGLE: Implementing code...")
        
        # For pipeline orchestration, create architecture summary for EAGLE
        falcon_data = pipeline_execution["stage_results"]["falcon"]
        
        # Create mock architecture for EAGLE (in real flow, this comes from FALCON LLM response)
        mock_falcon_architecture = {
            "raw_design": f"Technical architecture for {pipeline_execution['project_name']}",
            "json_data": {
                "tech_stack": {
                    "frontend": "HTML, CSS, JavaScript",
                    "backend": "None (client-side)",
                    "database": "LocalStorage"
                },
                "complexity": "simple"
            }
        }
        
        eagle_implementation = self.eagle.implement_code(mock_falcon_architecture)
        
        # Store the prompt for LLM2 combination
        pipeline_execution["stage_results"]["eagle"] = {
            "prompt": eagle_implementation["prompt"],
            "model": eagle_implementation["model"],
            "stage": eagle_implementation["stage"],
            "falcon_input": mock_falcon_architecture
        }
        
        pipeline_execution["stages_completed"].append("EAGLE")
        
        print(f"✅ EAGLE stage completed - {len(eagle_implementation['prompt'])} chars")
        return pipeline_execution
    
    def _execute_hawk_stage(self, pipeline_execution: Dict[str, Any]) -> Dict[str, Any]:
        """Execute HAWK quality assurance stage"""
        print(f"🦅 OUT-HOMING → HAWK: Analyzing quality...")
        
        # For pipeline orchestration, create implementation summary for HAWK
        eagle_data = pipeline_execution["stage_results"]["eagle"]
        
        # Create mock implementation for HAWK (in real flow, this comes from EAGLE LLM response)
        mock_eagle_implementation = {
            "raw_implementation": f"Code implementation for {pipeline_execution['project_name']}",
            "code_files": [
                {"filename": "index.html", "language": "html", "lines": 50, "size": 1200},
                {"filename": "styles.css", "language": "css", "lines": 75, "size": 1800},
                {"filename": "script.js", "language": "javascript", "lines": 150, "size": 4500}
            ]
        }
        
        hawk_analysis = self.hawk.analyze_implementation(mock_eagle_implementation)
        
        # Store the prompt for LLM2 combination
        pipeline_execution["stage_results"]["hawk"] = {
            "prompt": hawk_analysis["prompt"],
            "model": hawk_analysis["model"],
            "stage": hawk_analysis["stage"],
            "eagle_input": mock_eagle_implementation
        }
        
        pipeline_execution["stages_completed"].append("HAWK")
        
        print(f"✅ HAWK stage completed - {len(hawk_analysis['prompt'])} chars")
        return pipeline_execution
    
    def _prepare_llm2_payload(self, pipeline_execution: Dict[str, Any]) -> Dict[str, Any]:
        """Combine all bird work into LLM2 payload"""
        print(f"🏠 OUT-HOMING: Preparing LLM2 payload...")
        
        # Combine all prompts into master prompt
        combined_prompt = self._build_combined_prompt(pipeline_execution)
        
        # Prepare complete payload for LLM2
        llm2_payload = {
            "project_name": pipeline_execution["project_name"],
            "session_timestamp": self.session_timestamp,
            "combined_prompt": combined_prompt,
            "model": "llama-3.1-8b-instant",  # Best for final generation
            "temperature": 0.2,
            "max_tokens": 4096,  # Large for complete response
            "stages_summary": self._generate_stages_summary(pipeline_execution),
            "metadata": {
                "total_stages": len(pipeline_execution["stages_completed"]),
                "pipeline_duration": self._calculate_duration(pipeline_execution),
                "complexity_estimate": self._estimate_complexity(pipeline_execution),
                "user_request": pipeline_execution["user_request"]
            }
        }
        
        pipeline_execution["llm2_payload"] = llm2_payload
        
        print(f"✅ LLM2 payload prepared:")
        print(f"   📏 Prompt Length: {len(combined_prompt)} characters")
        print(f"   🤖 Model: {llm2_payload['model']}")
        print(f"   🎯 Max Tokens: {llm2_payload['max_tokens']}")
        print(f"   📊 Complexity: {llm2_payload['metadata']['complexity_estimate']}")
        
        return pipeline_execution
    
    def _build_combined_prompt(self, pipeline_execution: Dict[str, Any]) -> str:
        """Build the master prompt combining all bird work"""
        
        project_name = pipeline_execution["project_name"]
        user_request = pipeline_execution["user_request"]
        
        # Extract individual bird prompts
        spark_prompt = self._extract_stage_prompt(pipeline_execution, "spark")
        falcon_prompt = self._extract_stage_prompt(pipeline_execution, "falcon")
        eagle_prompt = self._extract_stage_prompt(pipeline_execution, "eagle")
        hawk_prompt = self._extract_stage_prompt(pipeline_execution, "hawk")
        
        combined_prompt = f"""You are the final implementation engine for the Peacock development pipeline. 

PROJECT: {project_name}
ORIGINAL REQUEST: {user_request}

The pipeline has completed 4 stages of analysis and design. Now generate the COMPLETE, WORKING implementation.

STAGE OUTPUTS:

=== SPARK REQUIREMENTS ===
{spark_prompt}

=== FALCON ARCHITECTURE ===
{falcon_prompt}

=== EAGLE IMPLEMENTATION ===
{eagle_prompt}

=== HAWK QA STRATEGY ===
{hawk_prompt}

FINAL INSTRUCTION:
Generate COMPLETE, WORKING code files that implement this entire system. Use this EXACT format:

**PROJECT OVERVIEW:**
[Brief description of the complete system]

**CODE FILES:**

```filename: index.html
[Complete HTML file]
```

```filename: styles.css
[Complete CSS file]
```

```filename: script.js
[Complete JavaScript file]
```

[Additional files as needed]

**IMPLEMENTATION NOTES:**
- [Key implementation decisions]
- [How components work together]
- [Usage instructions]

**TESTING CHECKLIST:**
- [How to test the implementation]
- [Expected behavior]
- [Known limitations]

Generate PRODUCTION-READY code that works immediately. No placeholders, no TODOs."""

        return combined_prompt
    
    def _extract_stage_prompt(self, pipeline_execution: Dict[str, Any], stage: str) -> str:
        """Extract the prompt from a specific stage"""
        stage_data = pipeline_execution["stage_results"].get(stage, {})
        return stage_data.get("prompt", f"{stage.upper()} prompt not available")
    
    def _generate_stages_summary(self, pipeline_execution: Dict[str, Any]) -> Dict[str, Any]:
        """Generate summary of all completed stages"""
        
        summary = {
            "stages_completed": pipeline_execution["stages_completed"],
            "total_stages": len(pipeline_execution["stages_completed"]),
            "stage_models": {},
            "prompt_lengths": {},
            "success_rate": 100.0  # All stages completed successfully
        }
        
        for stage in pipeline_execution["stages_completed"]:
            stage_data = pipeline_execution["stage_results"].get(stage.lower(), {})
            summary["stage_models"][stage] = stage_data.get("model", "unknown")
            summary["prompt_lengths"][stage] = len(stage_data.get("prompt", ""))
        
        return summary
    
    def _calculate_duration(self, pipeline_execution: Dict[str, Any]) -> str:
        """Calculate pipeline execution duration"""
        if "pipeline_start" in pipeline_execution and "pipeline_end" in pipeline_execution:
            start = datetime.datetime.fromisoformat(pipeline_execution["pipeline_start"])
            end = datetime.datetime.fromisoformat(pipeline_execution["pipeline_end"])
            duration = end - start
            return f"{duration.total_seconds():.2f} seconds"
        return "unknown"
    
    def _estimate_complexity(self, pipeline_execution: Dict[str, Any]) -> str:
        """Estimate overall project complexity based on stage results"""
        # Simple heuristic based on prompt lengths and requirements
        total_prompt_length = 0
        
        for stage in pipeline_execution["stages_completed"]:
            stage_data = pipeline_execution["stage_results"].get(stage.lower(), {})
            total_prompt_length += len(stage_data.get("prompt", ""))
        
        if total_prompt_length > 8000:
            return "complex"
        elif total_prompt_length > 4000:
            return "moderate"
        else:
            return "simple"
    
    def _generate_session_timestamp(self) -> str:
        """Generate session timestamp matching MCP format"""
        now = datetime.datetime.now()
        week = now.isocalendar()[1]
        day = now.day
        hour = now.hour
        minute = now.minute
        return f"{week}-{day}-{hour}{minute:02d}"
    
    def _generate_project_name(self, user_request: str) -> str:
        """Generate project name from user request"""
        # Simple project name generation
        words = user_request.lower().split()
        meaningful_words = [w for w in words if len(w) > 3 and w not in ['build', 'create', 'make', 'with', 'using']]
        
        if meaningful_words:
            return "_".join(meaningful_words[:3])
        else:
            return f"project_{self.session_timestamp}"
    
    def get_pipeline_status(self) -> Dict[str, Any]:
        """Get current pipeline status and metrics"""
        return {
            "session_timestamp": self.session_timestamp,
            "pipeline_results": self.pipeline_results,
            "birds_loaded": {
                "spark": self.spark is not None,
                "falcon": self.falcon is not None,
                "eagle": self.eagle is not None,
                "hawk": self.hawk is not None
            }
        }
    
    def validate_bird_responses(self, pipeline_execution: Dict[str, Any]) -> Dict[str, Any]:
        """Validate all bird responses for quality"""
        
        validation_results = {
            "overall_valid": True,
            "stage_validations": {},
            "quality_scores": {},
            "recommendations": []
        }
        
        for stage in pipeline_execution["stages_completed"]:
            stage_lower = stage.lower()
            stage_data = pipeline_execution["stage_results"].get(stage_lower, {})
            prompt = stage_data.get("prompt", "")
            
            # Simple validation based on prompt length and content
            validation = {
                "valid": len(prompt) > 100,
                "quality_score": min(len(prompt) / 100, 10),  # Simple scoring
                "has_content": len(prompt.strip()) > 0
            }
            
            validation_results["stage_validations"][stage] = validation
            validation_results["quality_scores"][stage] = validation.get("quality_score", 0)
            
            if not validation.get("valid", False):
                validation_results["overall_valid"] = False
                validation_results["recommendations"].append(f"Re-run {stage} stage - validation failed")
        
        return validation_results
    
    def generate_pipeline_report(self, pipeline_execution: Dict[str, Any]) -> Dict[str, Any]:
        """Generate comprehensive pipeline execution report"""
        
        report = {
            "execution_summary": {
                "project_name": pipeline_execution["project_name"],
                "user_request": pipeline_execution["user_request"],
                "session_timestamp": self.session_timestamp,
                "success": pipeline_execution["success"],
                "stages_completed": len(pipeline_execution["stages_completed"]),
                "total_duration": self._calculate_duration(pipeline_execution)
            },
            "stage_breakdown": {},
            "llm2_payload_summary": {},
            "quality_assessment": {},
            "recommendations": []
        }
        
        # Stage breakdown
        for stage in pipeline_execution["stages_completed"]:
            stage_data = pipeline_execution["stage_results"].get(stage.lower(), {})
            report["stage_breakdown"][stage] = {
                "model": stage_data.get("model", "unknown"),
                "prompt_length": len(stage_data.get("prompt", "")),
                "completed": True
            }
        
        # LLM2 payload summary
        if pipeline_execution.get("llm2_payload"):
            payload = pipeline_execution["llm2_payload"]
            report["llm2_payload_summary"] = {
                "total_prompt_length": len(payload["combined_prompt"]),
                "target_model": payload["model"],
                "max_tokens": payload["max_tokens"],
                "complexity": payload["metadata"]["complexity_estimate"]
            }
        
        # Quality assessment
        validation = self.validate_bird_responses(pipeline_execution)
        report["quality_assessment"] = {
            "overall_valid": validation["overall_valid"],
            "average_quality_score": sum(validation["quality_scores"].values()) / len(validation["quality_scores"]) if validation["quality_scores"] else 0,
            "recommendations": validation["recommendations"]
        }
        
        return report

# Factory function for OUT-HOMING orchestrator
def create_homing_orchestrator() -> OutHomingOrchestrator:
    """Factory function to create OUT-HOMING orchestrator instance"""
    return OutHomingOrchestrator()

# Test function for complete pipeline
def test_out_homing_pipeline():
    """Test the complete OUT-HOMING pipeline orchestration"""
    homing = create_homing_orchestrator()
    
    test_request = "Build a snake game with HTML, CSS, and JavaScript"
    
    print("🧪 TESTING COMPLETE OUT-HOMING PIPELINE")
    print(f"📝 Request: {test_request}")
    print("="*70)
    
    # Execute complete pipeline
    pipeline_result = homing.orchestrate_full_pipeline(test_request)
    
    print("\n📊 PIPELINE RESULTS:")
    print(f"✅ Success: {pipeline_result['success']}")
    print(f"📁 Project: {pipeline_result['project_name']}")
    print(f"🕐 Session: {pipeline_result['session_timestamp']}")
    print(f"🔄 Stages: {', '.join(pipeline_result['stages_completed'])}")
    
    if pipeline_result["llm2_payload"]:
        payload = pipeline_result["llm2_payload"]
        print(f"\n📦 LLM2 PAYLOAD:")
        print(f"   📏 Prompt Length: {len(payload['combined_prompt'])} characters")
        print(f"   🤖 Model: {payload['model']}")
        print(f"   🎯 Max Tokens: {payload['max_tokens']}")
        print(f"   📊 Complexity: {payload['metadata']['complexity_estimate']}")
        print(f"   ⏱️ Duration: {payload['metadata']['pipeline_duration']}")
    
    if pipeline_result["error"]:
        print(f"❌ Error: {pipeline_result['error']}")
    
    # Generate pipeline report
    report = homing.generate_pipeline_report(pipeline_result)
    print(f"\n📋 PIPELINE REPORT:")
    print(f"   🎯 Stages Completed: {report['execution_summary']['stages_completed']}")
    print(f"   📈 Quality Score: {report['quality_assessment']['average_quality_score']:.1f}/10")
    print(f"   ✅ Overall Valid: {report['quality_assessment']['overall_valid']}")
    
    return pipeline_result

if __name__ == "__main__":
    # Test complete OUT-HOMING pipeline
    test_out_homing_pipeline()

╭────────────────────────╮
--- File: spark.py ---
╰────────────────────────╯

#!/usr/bin/env python3
"""
spark.py - SPARK Requirements Analysis Bird
The strategic analyst who breaks down user requests into clear, actionable requirements
"""

import json
import re
from typing import Dict, List, Any

class SparkAnalyst:
    """SPARK - The Requirements Whisperer"""
    
    def __init__(self):
        self.stage_name = "SPARK"
        self.icon = "⚡"
        self.specialty = "Strategic Requirements Analysis"
        self.optimal_model = "llama3-8b-8192"  # Speed for requirements
    
    def analyze_project_request(self, user_request: str) -> Dict[str, Any]:
        """
        Main SPARK function - analyze user request and extract clear requirements
        """
        print(f"⚡ SPARK ANALYST: Breaking down project requirements...")
        
        # Generate the SPARK analysis prompt
        spark_prompt = self._build_spark_prompt(user_request)
        
        # Package the analysis for MCP processing
        spark_analysis = {
            "stage": "SPARK",
            "prompt": spark_prompt,
            "user_request": user_request,
            "model": self.optimal_model,
            "temperature": 0.3,
            "max_tokens": 1024,
            "analysis_type": "requirements_extraction"
        }
        
        return spark_analysis
    
    def _build_spark_prompt(self, user_request: str) -> str:
        """Build the strategic SPARK analysis prompt"""
        
        prompt = f"""<thinking>
The user wants me to analyze this project idea strategically. I need to break this down into clear, actionable components.

Project: {user_request}

I should provide:
1. Core objective - what's the main goal?
2. Current state - what problems does this solve?
3. Target state - what's the desired outcome?
4. In scope - what features are included?
5. Out of scope - what's not included?
</thinking>

Act as Spark, a strategic requirements analyst. Analyze this project idea:

Project: {user_request}

Provide analysis in this EXACT format:

**1. Core Objective:**
[One clear sentence describing the main goal]

**2. Current State:**
[Current situation/problems this solves]

**3. Target State:**
[Desired end state after implementation]

**4. In Scope:**
- [Feature 1]
- [Feature 2] 
- [Feature 3]

**5. Out of Scope:**
- [What's NOT included]
- [Future considerations]

Then provide the structured data as JSON:
```json
{{
    "core_objective": "string",
    "current_state": "string",
    "target_state": "string", 
    "in_scope": ["list"],
    "out_of_scope": ["list"],
    "confidence_score": 8
}}
```

Keep it strategic and concise. Use your reasoning capabilities."""
        
        return prompt
    
    def validate_spark_response(self, response_text: str) -> Dict[str, Any]:
        """Validate that SPARK response contains required elements"""
        
        validation_result = {
            "valid": False,
            "has_objective": False,
            "has_scope": False,
            "has_json": False,
            "character_count": len(response_text),
            "quality_score": 0
        }
        
        # Check for core sections
        if "Core Objective:" in response_text:
            validation_result["has_objective"] = True
            validation_result["quality_score"] += 2
        
        if "In Scope:" in response_text and "Out of Scope:" in response_text:
            validation_result["has_scope"] = True
            validation_result["quality_score"] += 2
        
        # Check for JSON data
        json_pattern = r'```json\s*\n(.*?)\n```'
        json_match = re.search(json_pattern, response_text, re.DOTALL)
        if json_match:
            try:
                json.loads(json_match.group(1))
                validation_result["has_json"] = True
                validation_result["quality_score"] += 3
            except json.JSONDecodeError:
                pass
        
        # Determine if valid
        validation_result["valid"] = (
            validation_result["has_objective"] and 
            validation_result["has_scope"] and
            validation_result["character_count"] > 200
        )
        
        return validation_result
    
    def extract_requirements_data(self, response_text: str) -> Dict[str, Any]:
        """Extract structured requirements data from SPARK response"""
        
        requirements = {
            "core_objective": "",
            "current_state": "",
            "target_state": "",
            "in_scope": [],
            "out_of_scope": [],
            "json_data": {},
            "raw_analysis": response_text
        }
        
        # Extract core objective
        obj_match = re.search(r'\*\*1\. Core Objective:\*\*\s*\n([^\n*]+)', response_text)
        if obj_match:
            requirements["core_objective"] = obj_match.group(1).strip()
        
        # Extract current state
        current_match = re.search(r'\*\*2\. Current State:\*\*\s*\n([^\n*]+)', response_text)
        if current_match:
            requirements["current_state"] = current_match.group(1).strip()
        
        # Extract target state
        target_match = re.search(r'\*\*3\. Target State:\*\*\s*\n([^\n*]+)', response_text)
        if target_match:
            requirements["target_state"] = target_match.group(1).strip()
        
        # Extract in scope items
        in_scope_section = re.search(r'\*\*4\. In Scope:\*\*\s*\n((?:- [^\n]+\n?)+)', response_text)
        if in_scope_section:
            scope_items = re.findall(r'- ([^\n]+)', in_scope_section.group(1))
            requirements["in_scope"] = [item.strip() for item in scope_items]
        
        # Extract out of scope items
        out_scope_section = re.search(r'\*\*5\. Out of Scope:\*\*\s*\n((?:- [^\n]+\n?)+)', response_text)
        if out_scope_section:
            out_items = re.findall(r'- ([^\n]+)', out_scope_section.group(1))
            requirements["out_of_scope"] = [item.strip() for item in out_items]
        
        # Extract JSON data
        json_pattern = r'```json\s*\n(.*?)\n```'
        json_match = re.search(json_pattern, response_text, re.DOTALL)
        if json_match:
            try:
                requirements["json_data"] = json.loads(json_match.group(1))
            except json.JSONDecodeError:
                requirements["json_data"] = {}
        
        return requirements

# Factory function for SPARK bird
def create_spark_analyst() -> SparkAnalyst:
    """Factory function to create SPARK analyst instance"""
    return SparkAnalyst()

# Test function for SPARK bird
def test_spark_bird():
    """Test the SPARK bird with sample input"""
    spark = create_spark_analyst()
    
    test_request = "Build a snake game with HTML, CSS, and JavaScript"
    analysis = spark.analyze_project_request(test_request)
    
    print("🧪 TESTING SPARK BIRD")
    print(f"📝 Request: {test_request}")
    print(f"⚡ Stage: {analysis['stage']}")
    print(f"🤖 Model: {analysis['model']}")
    print(f"📊 Analysis Type: {analysis['analysis_type']}")
    print(f"📏 Prompt Length: {len(analysis['prompt'])} characters")
    
    return analysis

if __name__ == "__main__":
    # Test SPARK bird independently
    test_spark_bird()