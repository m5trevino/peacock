╭━─━─━─≪✠≫─━─━─━╮
--- File: __init__.py ---
╰━─━─━─≪✠≫─━─━─━╯

"""
Peacock Aviary - Modular AI Development Pipeline
Each bird specializes in one domain for optimal results
"""

from .spark import SparkAnalyst
from .falcon import FalconArchitect  
from .eagle import EagleImplementer
from .hawk import HawkQASpecialist
from .homing import HomingOrchestrator
from .return_homing import ReturnHomingProcessor

__version__ = "2.0.0"
__author__ = "Peacock Development Team"

# Export main classes
__all__ = [
    'SparkAnalyst',
    'FalconArchitect', 
    'EagleImplementer',
    'HawkQASpecialist',
    'HomingOrchestrator',
    'ReturnHomingProcessor'
]

# Bird factory functions
def create_spark_analyst():
    """Factory function for SPARK requirements analyst"""
    return SparkAnalyst()

def create_falcon_architect():
    """Factory function for FALCON system architect"""
    return FalconArchitect()

def create_eagle_implementer():
    """Factory function for EAGLE code implementer"""
    return EagleImplementer()

def create_hawk_qa_specialist():
    """Factory function for HAWK QA specialist"""
    return HawkQASpecialist()

def create_homing_orchestrator():
    """Factory function for HOMING pipeline orchestrator"""
    return HomingOrchestrator()

def create_return_homing_processor():
    """Factory function for RETURN-HOMING response processor"""
    return ReturnHomingProcessor()

# Pipeline configuration
OPTIMAL_MODEL_ASSIGNMENTS = {
    "spark_analysis": "llama3-8b-8192",        # Speed for requirements
    "falcon_architecture": "gemma2-9b-it",     # Structure champion  
    "eagle_implementation": "llama-3.1-8b-instant", # Code generation beast
    "hawk_qa": "gemma2-9b-it",                  # QA structure
    "code_analysis": "llama-3.1-8b-instant"    # Code review king
}

PIPELINE_STAGES = [
    "spark_analysis",
    "falcon_architecture", 
    "eagle_implementation",
    "hawk_qa"
]

┎━─━─━─━─━─━─━─━─━┒
--- File: eagle.py ---
┖━─━─━─━─━─━─━─━─━┚

#!/usr/bin/env python3
"""
eagle.py - EAGLE Code Implementation Bird
The coding beast who transforms architecture into working code
"""

import json
import re
from typing import Dict, List, Any

class EagleImplementer:
    """EAGLE - The Code Generation Beast"""
    
    def __init__(self):
        self.stage_name = "EAGLE"
        self.icon = "🦅"
        self.specialty = "Code Implementation & Generation"
        self.optimal_model = "llama-3.1-8b-instant"  # Code generation beast
    
    def implement_code(self, falcon_architecture: Dict[str, Any]) -> Dict[str, Any]:
        """
        Main EAGLE function - generate working code based on FALCON architecture
        """
        print(f"🦅 EAGLE IMPLEMENTER: Generating working code...")
        
        # Extract architecture data
        architecture_text = falcon_architecture.get("raw_design", "")
        architecture_data = falcon_architecture.get("json_data", {})
        
        # Generate the EAGLE implementation prompt
        eagle_prompt = self._build_eagle_prompt(architecture_data)
        
        # Package the implementation for MCP processing
        eagle_implementation = {
            "stage": "EAGLE",
            "prompt": eagle_prompt,
            "falcon_input": falcon_architecture,
            "model": self.optimal_model,
            "temperature": 0.2,  # Lower for more consistent code
            "max_tokens": 2048,  # More tokens for code generation
            "implementation_type": "code_generation"
        }
        
        return eagle_implementation
    
    def _build_eagle_prompt(self, falcon_design: Dict[str, Any]) -> str:
        """Build the implementation prompt for EAGLE"""
        
        architecture = falcon_design.get("raw_design", "")
        tech_stack = falcon_design.get("json_data", {}).get("tech_stack", {})
        
        return f"""<thinking>
I need to implement the actual code based on this architecture design.

Architecture:
{architecture}

I should:
1. Write clean, well-documented code
2. Follow best practices for the chosen tech stack
3. Include necessary imports and dependencies
4. Add comments explaining complex logic
5. Structure the code for maintainability
</thinking>

Act as Eagle, a senior software engineer. Implement the code for this project.

Architecture:
{architecture}

Provide the implementation in this EXACT format:

**OVERVIEW:**
[Brief overview of implementation approach]

**TECH STACK:**
- Frontend: {tech_stack.get('frontend', 'Not specified')}
- Backend: {tech_stack.get('backend', 'Not specified')}
- Database: {tech_stack.get('database', 'Not specified')}

**IMPLEMENTATION DETAILS:**
[Explain key implementation decisions and considerations]

**CODE FILES:**

**filename: [filename]**
```[language]
[code content]
```

[Repeat for each file]

**TESTING INSTRUCTIONS:**
[How to test the implementation]

Then provide the structured data as JSON:
```json
{{
    "files_created": ["list of filenames"],
    "dependencies": ["list of required dependencies"],
    "complexity": "simple|moderate|complex",
    "confidence_score": 8
}}
```

Focus on production-quality, maintainable code."""
    
    def validate_eagle_response(self, response_text: str) -> Dict[str, Any]:
        """Validate that EAGLE response contains working code"""
        
        validation_result = {
            "valid": False,
            "has_overview": False,
            "has_code_files": False,
            "has_implementation_notes": False,
            "has_json": False,
            "file_count": 0,
            "character_count": len(response_text),
            "quality_score": 0
        }
        
        # Check for implementation overview
        if "IMPLEMENTATION OVERVIEW:" in response_text:
            validation_result["has_overview"] = True
            validation_result["quality_score"] += 1
        
        # Check for code files
        code_files = re.findall(r'```filename:\s*([^\n]+)\n(.*?)\n```', response_text, re.DOTALL)
        if code_files:
            validation_result["has_code_files"] = True
            validation_result["file_count"] = len(code_files)
            validation_result["quality_score"] += min(len(code_files), 3)  # Max 3 points for files
        
        # Check for implementation notes
        if "IMPLEMENTATION NOTES:" in response_text:
            validation_result["has_implementation_notes"] = True
            validation_result["quality_score"] += 1
        
        # Check for JSON data
        json_pattern = r'```json\s*\n(.*?)\n```'
        json_match = re.search(json_pattern, response_text, re.DOTALL)
        if json_match:
            try:
                json.loads(json_match.group(1))
                validation_result["has_json"] = True
                validation_result["quality_score"] += 2
            except json.JSONDecodeError:
                pass
        
        # Determine if valid
        validation_result["valid"] = (
            validation_result["has_code_files"] and 
            validation_result["file_count"] >= 1 and
            validation_result["character_count"] > 500
        )
        
        return validation_result
    
    def extract_code_files(self, response_text: str) -> List[Dict[str, Any]]:
        """Extract all code files from EAGLE response"""
        
        code_files = []
        
        # Pattern for filename-based code blocks
        filename_pattern = r'```filename:\s*([^\n]+)\n(.*?)\n```'
        filename_matches = re.findall(filename_pattern, response_text, re.DOTALL)
        
        for filename, code in filename_matches:
            file_data = {
                "filename": filename.strip(),
                "code": code.strip(),
                "language": self._detect_language(filename.strip()),
                "size": len(code.strip()),
                "lines": len(code.strip().split('\n'))
            }
            code_files.append(file_data)
        
        return code_files
    
    def extract_implementation_data(self, response_text: str) -> Dict[str, Any]:
        """Extract structured implementation data from EAGLE response"""
        
        implementation = {
            "overview": "",
            "code_files": [],
            "implementation_notes": [],
            "testing_instructions": [],
            "json_data": {},
            "raw_implementation": response_text
        }
        
        # Extract implementation overview
        overview_match = re.search(r'\*\*OVERVIEW:\*\*\s*\n([^\n*]+(?:\n[^\n*]+)*)', response_text)
        if overview_match:
            implementation["overview"] = overview_match.group(1).strip()
        
        # Extract code files
        implementation["code_files"] = self.extract_code_files(response_text)
        
        # Extract implementation notes
        notes_section = re.search(r'\*\*IMPLEMENTATION DETAILS:\*\*\s*\n((?:[^\n]+\n?)+)', response_text)
        if notes_section:
            notes = re.findall(r'[^\n]+', notes_section.group(1))
            implementation["implementation_notes"] = [note.strip() for note in notes]
        
        # Extract testing instructions
        test_section = re.search(r'\*\*TESTING INSTRUCTIONS:\*\*\s*\n((?:[^\n]+\n?)+)', response_text)
        if test_section:
            instructions = re.findall(r'[^\n]+', test_section.group(1))
            implementation["testing_instructions"] = [instruction.strip() for instruction in instructions]
        
        # Extract JSON data
        json_pattern = r'```json\s*\n(.*?)\n```'
        json_match = re.search(json_pattern, response_text, re.DOTALL)
        if json_match:
            try:
                implementation["json_data"] = json.loads(json_match.group(1))
            except json.JSONDecodeError:
                implementation["json_data"] = {}
        
        return implementation
    
    def _detect_language(self, filename: str) -> str:
        """Detect programming language from filename"""
        ext_map = {
            '.html': 'html',
            '.css': 'css', 
            '.js': 'javascript',
            '.py': 'python',
            '.java': 'java',
            '.cpp': 'cpp',
            '.c': 'c',
            '.php': 'php',
            '.rb': 'ruby',
            '.go': 'go',
            '.rs': 'rust',
            '.ts': 'typescript',
            '.jsx': 'jsx',
            '.tsx': 'tsx'
        }
        
        for ext, lang in ext_map.items():
            if filename.lower().endswith(ext):
                return lang
        
        return 'text'
    
    def generate_project_structure(self, code_files: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Generate project structure and file organization"""
        
        structure = {
            "total_files": len(code_files),
            "total_lines": sum(file_data["lines"] for file_data in code_files),
            "total_size": sum(file_data["size"] for file_data in code_files),
            "languages": list(set(file_data["language"] for file_data in code_files)),
            "file_breakdown": {}
        }
        
        # Categorize files by type
        for file_data in code_files:
            lang = file_data["language"]
            if lang not in structure["file_breakdown"]:
                structure["file_breakdown"][lang] = {
                    "count": 0,
                    "total_lines": 0,
                    "files": []
                }
            
            structure["file_breakdown"][lang]["count"] += 1
            structure["file_breakdown"][lang]["total_lines"] += file_data["lines"]
            structure["file_breakdown"][lang]["files"].append(file_data["filename"])
        
        return structure
    
    def optimize_code_structure(self, implementation_data: Dict[str, Any]) -> Dict[str, Any]:
        """Optimize code structure for better organization"""
        
        optimization_suggestions = {
            "structure_improvements": [],
            "performance_tips": [],
            "maintainability_suggestions": [],
            "scalability_considerations": []
        }
        
        code_files = implementation_data.get("code_files", [])
        
        # Analyze structure
        if len(code_files) > 5:
            optimization_suggestions["structure_improvements"].append(
                "Consider organizing files into folders (src/, assets/, components/)"
            )
        
        # Check for large files
        for file_data in code_files:
            if file_data["lines"] > 200:
                optimization_suggestions["maintainability_suggestions"].append(
                    f"Consider breaking down {file_data['filename']} - {file_data['lines']} lines is quite large"
                )
        
        # Performance suggestions based on file types
        languages = [file_data["language"] for file_data in code_files]
        if "javascript" in languages:
            optimization_suggestions["performance_tips"].extend([
                "Consider code splitting for large JavaScript files",
                "Implement lazy loading for better performance",
                "Minify JavaScript for production"
            ])
        
        if "css" in languages:
            optimization_suggestions["performance_tips"].extend([
                "Consider CSS minification and compression",
                "Use CSS custom properties for better maintainability"
            ])
        
        # Scalability considerations
        if len(code_files) >= 3:
            optimization_suggestions["scalability_considerations"].extend([
                "Consider implementing a build system (webpack, vite, etc.)",
                "Set up testing framework for future development",
                "Consider version control and deployment strategy"
            ])
        
        return optimization_suggestions

# Factory function for EAGLE bird
def create_eagle_implementer() -> EagleImplementer:
    """Factory function to create EAGLE implementer instance"""
    return EagleImplementer()

# Test function for EAGLE bird
def test_eagle_bird():
    """Test the EAGLE bird with sample FALCON input"""
    eagle = create_eagle_implementer()
    
    # Mock FALCON architecture
    falcon_architecture = {
        "raw_design": """
TECHNOLOGY STACK:
- Frontend: HTML, CSS, JavaScript
- Backend: None (client-side only)
- Database: LocalStorage

CORE COMPONENTS:
1. Game Engine - Handles snake movement and collision detection
2. Renderer - Draws game elements on canvas
3. Input Handler - Processes user keyboard input

FILE STRUCTURE:
```
snake_game/
├── index.html
├── styles.css
└── script.js
```
        """,
        "json_data": {
            "tech_stack": {
                "frontend": "HTML, CSS, JavaScript",
                "backend": "None",
                "database": "LocalStorage"
            },
            "complexity": "simple"
        }
    }
    
    implementation = eagle.implement_code(falcon_architecture)
    
    print("🧪 TESTING EAGLE BIRD")
    print(f"🦅 Stage: {implementation['stage']}")
    print(f"🤖 Model: {implementation['model']}")
    print(f"💻 Implementation Type: {implementation['implementation_type']}")
    print(f"📏 Prompt Length: {len(implementation['prompt'])} characters")
    print(f"🔥 Temperature: {implementation['temperature']}")
    print(f"📊 Max Tokens: {implementation['max_tokens']}")
    
    return implementation

if __name__ == "__main__":
    # Test EAGLE bird independently
    test_eagle_bird()

┍──━──━──┙◆┕──━──━──┑
--- File: falcon.py ---
┕──━──━──┑◆┍──━──━──┙

#!/usr/bin/env python3
"""
falcon.py - FALCON Architecture Design Bird
The senior architect who designs technical systems and component structures
"""

import json
import re
from typing import Dict, List, Any

class FalconArchitect:
    """FALCON - The System Architect"""
    
    def __init__(self):
        self.stage_name = "FALCON"
        self.icon = "🦅"
        self.specialty = "Technical Architecture Design"
        self.optimal_model = "gemma2-9b-it"  # Structure champion
    
    def design_architecture(self, spark_requirements: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generate architecture design prompt based on SPARK requirements.
        MCP will handle the actual LLM call.
        """
        print(f"🦅 FALCON ARCHITECT: Generating architecture design prompt...")
        
        # Extract key data from SPARK analysis
        spark_analysis = spark_requirements.get("analysis", {})
        spark_text = spark_requirements.get("raw_response", "")
        
        if not spark_text and isinstance(spark_analysis, dict):
            spark_text = "\n".join(f"{k}: {v}" for k, v in spark_analysis.items())
        
        # Generate the FALCON architecture prompt
        falcon_prompt = self._build_falcon_prompt(spark_text, spark_analysis)
        
        # Package the prompt for MCP processing
        falcon_design = {
            "stage": "FALCON",
            "prompt": falcon_prompt,
            "spark_input": spark_requirements,
            "model": self.optimal_model,
            "temperature": 0.3,
            "max_tokens": 1024,
            "design_type": "technical_architecture"
        }
        
        print(f"✅ FALCON prompt generated: {len(falcon_prompt)} characters")
        return falcon_design
    
    def _build_falcon_prompt(self, spark_text: str, requirements_data: Dict[str, Any]) -> str:
        """Build the technical architecture design prompt"""
        
        return f"""<thinking>
Based on the requirements from Spark, I need to design a technical architecture.

Requirements: {spark_text}

I should think about:
- What technologies would work best
- How to structure the codebase
- What components are needed
- How they interact
</thinking>

Act as Falcon, a senior software architect. Design the technical architecture for this project.

Requirements Analysis:
{spark_text}

Provide architecture design in this EXACT format:

**TECHNOLOGY STACK:**
- Frontend: [Technology choices]
- Backend: [Technology choices]  
- Database: [Technology choices]
- Additional: [Other technologies]

**CORE COMPONENTS:**
1. [Component Name] - [Purpose and functionality]
2. [Component Name] - [Purpose and functionality]
3. [Component Name] - [Purpose and functionality]

**FILE STRUCTURE:**
```
project_root/
├── [folder1]/
│   ├── [file1.ext]
│   └── [file2.ext]
├── [folder2]/
└── [file3.ext]
```

**COMPONENT INTERACTIONS:**
[Describe how components communicate and data flows]

Then provide the structured data as JSON:
```json
{{
    "tech_stack": {{
        "frontend": "string",
        "backend": "string",
        "database": "string"
    }},
    "components": ["list"],
    "complexity": "simple|moderate|complex",
    "file_structure": ["list"],
    "interactions": "string"
}}
"""
        
    def validate_falcon_response(self, response_text: str) -> Dict[str, Any]:
        """Validate that FALCON response contains required architecture elements"""
        
        validation_result = {
            "valid": False,
            "has_tech_stack": False,
            "has_components": False,
            "has_file_structure": False,
            "has_json": False,
            "character_count": len(response_text),
            "quality_score": 0
        }
        
        # Check for technology stack
        if "TECHNOLOGY STACK:" in response_text:
            validation_result["has_tech_stack"] = True
            validation_result["quality_score"] += 2
        
        # Check for core components
        if "CORE COMPONENTS:" in response_text:
            validation_result["has_components"] = True
            validation_result["quality_score"] += 2
        
        # Check for file structure
        if "FILE STRUCTURE:" in response_text and "project_root/" in response_text:
            validation_result["has_file_structure"] = True
            validation_result["quality_score"] += 2
        
        # Check for JSON data
        json_pattern = r'```json\s*\n(.*?)\n```'
        json_match = re.search(json_pattern, response_text, re.DOTALL)
        if json_match:
            try:
                json.loads(json_match.group(1))
                validation_result["has_json"] = True
                validation_result["quality_score"] += 3
            except json.JSONDecodeError:
                pass
        
        # Determine if valid
        validation_result["valid"] = (
            validation_result["has_tech_stack"] and 
            validation_result["has_components"] and
            validation_result["character_count"] > 300
        )
        
        return validation_result
    
    def extract_architecture_data(self, response_text: str) -> Dict[str, Any]:
        """Extract structured architecture data from FALCON response"""
        
        architecture = {
            "tech_stack": {},
            "components": [],
            "file_structure": "",
            "component_interactions": "",
            "json_data": {},
            "raw_design": response_text
        }
        
        # Extract technology stack
        tech_section = re.search(r'\*\*TECHNOLOGY STACK:\*\*\s*\n((?:- [^\n]+\n?)+)', response_text)
        if tech_section:
            tech_items = re.findall(r'- ([^:]+): ([^\n]+)', tech_section.group(1))
            for category, tech in tech_items:
                architecture["tech_stack"][category.strip().lower()] = tech.strip()
        
        # Extract core components
        comp_section = re.search(r'\*\*CORE COMPONENTS:\*\*\s*\n((?:\d+\. [^\n]+\n?)+)', response_text)
        if comp_section:
            components = re.findall(r'\d+\. ([^-]+) - ([^\n]+)', comp_section.group(1))
            for name, purpose in components:
                architecture["components"].append({
                    "name": name.strip(),
                    "purpose": purpose.strip()
                })
        
        # Extract file structure
        file_match = re.search(r'\*\*FILE STRUCTURE:\*\*\s*\n```\s*\n(.*?)\n```', response_text, re.DOTALL)
        if file_match:
            architecture["file_structure"] = file_match.group(1).strip()
        
        # Extract component interactions
        interact_match = re.search(r'\*\*COMPONENT INTERACTIONS:\*\*\s*\n([^\n*]+(?:\n[^\n*]+)*)', response_text)
        if interact_match:
            architecture["component_interactions"] = interact_match.group(1).strip()
        
        # Extract JSON data
        json_pattern = r'```json\s*\n(.*?)\n```'
        json_match = re.search(json_pattern, response_text, re.DOTALL)
        if json_match:
            try:
                architecture["json_data"] = json.loads(json_match.group(1))
            except json.JSONDecodeError:
                architecture["json_data"] = {}
        
        return architecture
    
    def generate_component_specs(self, architecture_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Generate detailed specifications for each component"""
        
        component_specs = []
        
        for component in architecture_data.get("components", []):
            spec = {
                "name": component["name"],
                "purpose": component["purpose"],
                "technologies": self._suggest_technologies_for_component(component["name"]),
                "interfaces": self._define_component_interfaces(component["name"]),
                "dependencies": self._identify_dependencies(component["name"], architecture_data)
            }
            component_specs.append(spec)
        
        return component_specs
    
    def _suggest_technologies_for_component(self, component_name: str) -> List[str]:
        """Suggest appropriate technologies for a component"""
        name_lower = component_name.lower()
        
        if any(term in name_lower for term in ['ui', 'interface', 'frontend', 'view']):
            return ['HTML', 'CSS', 'JavaScript']
        elif any(term in name_lower for term in ['api', 'server', 'backend', 'service']):
            return ['Python', 'Node.js', 'Express']
        elif any(term in name_lower for term in ['database', 'storage', 'data']):
            return ['SQLite', 'PostgreSQL', 'MongoDB']
        elif any(term in name_lower for term in ['auth', 'security', 'login']):
            return ['JWT', 'OAuth', 'bcrypt']
        else:
            return ['JavaScript', 'Python']
    
    def _define_component_interfaces(self, component_name: str) -> Dict[str, List[str]]:
        """Define interfaces for component communication"""
        return {
            "inputs": ["data", "user_actions", "events"],
            "outputs": ["responses", "updates", "notifications"],
            "methods": ["initialize", "process", "validate", "cleanup"]
        }
    
    def _identify_dependencies(self, component_name: str, architecture_data: Dict[str, Any]) -> List[str]:
        """Identify dependencies between components"""
        all_components = [comp["name"] for comp in architecture_data.get("components", [])]
        # Simple dependency logic - can be enhanced
        return [comp for comp in all_components if comp != component_name]

# Factory function for FALCON bird
def create_falcon_architect() -> FalconArchitect:
    """Factory function to create FALCON architect instance"""
    return FalconArchitect()

# Test function for FALCON bird
def test_falcon_bird():
    """Test the FALCON bird with sample SPARK input"""
    falcon = create_falcon_architect()
    
    # Mock SPARK requirements
    spark_requirements = {
        "raw_response": "Build a snake game with HTML, CSS, and JavaScript",
        "analysis": {
            "core_objective": "Create an interactive snake game",
            "in_scope": ["Game mechanics", "Score tracking", "Visual interface"],
            "complexity": "simple"
        }
    }
    
    design = falcon.design_architecture(spark_requirements)
    
    print("🧪 TESTING FALCON BIRD")
    print(f"🦅 Stage: {design['stage']}")
    print(f"🤖 Model: {design['model']}")
    print(f"🏗️ Design Type: {design['design_type']}")
    print(f"📏 Prompt Length: {len(design['prompt'])} characters")
    
    return design

if __name__ == "__main__":
    # Test FALCON bird independently
    test_falcon_bird()

╔═══━━━─── • ───━━━═══╗
--- File: hawk.py ---
╚═══━━━─── • ───━━━═══╝

#!/usr/bin/env python3
"""
hawk.py - HAWK Quality Assurance Bird
The QA specialist who ensures code quality and creates comprehensive testing strategies
"""

import json
import re
from typing import Dict, List, Any

class HawkQASpecialist:
    
    def create_qa_strategy(self, eagle_implementation: Dict[str, Any]) -> Dict[str, Any]:
        """
        Method expected by OUT-HOMING orchestrator
        """
        user_request = eagle_implementation.get("user_request", "")
        eagle_results = eagle_implementation.get("response", "")
        prompt = self.create_qa_strategy_prompt(user_request, eagle_results)
        return {
            "stage": "HAWK",
            "prompt": prompt,
            "model": "meta-llama/llama-4-maverick-17b-128e-instruct",
            "temperature": 0.3,
            "max_tokens": 1024
        }

    def create_qa_strategy_prompt(self, user_request: str, eagle_results: str) -> str:
        """
        Generate QA strategy prompt - FIXED METHOD
        """
        return self.analyze_implementation_prompt(user_request, eagle_results)

    def analyze_implementation_prompt(self, user_request: str, eagle_results: str) -> str:
        """
        Build comprehensive QA prompt
        """
        return f"""Act as Hawk, a senior QA engineer. Create comprehensive QA strategy.

Project Request: {user_request}

Implementation Details:
{eagle_results}

Provide QA strategy in this format:

**1. Test Cases:**
- Core functionality tests
- Edge case scenarios
- Integration requirements

**2. Security Validation:**
- Input validation checks
- Authentication requirements
- Data protection measures

**3. Performance Considerations:**
- Load testing needs
- Scalability checkpoints
- Resource optimization

**4. Error Handling:**
- Network failure handling
- Data corruption recovery
- User error management

**5. Production Readiness:**
- Deployment requirements
- Monitoring setup
- Backup strategies

Be specific and actionable."""
    """HAWK - The Quality Assurance Master"""
    
    def __init__(self):
        self.stage_name = "HAWK"
        self.icon = "🦅"
        self.specialty = "Quality Assurance & Testing Strategy"
        self.optimal_model = "gemma2-9b-it"  # QA structure specialist
    
    def analyze_implementation(self, eagle_implementation: Dict[str, Any]) -> Dict[str, Any]:
        """
        Main HAWK function - analyze code quality and create testing strategy
        """
        print(f"🦅 HAWK QA SPECIALIST: Analyzing code quality and creating test strategy...")
        
        # Extract implementation data
        implementation_text = eagle_implementation.get("raw_implementation", "")
        code_files = eagle_implementation.get("code_files", [])
        
        # Generate the HAWK QA prompt
        hawk_prompt = self._build_hawk_prompt(implementation_text, code_files)
        
        # Package the QA analysis for MCP processing
        hawk_analysis = {
            "stage": "HAWK",
            "prompt": hawk_prompt,
            "eagle_input": eagle_implementation,
            "model": self.optimal_model,
            "temperature": 0.3,
            "max_tokens": 1024,
            "analysis_type": "quality_assurance"
        }
        
        return hawk_analysis
    
    def _build_hawk_prompt(self, implementation_text: str, code_files: List[Dict[str, Any]]) -> str:
        """Build the QA analysis and testing strategy prompt"""
        
        files_summary = self._generate_files_summary(code_files)
        
        return f"""<thinking>
I need to analyze the implementation from Eagle and create a comprehensive QA strategy. I should look at:
- Code quality and best practices
- Security considerations
- Performance implications
- Testing requirements
- Deployment readiness

Implementation: {implementation_text[:500]}...
Files: {files_summary}
</thinking>

Act as Hawk, a senior QA engineer. Create comprehensive QA strategy for this implementation.

Implementation Details:
{implementation_text}

Provide QA strategy in this EXACT format:

**1. Test Cases:**
- Functional tests for core features
- Edge cases and error scenarios
- Integration test requirements

**2. Security Validation:**
- Authentication/authorization checks
- Input validation requirements
- Data protection measures

**3. Performance Considerations:**
- Load testing requirements
- Scalability checkpoints
- Resource optimization

**4. Error Handling Scenarios:**
- Network failure handling
- Data corruption recovery
- User error management

**5. Production Readiness Checklist:**
- Deployment requirements
- Monitoring setup
- Backup strategies

Then provide the structured data as JSON:
```json
{{
    "test_coverage": 85,
    "security_score": 9,
    "performance_rating": "good",
    "production_ready": true,
    "confidence_score": 8
}}
```

Be specific and actionable for each area."""
        
        return prompt
    
    def _generate_files_summary(self, code_files: List[Dict[str, Any]]) -> str:
        """Generate a summary of code files for the prompt"""
        if not code_files:
            return "No code files provided"
        
        summary_parts = []
        for file_data in code_files:
            summary_parts.append(f"{file_data['filename']} ({file_data['language']}, {file_data['lines']} lines)")
        
        return ", ".join(summary_parts)
    
    def validate_hawk_response(self, response_text: str) -> Dict[str, Any]:
        """Validate that HAWK response contains comprehensive QA analysis"""
        
        validation_result = {
            "valid": False,
            "has_test_cases": False,
            "has_security": False,
            "has_performance": False,
            "has_error_handling": False,
            "has_production_checklist": False,
            "has_json": False,
            "character_count": len(response_text),
            "quality_score": 0
        }
        
        # Check for test cases
        if "1. Test Cases:" in response_text:
            validation_result["has_test_cases"] = True
            validation_result["quality_score"] += 2
        
        # Check for security validation
        if "2. Security Validation:" in response_text:
            validation_result["has_security"] = True
            validation_result["quality_score"] += 2
        
        # Check for performance considerations
        if "3. Performance Considerations:" in response_text:
            validation_result["has_performance"] = True
            validation_result["quality_score"] += 2
        
        # Check for error handling
        if "4. Error Handling Scenarios:" in response_text:
            validation_result["has_error_handling"] = True
            validation_result["quality_score"] += 1
        
        # Check for production readiness
        if "5. Production Readiness Checklist:" in response_text:
            validation_result["has_production_checklist"] = True
            validation_result["quality_score"] += 2
        
        # Check for JSON data
        json_pattern = r'```json\s*\n(.*?)\n```'
        json_match = re.search(json_pattern, response_text, re.DOTALL)
        if json_match:
            try:
                json.loads(json_match.group(1))
                validation_result["has_json"] = True
                validation_result["quality_score"] += 2
            except json.JSONDecodeError:
                pass
        
        # Determine if valid
        validation_result["valid"] = (
            validation_result["has_test_cases"] and 
            validation_result["has_security"] and
            validation_result["has_performance"] and
            validation_result["character_count"] > 400
        )
        
        return validation_result
    
    def extract_qa_data(self, response_text: str) -> Dict[str, Any]:
        """Extract structured QA data from HAWK response"""
        
        qa_analysis = {
            "test_cases": [],
            "security_validation": [],
            "performance_considerations": [],
            "error_handling": [],
            "production_checklist": [],
            "json_data": {},
            "raw_analysis": response_text
        }
        
        # Extract test cases
        test_section = re.search(r'\*\*1\. Test Cases:\*\*\s*\n((?:- [^\n]+\n?)+)', response_text)
        if test_section:
            tests = re.findall(r'- ([^\n]+)', test_section.group(1))
            qa_analysis["test_cases"] = [test.strip() for test in tests]
        
        # Extract security validation
        security_section = re.search(r'\*\*2\. Security Validation:\*\*\s*\n((?:- [^\n]+\n?)+)', response_text)
        if security_section:
            security_items = re.findall(r'- ([^\n]+)', security_section.group(1))
            qa_analysis["security_validation"] = [item.strip() for item in security_items]
        
        # Extract performance considerations
        perf_section = re.search(r'\*\*3\. Performance Considerations:\*\*\s*\n((?:- [^\n]+\n?)+)', response_text)
        if perf_section:
            perf_items = re.findall(r'- ([^\n]+)', perf_section.group(1))
            qa_analysis["performance_considerations"] = [item.strip() for item in perf_items]
        
        # Extract error handling
        error_section = re.search(r'\*\*4\. Error Handling Scenarios:\*\*\s*\n((?:- [^\n]+\n?)+)', response_text)
        if error_section:
            error_items = re.findall(r'- ([^\n]+)', error_section.group(1))
            qa_analysis["error_handling"] = [item.strip() for item in error_items]
        
        # Extract production checklist
        prod_section = re.search(r'\*\*5\. Production Readiness Checklist:\*\*\s*\n((?:- [^\n]+\n?)+)', response_text)
        if prod_section:
            prod_items = re.findall(r'- ([^\n]+)', prod_section.group(1))
            qa_analysis["production_checklist"] = [item.strip() for item in prod_items]
        
        # Extract JSON data
        json_pattern = r'```json\s*\n(.*?)\n```'
        json_match = re.search(json_pattern, response_text, re.DOTALL)
        if json_match:
            try:
                qa_analysis["json_data"] = json.loads(json_match.group(1))
            except json.JSONDecodeError:
                qa_analysis["json_data"] = {}
        
        return qa_analysis
    
    def generate_test_suite(self, qa_data: Dict[str, Any], code_files: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Generate automated test suite based on QA analysis"""
        
        test_suite = {
            "unit_tests": [],
            "integration_tests": [],
            "e2e_tests": [],
            "performance_tests": [],
            "security_tests": []
        }
        
        # Generate unit tests based on code files
        for file_data in code_files:
            if file_data["language"] == "javascript":
                test_suite["unit_tests"].extend(
                    self._generate_js_unit_tests(file_data)
                )
            elif file_data["language"] == "python":
                test_suite["unit_tests"].extend(
                    self._generate_python_unit_tests(file_data)
                )
        
        # Generate integration tests
        if len(code_files) > 1:
            test_suite["integration_tests"] = [
                "Test component communication",
                "Test data flow between modules",
                "Test API integration points"
            ]
        
        # Generate E2E tests for web applications
        if any(file_data["language"] == "html" for file_data in code_files):
            test_suite["e2e_tests"] = [
                "Test complete user workflows",
                "Test cross-browser compatibility",
                "Test responsive design on different devices"
            ]
        
        # Generate performance tests
        test_suite["performance_tests"] = [
            "Load testing with simulated users",
            "Memory usage profiling",
            "Response time benchmarking"
        ]
        
        # Generate security tests
        test_suite["security_tests"] = qa_data.get("security_validation", [])
        
        return test_suite
    
    def _generate_js_unit_tests(self, file_data: Dict[str, Any]) -> List[str]:
        """Generate JavaScript unit test suggestions"""
        return [
            f"Test {file_data['filename']} function exports",
            f"Test {file_data['filename']} error handling",
            f"Test {file_data['filename']} input validation"
        ]
    
    def _generate_python_unit_tests(self, file_data: Dict[str, Any]) -> List[str]:
        """Generate Python unit test suggestions"""
        return [
            f"Test {file_data['filename']} class methods",
            f"Test {file_data['filename']} exception handling",
            f"Test {file_data['filename']} edge cases"
        ]
    
    def calculate_quality_metrics(self, qa_data: Dict[str, Any], code_files: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Calculate overall quality metrics for the implementation"""
        
        metrics = {
            "overall_score": 0,
            "test_coverage_estimate": 0,
            "security_rating": "unknown",
            "maintainability_score": 0,
            "performance_rating": "unknown",
            "production_readiness": False
        }
        
        # Calculate based on QA analysis completeness
        json_data = qa_data.get("json_data", {})
        
        if "test_coverage" in json_data:
            metrics["test_coverage_estimate"] = json_data["test_coverage"]
        
        if "security_score" in json_data:
            score = json_data["security_score"]
            if score >= 8:
                metrics["security_rating"] = "excellent"
            elif score >= 6:
                metrics["security_rating"] = "good"
            elif score >= 4:
                metrics["security_rating"] = "fair"
            else:
                metrics["security_rating"] = "poor"
        
        if "performance_rating" in json_data:
            metrics["performance_rating"] = json_data["performance_rating"]
        
        if "production_ready" in json_data:
            metrics["production_readiness"] = json_data["production_ready"]
        
        # Calculate maintainability based on code structure
        total_lines = sum(file_data["lines"] for file_data in code_files)
        file_count = len(code_files)
        
        if file_count > 0:
            avg_lines_per_file = total_lines / file_count
            if avg_lines_per_file < 100:
                metrics["maintainability_score"] = 9
            elif avg_lines_per_file < 200:
                metrics["maintainability_score"] = 7
            elif avg_lines_per_file < 300:
                metrics["maintainability_score"] = 5
            else:
                metrics["maintainability_score"] = 3
        
        # Calculate overall score
        scores = [
            metrics["test_coverage_estimate"] / 10,  # Convert to 0-10 scale
            json_data.get("security_score", 5),
            metrics["maintainability_score"],
            8 if metrics["performance_rating"] == "excellent" else 
            6 if metrics["performance_rating"] == "good" else 4
        ]
        
        metrics["overall_score"] = sum(scores) / len(scores)
        
        return metrics

# Factory function for HAWK bird
def create_hawk_qa_specialist() -> HawkQASpecialist:
    """Factory function to create HAWK QA specialist instance"""
    return HawkQASpecialist()

# Test function for HAWK bird
def test_hawk_bird():
    """Test the HAWK bird with sample EAGLE input"""
    hawk = create_hawk_qa_specialist()
    
    # Mock EAGLE implementation
    eagle_implementation = {
        "raw_implementation": """
IMPLEMENTATION OVERVIEW:
Complete snake game with HTML5 canvas, CSS styling, and JavaScript game logic.

CODE FILES:
- index.html (50 lines)
- styles.css (75 lines) 
- script.js (150 lines)

IMPLEMENTATION NOTES:
- Used HTML5 Canvas for game rendering
- Implemented collision detection
- Added score tracking system
        """,
        "code_files": [
            {"filename": "index.html", "language": "html", "lines": 50, "size": 1200},
            {"filename": "styles.css", "language": "css", "lines": 75, "size": 1800},
            {"filename": "script.js", "language": "javascript", "lines": 150, "size": 4500}
        ]
    }
    
    analysis = hawk.analyze_implementation(eagle_implementation)
    
    print("🧪 TESTING HAWK BIRD")
    print(f"🦅 Stage: {analysis['stage']}")
    print(f"🤖 Model: {analysis['model']}")
    print(f"🔍 Analysis Type: {analysis['analysis_type']}")
    print(f"📏 Prompt Length: {len(analysis['prompt'])} characters")
    
    return analysis

if __name__ == "__main__":
    # Test HAWK bird independently
    test_hawk_bird()
    def create_qa_strategy(self, eagle_implementation: Dict[str, Any]) -> Dict[str, Any]:
        """
        Method expected by OUT-HOMING orchestrator
        """
        user_request = eagle_implementation.get("user_request", "")
        eagle_results = eagle_implementation.get("response", "")
        prompt = self.create_qa_strategy_prompt(user_request, eagle_results)
        return {
            "stage": "HAWK",
            "prompt": prompt,
            "model": "meta-llama/llama-4-maverick-17b-128e-instruct",
            "temperature": 0.3,
            "max_tokens": 1024
        }

    def create_qa_strategy_prompt(self, user_request: str, eagle_results: str) -> str:
        """
        Generate QA strategy prompt - FIXED METHOD for OUT-HOMING
        """
        return f"""Act as Hawk, a senior QA engineer. Create comprehensive QA strategy.

Project Request: {user_request}

Implementation Details:
{eagle_results}

Provide QA strategy in this format:

**1. Test Cases:**
- Core functionality tests
- Edge case scenarios  
- Integration requirements

**2. Security Validation:**
- Input validation checks
- Authentication requirements
- Data protection measures

**3. Performance Considerations:**
- Load testing needs
- Scalability checkpoints
- Resource optimization

**4. Error Handling:**
- Network failure handling
- Data corruption recovery
- User error management

**5. Production Readiness:**
- Deployment requirements
- Monitoring setup
- Backup strategies

Be specific and actionable."""


╔══════════•⊱✦⊰•══════════╗
--- File: in_homing.py ---
╚══════════•⊱✦⊰•══════════╝

#!/usr/bin/env python3
"""
in-homing.py - IN-HOMING Response Processing & XEdit Generation Bird
Handles LLM2 responses coming back IN and creates the final XEdit interface
"""

import json
import re
import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

class InHomingProcessor:
    """IN-HOMING - The Response Handler & XEdit Generator"""
    
    def __init__(self):
        self.stage_name = "IN-HOMING"
        self.icon = "🔄"
        self.specialty = "LLM2 Response Processing & XEdit Generation"
        self.session_timestamp = self._generate_session_timestamp()
    
    def process_llm2_response(self, llm2_response: str, pipeline_metadata: Dict[str, Any]) -> Dict[str, Any]:
        """
        Main IN-HOMING function - process LLM2 response and generate XEdit interface
        """
        print(f"🔄 IN-HOMING: Processing LLM2 response and generating XEdit...")
        
        processing_result = {
            "success": False,
            "llm2_response": llm2_response,
            "pipeline_metadata": pipeline_metadata,
            "parsed_data": {},
            "xedit_interface": None,
            "xedit_paths": {},
            "project_files": [],
            "session_timestamp": self.session_timestamp,
            "processing_timestamp": datetime.datetime.now().isoformat(),
            "error": None
        }
        
        try:
            # Parse the LLM2 response
            processing_result["parsed_data"] = self._parse_llm2_response(llm2_response)
            
            # Extract code files
            processing_result["project_files"] = self._extract_project_files(processing_result["parsed_data"])
            
            # Generate XEdit paths
            processing_result["xedit_paths"] = self._generate_xedit_paths(processing_result["project_files"])
            
            # Generate XEdit interface
            processing_result["xedit_interface"] = self._generate_xedit_interface(
                processing_result["parsed_data"],
                processing_result["xedit_paths"],
                pipeline_metadata
            )
            
            # Save XEdit interface to file
            xedit_file_path = self._save_xedit_interface(
                processing_result["xedit_interface"],
                pipeline_metadata.get("project_name", "project")
            )
            
            processing_result["xedit_file_path"] = str(xedit_file_path)
            processing_result["success"] = True
            
            print(f"✅ IN-HOMING: Processing completed successfully!")
            print(f"📁 Generated: {len(processing_result['project_files'])} files")
            print(f"🎯 XEdit Paths: {len(processing_result['xedit_paths'])}")
            print(f"💾 Saved: {xedit_file_path}")
            
        except Exception as e:
            processing_result["error"] = str(e)
            processing_result["success"] = False
            print(f"❌ IN-HOMING: Processing failed - {e}")
        
        return processing_result
    
    def _parse_llm2_response(self, response_text: str) -> Dict[str, Any]:
        """Parse the LLM2 response into structured data"""
        
        parsed_data = {
            "project_overview": "",
            "code_files": [],
            "implementation_notes": [],
            "testing_checklist": [],
            "raw_response": response_text
        }
        
        # Extract project overview
        overview_match = re.search(r'\*\*PROJECT OVERVIEW:\*\*\s*\n([^\n*]+(?:\n[^\n*]+)*)', response_text)
        if overview_match:
            parsed_data["project_overview"] = overview_match.group(1).strip()
        
        # Extract code files
        code_files = self._extract_code_blocks_with_filenames(response_text)
        parsed_data["code_files"] = code_files
        
        # Extract implementation notes
        notes_match = re.search(r'\*\*IMPLEMENTATION NOTES:\*\*\s*\n((?:- [^\n]+\n?)+)', response_text)
        if notes_match:
            notes = re.findall(r'- ([^\n]+)', notes_match.group(1))
            parsed_data["implementation_notes"] = [note.strip() for note in notes]
        
        # Extract testing checklist
        testing_match = re.search(r'\*\*TESTING CHECKLIST:\*\*\s*\n((?:- [^\n]+\n?)+)', response_text)
        if testing_match:
            tests = re.findall(r'- ([^\n]+)', testing_match.group(1))
            parsed_data["testing_checklist"] = [test.strip() for test in tests]
        
        print(f"📝 Parsed: {len(parsed_data['code_files'])} files, {len(parsed_data['implementation_notes'])} notes")
        return parsed_data
    
    def _extract_code_blocks_with_filenames(self, response_text: str) -> List[Dict[str, Any]]:
        """Extract code blocks with filenames from response"""
        
        code_files = []
        
        # Pattern for filename-based code blocks
        filename_pattern = r'```filename:\s*([^\n]+)\n(.*?)\n```'
        filename_matches = re.findall(filename_pattern, response_text, re.DOTALL)
        
        for filename, code in filename_matches:
            file_data = {
                "filename": filename.strip(),
                "code": code.strip(),
                "language": self._detect_language(filename.strip()),
                "size": len(code.strip()),
                "lines": len(code.strip().split('\n'))
            }
            code_files.append(file_data)
            print(f"📄 Found: {file_data['filename']} ({file_data['language']}, {file_data['lines']} lines)")
        
        return code_files
    
    def _extract_project_files(self, parsed_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Convert parsed data to project files format"""
        return parsed_data.get("code_files", [])
    
    def _generate_xedit_paths(self, project_files: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
        """Generate 7x001 style XEdit paths for all code elements"""
        
        xedit_paths = {}
        path_counter = 1
        
        for file_data in project_files:
            filename = file_data["filename"]
            language = file_data["language"]
            code = file_data["code"]
            
            print(f"🔍 Analyzing {filename} ({language}) for XEdit paths...")
            
            # Parse code elements in this file
            code_elements = self._parse_code_elements(code, language, filename)
            
            for element in code_elements:
                xedit_id = f"7x{path_counter:03d}"
                
                xedit_paths[xedit_id] = {
                    "display_name": element["name"],
                    "type": element["type"],
                    "filename": filename,
                    "language": language,
                    "line_start": element["line_start"],
                    "line_end": element["line_end"],
                    "lines_display": f"{element['line_start']}-{element['line_end']}",
                    "technical_path": f"{filename}::{element['type']}.{element['name']}/lines[{element['line_start']}-{element['line_end']}]",
                    "optimal_model": self._select_optimal_model(element["type"], language)
                }
                
                path_counter += 1
                print(f"  🎯 {xedit_id}: {element['name']} ({element['type']})")
        
        print(f"✅ Generated {len(xedit_paths)} XEdit paths")
        return xedit_paths
    
    def _parse_code_elements(self, code: str, language: str, filename: str) -> List[Dict[str, Any]]:
        """Parse functions, classes, and other code elements"""
        elements = []
        lines = code.split('\n')
        
        if language == 'python':
            elements.extend(self._parse_python_elements(lines))
        elif language in ['javascript', 'js']:
            elements.extend(self._parse_javascript_elements(lines))
        elif language == 'html':
            elements.extend(self._parse_html_elements(lines))
        elif language == 'css':
            elements.extend(self._parse_css_elements(lines))
        else:
            # Generic parsing for other languages
            elements.extend(self._parse_generic_elements(lines))
        
        return elements
    
    def _parse_python_elements(self, lines: List[str]) -> List[Dict[str, Any]]:
        """Parse Python functions and classes"""
        elements = []
        
        for i, line in enumerate(lines, 1):
            # Function definitions
            func_match = re.match(r'^(\s*)def\s+(\w+)\s*\(', line)
            if func_match:
                elements.append({
                    "name": func_match.group(2),
                    "type": "function",
                    "line_start": i,
                    "line_end": min(i + 20, len(lines))
                })
            
            # Class definitions
            class_match = re.match(r'^(\s*)class\s+(\w+)', line)
            if class_match:
                elements.append({
                    "name": class_match.group(2),
                    "type": "class",
                    "line_start": i,
                    "line_end": min(i + 50, len(lines))
                })
        
        return elements
    
    def _parse_javascript_elements(self, lines: List[str]) -> List[Dict[str, Any]]:
        """Parse JavaScript functions and classes"""
        elements = []
        
        for i, line in enumerate(lines, 1):
            # Function declarations
            func_match = re.search(r'function\s+(\w+)\s*\(', line)
            if func_match:
                elements.append({
                    "name": func_match.group(1),
                    "type": "function",
                    "line_start": i,
                    "line_end": min(i + 15, len(lines))
                })
            
            # Arrow functions and const assignments
            arrow_match = re.search(r'(?:const|let|var)\s+(\w+)\s*=\s*(?:\([^)]*\)\s*=>|function)', line)
            if arrow_match:
                elements.append({
                    "name": arrow_match.group(1),
                    "type": "function",
                    "line_start": i,
                    "line_end": min(i + 10, len(lines))
                })
            
            # Class definitions
            class_match = re.search(r'class\s+(\w+)', line)
            if class_match:
                elements.append({
                    "name": class_match.group(1),
                    "type": "class",
                    "line_start": i,
                    "line_end": min(i + 30, len(lines))
                })
            
            # Method definitions (inside classes)
            method_match = re.search(r'^\s+(\w+)\s*\([^)]*\)\s*\{', line)
            if method_match and not line.strip().startswith('//'):
                elements.append({
                    "name": method_match.group(1),
                    "type": "method",
                    "line_start": i,
                    "line_end": min(i + 12, len(lines))
                })
        
        return elements
    
    def _parse_html_elements(self, lines: List[str]) -> List[Dict[str, Any]]:
        """Parse HTML elements"""
        elements = []
        
        for i, line in enumerate(lines, 1):
            # Major HTML tags with IDs
            id_match = re.search(r'<(div|section|header|footer|main|nav|article)\s*[^>]*id=["\']([^"\']+)["\']', line)
            if id_match:
                elements.append({
                    "name": id_match.group(2),
                    "type": f"html_{id_match.group(1)}",
                    "line_start": i,
                    "line_end": min(i + 5, len(lines))
                })
            
            # Major HTML tags with classes
            class_match = re.search(r'<(div|section|header|footer|main|nav)\s*[^>]*class=["\']([^"\']+)["\']', line)
            if class_match:
                elements.append({
                    "name": class_match.group(2).split()[0],  # First class name
                    "type": f"html_{class_match.group(1)}",
                    "line_start": i,
                    "line_end": min(i + 5, len(lines))
                })
        
        return elements
    
    def _parse_css_elements(self, lines: List[str]) -> List[Dict[str, Any]]:
        """Parse CSS classes and IDs"""
        elements = []
        
        for i, line in enumerate(lines, 1):
            # CSS classes
            class_match = re.search(r'\.([a-zA-Z][\w-]*)\s*\{', line)
            if class_match:
                elements.append({
                    "name": class_match.group(1),
                    "type": "css_class",
                    "line_start": i,
                    "line_end": min(i + 10, len(lines))
                })
            
            # CSS IDs
            id_match = re.search(r'#([a-zA-Z][\w-]*)\s*\{', line)
            if id_match:
                elements.append({
                    "name": id_match.group(1),
                    "type": "css_id",
                    "line_start": i,
                    "line_end": min(i + 10, len(lines))
                })
        
        return elements
    
    def _parse_generic_elements(self, lines: List[str]) -> List[Dict[str, Any]]:
        """Generic parsing for unknown languages"""
        elements = []
        
        for i, line in enumerate(lines, 1):
            # Generic function-like patterns
            if re.search(r'\w+\s*\([^)]*\)\s*\{', line):
                func_match = re.search(r'(\w+)\s*\(', line)
                if func_match:
                    elements.append({
                        "name": func_match.group(1),
                        "type": "function",
                        "line_start": i,
                        "line_end": min(i + 10, len(lines))
                    })
        
        return elements
    
    def _generate_xedit_interface(self, parsed_data: Dict[str, Any], xedit_paths: Dict[str, Dict[str, Any]], pipeline_metadata: Dict[str, Any]) -> str:
        """Generate complete XEdit HTML interface"""
        
        project_name = pipeline_metadata.get("project_name", "Unknown Project")
        
        # Combine all code for display
        combined_code = self._combine_code_for_display(parsed_data["code_files"])
        
        # Generate functions list HTML
        functions_html = self._generate_functions_html(xedit_paths)
        
        # Generate code display HTML with line numbers
        code_html = self._generate_code_html(combined_code)
        
        html_content = f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>🦚 Peacock XEdit Interface - {project_name}</title>
    <style>
        * {{ margin: 0; padding: 0; box-sizing: border-box; }}
        body {{ font-family: 'SF Mono', monospace; background: #0d1117; color: #e6edf3; height: 100vh; overflow: hidden; }}
        
        .header {{ background: #161b22; border-bottom: 1px solid #30363d; padding: 12px 20px; display: flex; justify-content: space-between; align-items: center; }}
        .peacock-logo {{ font-size: 18px; font-weight: bold; color: #ff6b35; }}
        .project-info {{ color: #8b949e; font-size: 14px; }}
        .session-info {{ background: rgba(0, 255, 136, 0.1); border: 1px solid #00ff88; border-radius: 6px; padding: 4px 8px; font-size: 12px; color: #00ff88; }}
        
        .main-container {{ display: flex; height: calc(100vh - 60px); }}
        
        .left-panel {{ width: 320px; background: #161b22; border-right: 1px solid #30363d; display: flex; flex-direction: column; }}
        .panel-header {{ background: #21262d; padding: 12px 16px; border-bottom: 1px solid #30363d; font-weight: 600; font-size: 13px; color: #7c3aed; }}
        
        .functions-list {{ flex: 1; overflow-y: auto; padding: 8px; }}
        .function-item {{ background: #21262d; border: 1px solid #30363d; border-radius: 6px; padding: 12px; margin-bottom: 8px; cursor: pointer; transition: all 0.2s; position: relative; }}
        .function-item:hover {{ border-color: #ff6b35; background: #2d333b; transform: translateX(3px); }}
        .function-item.selected {{ border-color: #ff6b35; background: #2d333b; box-shadow: 0 0 0 1px #ff6b35; }}
        
        .function-info {{ display: flex; align-items: center; gap: 8px; flex-wrap: wrap; }}
        .function-name {{ font-weight: 600; color: #79c0ff; }}
        .function-type {{ background: #30363d; color: #8b949e; padding: 2px 6px; border-radius: 4px; font-size: 10px; }}
        .xedit-id {{ background: #238636; color: white; padding: 2px 6px; border-radius: 4px; font-size: 10px; font-weight: 600; }}
        .function-lines {{ color: #6e7681; font-size: 10px; }}
        
        .add-btn {{ position: absolute; right: 8px; top: 50%; transform: translateY(-50%); background: #238636; color: white; border: none; border-radius: 4px; width: 24px; height: 24px; cursor: pointer; font-weight: bold; }}
        .add-btn:hover {{ background: #2ea043; }}
        
        .middle-panel {{ width: 280px; background: #0d1117; border-right: 1px solid #30363d; display: flex; flex-direction: column; }}
        .payload-header {{ background: #21262d; padding: 12px 16px; border-bottom: 1px solid #30363d; font-weight: 600; font-size: 13px; color: #ff6b35; }}
        
        .payload-list {{ flex: 1; overflow-y: auto; padding: 8px; }}
        .payload-item {{ background: rgba(255, 107, 53, 0.1); border: 1px solid #ff6b35; border-radius: 6px; padding: 8px; margin-bottom: 6px; font-size: 12px; }}
        .payload-item .remove-btn {{ float: right; background: #da3633; color: white; border: none; border-radius: 3px; width: 18px; height: 18px; cursor: pointer; font-size: 10px; }}
        
        .send-btn {{ margin: 8px; padding: 12px; background: linear-gradient(45deg, #238636, #2ea043); color: white; border: none; border-radius: 6px; font-weight: 600; cursor: pointer; }}
        .send-btn:hover {{ background: linear-gradient(45deg, #2ea043, #238636); }}
        
        .right-panel {{ flex: 1; background: #0d1117; display: flex; flex-direction: column; }}
        .code-header {{ background: #21262d; padding: 12px 16px; border-bottom: 1px solid #30363d; font-weight: 600; font-size: 13px; color: #79c0ff; }}
        
        .code-container {{ flex: 1; overflow: auto; }}
        .code-line {{ display: flex; font-family: 'SF Mono', monospace; font-size: 13px; line-height: 1.4; }}
        .code-line:hover {{ background: rgba(255, 255, 255, 0.05); }}
        .line-number {{ width: 60px; padding: 4px 8px; background: #161b22; border-right: 1px solid #30363d; color: #6e7681; text-align: right; flex-shrink: 0; }}
        .line-content {{ padding: 4px 12px; flex: 1; white-space: pre; }}
        
        .highlighted {{ background: rgba(255, 107, 53, 0.2) !important; }}
    </style>
</head>
<body>
    <div class="header">
        <div class="peacock-logo">🦚 Peacock XEdit Interface</div>
        <div class="project-info">{project_name}</div>
        <div class="session-info">Session: {self.session_timestamp}</div>
    </div>
    
    <div class="main-container">
        <!-- Left Panel: Functions List -->
        <div class="left-panel">
            <div class="panel-header">📋 Functions & Classes ({len(xedit_paths)})</div>
            <div class="functions-list">
                {functions_html}
            </div>
        </div>
        
        <!-- Middle Panel: Payload -->
        <div class="middle-panel">
            <div class="payload-header">🎯 Payload</div>
            <div class="payload-list" id="payloadList">
                <div style="color: #6e7681; text-align: center; padding: 20px; font-size: 12px;">
                    Click functions to add XEdit-Paths
                </div>
            </div>
            <button class="send-btn" onclick="sendToLLM2()">🚀 Send to LLM2</button>
            <button class="deploy-btn" onclick="deployProject()" style="margin: 8px; padding: 12px; background: linear-gradient(45deg, #0969da, #1f6feb); color: white; border: none; border-radius: 6px; font-weight: 600; cursor: pointer;">🦚 PCOCK Deploy</button>
        </div>
        
        <!-- Right Panel: Code Display -->
        <div class="right-panel">
            <div class="code-header">💻 Generated Code ({len(parsed_data["code_files"])} files)</div>
            <div class="code-container">
                {code_html}
            </div>
        </div>
    </div>
    
    <script>
        let payloadItems = [];
        const xeditPaths = {json.dumps(xedit_paths)};
        
        function highlightFunction(xeditId) {{
            // Remove previous highlights
            document.querySelectorAll('.highlighted').forEach(el => {{
                el.classList.remove('highlighted');
            }});
            
            // Remove previous selection
            document.querySelectorAll('.function-item.selected').forEach(el => {{
                el.classList.remove('selected');
            }});
            
            // Add selection to clicked item
            event.currentTarget.classList.add('selected');
            
            // Highlight lines based on XEdit path data
            const pathData = xeditPaths[xeditId];
            if (pathData) {{
                for (let lineNum = pathData.line_start; lineNum <= pathData.line_end; lineNum++) {{
                    const lineElement = document.querySelector(`[data-line="${{lineNum}}"]`);
                    if (lineElement) {{
                        lineElement.classList.add('highlighted');
                    }}
                }}
                
                // Scroll to the highlighted section
                const firstHighlighted = document.querySelector('.highlighted');
                if (firstHighlighted) {{
                    firstHighlighted.scrollIntoView({{ behavior: 'smooth', block: 'center' }});
                }}
            }}
            
            console.log('Highlighted function:', xeditId, pathData);
        }}
        
        function addToPayload(xeditId) {{
            event.stopPropagation(); // Prevent highlighting when clicking add button
            
            if (payloadItems.includes(xeditId)) return;
            
            payloadItems.push(xeditId);
            updatePayloadDisplay();
            console.log('Added to payload:', xeditId);
        }}
        
        function removeFromPayload(xeditId) {{
            payloadItems = payloadItems.filter(item => item !== xeditId);
            updatePayloadDisplay();
            console.log('Removed from payload:', xeditId);
        }}
        
        function updatePayloadDisplay() {{
            const payloadList = document.getElementById('payloadList');
            
            if (payloadItems.length === 0) {{
                payloadList.innerHTML = '<div style="color: #6e7681; text-align: center; padding: 20px; font-size: 12px;">Click functions to add XEdit-Paths</div>';
                return;
            }}
            
            payloadList.innerHTML = payloadItems.map(xeditId => {{
                const pathData = xeditPaths[xeditId];
                const displayName = pathData ? pathData.display_name : xeditId;
                return `<div class="payload-item">
                    <strong>${{xeditId}}</strong><br>
                    ${{displayName}} (${{pathData ? pathData.type : 'unknown'}})
                    <button class="remove-btn" onclick="removeFromPayload('${{xeditId}}')">&times;</button>
                </div>`;
            }}).join('');
        }}
        
        function deployProject() {{
            console.log('🦚 PCOCK DEPLOY: Starting deployment...');
            
            // Send deploy request to MCP
            fetch('http://127.0.0.1:8000/deploy', {{
                method: 'POST',
                headers: {{
                    'Content-Type': 'application/json',
                }},
                body: JSON.stringify({{
                    project_name: '{project_name}',
                    action: 'deploy_and_run'
                }})
            }})
            .then(response => response.json())
            .then(data => {{
                if (data.success) {{
                    alert(`🦚 PCOCK DEPLOY SUCCESS!\\n\\n` +
                          `📁 Project: ${{data.project_name}}\\n` +
                          `📄 Files: ${{data.files_deployed}}\\n` +
                          `🌐 Running: ${{data.server_url}}\\n\\n` +
                          `Browser should open automatically!`);
                }} else {{
                    alert(`❌ PCOCK DEPLOY FAILED:\\n${{data.error}}`);
                }}
            }})
            .catch(error => {{
                console.error('Deploy error:', error);
                alert(`❌ Deploy request failed: ${{error.message}}`);
            }});
        }}
        
        function sendToLLM2() {{
            if (payloadItems.length === 0) {{
                alert('Please add some XEdit-Paths to the payload first');
                return;
            }}
            
            const payloadData = payloadItems.map(xeditId => xeditPaths[xeditId]);
            console.log('Sending to LLM2:', payloadData);
            
            alert(`Sending ${{payloadItems.length}} XEdit-Paths to LLM2 for optimization:\n\n${{payloadItems.join(', ')}}`);
            
            // In real implementation, would send to MCP server for LLM2 processing
        }}
        
        console.log('🦚 Peacock XEdit Interface Loaded');
        console.log('📁 Project:', '{project_name}');
        console.log('🔢 XEdit Paths:', {len(xedit_paths)});
        console.log('📄 Code Files:', {len(parsed_data["code_files"])});
        console.log('🎯 XEdit Paths Data:', xeditPaths);
    </script>
</body>
</html>"""
        
        return html_content
    
    def _combine_code_for_display(self, code_files: List[Dict[str, Any]]) -> str:
        """Combine all code files for display in interface"""
        combined = []
        
        for file_data in code_files:
            combined.append(f"// File: {file_data['filename']}")
            combined.append(f"// Language: {file_data['language']}")
            combined.append(f"// Size: {file_data['size']} characters")
            combined.append("")
            combined.append(file_data['code'])
            combined.append("")
            combined.append("// " + "="*60)
            combined.append("")
        
        return "\n".join(combined)
    
    def _generate_functions_html(self, xedit_paths: Dict[str, Dict[str, Any]]) -> str:
        """Generate HTML for functions list"""
        functions_html = ""
        
        for xedit_id, path_data in xedit_paths.items():
            icon = "🏗️" if path_data["type"] == "class" else "⚡"
            if "method" in path_data["type"]:
                icon = "🔧"
            elif "html" in path_data["type"]:
                icon = "🌐"
            elif "css" in path_data["type"]:
                icon = "🎨"
            
            functions_html += f"""
            <div class="function-item" onclick="highlightFunction('{xedit_id}')">
                <div class="function-info">
                    <span>{icon}</span>
                    <span class="function-name">{path_data['display_name']}</span>
                    <span class="function-type">{path_data['type']}</span>
                    <span class="xedit-id">{xedit_id}</span>
                    <div class="function-lines">Lines {path_data['lines_display']}</div>
                </div>
                <button class="add-btn" onclick="addToPayload('{xedit_id}')" title="Add to payload">+</button>
            </div>"""
        
        if not xedit_paths:
            functions_html = '<div style="color: #6e7681; text-align: center; padding: 20px;">No functions or classes found</div>'
        
        return functions_html
    
    def _generate_code_html(self, combined_code: str) -> str:
        """Generate HTML for code display with line numbers"""
        lines = combined_code.split('\n')
        code_html = ""
        
        for i, line in enumerate(lines, 1):
            escaped_line = line.replace('<', '&lt;').replace('>', '&gt;')
            code_html += f'<div class="code-line" data-line="{i}"><span class="line-number">{i:3d}</span><span class="line-content">{escaped_line}</span></div>\n'
        
        return code_html
    
    def _save_xedit_interface(self, html_content: str, project_name: str) -> Path:
        """Save XEdit interface to file"""
        output_dir = Path("/home/flintx/peacock/html")
        output_dir.mkdir(exist_ok=True)
        
        file_path = output_dir / f"xedit-{self.session_timestamp}.html"
        
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(html_content)
        
        print(f"💾 XEdit interface saved: {file_path}")
        return file_path
    
    def _detect_language(self, filename: str) -> str:
        """Detect programming language from filename"""
        ext_map = {
            '.html': 'html',
            '.css': 'css', 
            '.js': 'javascript',
            '.py': 'python',
            '.java': 'java',
            '.cpp': 'cpp',
            '.c': 'c',
            '.php': 'php',
            '.rb': 'ruby',
            '.go': 'go',
            '.rs': 'rust',
            '.ts': 'typescript',
            '.jsx': 'jsx',
            '.tsx': 'tsx'
        }
        
        for ext, lang in ext_map.items():
            if filename.lower().endswith(ext):
                return lang
        
        return 'text'
    
    def _select_optimal_model(self, element_type: str, language: str) -> str:
        """Select optimal model based on element type and language"""
        # Based on testing results
        if element_type == "class" or language in ["html", "css"]:
            return "gemma2-9b-it"  # Better structure handling
        else:
            return "llama-3.1-8b-instant"  # Better code analysis
    
    def _generate_session_timestamp(self) -> str:
        """Generate session timestamp matching MCP format - MILITARY TIME"""
        now = datetime.datetime.now()
        week = now.isocalendar()[1]
        day = now.day
        hour = now.hour  # Already 24-hour format
        minute = now.minute
        return f"{week:02d}-{day:02d}-{hour:02d}{minute:02d}"
    
    def generate_project_summary(self, processing_result: Dict[str, Any]) -> Dict[str, Any]:
        """Generate complete project summary"""
        
        summary = {
            "project_overview": processing_result["parsed_data"].get("project_overview", ""),
            "files_generated": len(processing_result["project_files"]),
            "xedit_paths_created": len(processing_result["xedit_paths"]),
            "total_lines_of_code": sum(file_data["lines"] for file_data in processing_result["project_files"]),
            "languages_used": list(set(file_data["language"] for file_data in processing_result["project_files"])),
            "implementation_notes": processing_result["parsed_data"].get("implementation_notes", []),
            "testing_checklist": processing_result["parsed_data"].get("testing_checklist", []),
            "xedit_interface_path": processing_result.get("xedit_file_path", ""),
            "session_info": {
                "timestamp": self.session_timestamp,
                "processing_time": processing_result["processing_timestamp"]
            }
        }
        
        return summary
    
    def deploy_project_files(self, project_files: List[Dict[str, Any]], project_name: str) -> Dict[str, Any]:
        """Deploy project files to local apps directory and start server"""
        print(f"🚀 IN-HOMING: Deploying {project_name}...")
        
        deploy_result = {
            "success": False,
            "project_path": "",
            "server_url": "",
            "files_deployed": 0,
            "error": None
        }
        
        try:
            # Create apps directory structure
            apps_dir = Path("/home/flintx/peacock/apps")
            apps_dir.mkdir(exist_ok=True)
            
            project_dir = apps_dir / project_name
            project_dir.mkdir(exist_ok=True)
            
            # Deploy all project files
            for file_data in project_files:
                file_path = project_dir / file_data["filename"]
                
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(file_data["code"])
                
                print(f"📄 Deployed: {file_data['filename']} ({file_data['size']} chars)")
                deploy_result["files_deployed"] += 1
            
            # Create project manifest
            manifest = {
                "name": project_name,
                "created": datetime.datetime.now().isoformat(),
                "files": [f["filename"] for f in project_files],
                "languages": list(set(f["language"] for f in project_files)),
                "total_lines": sum(f["lines"] for f in project_files),
                "session": self.session_timestamp
            }
            
            manifest_path = project_dir / "peacock.json"
            with open(manifest_path, "w", encoding="utf-8") as f:
                json.dump(manifest, f, indent=2)
            
            deploy_result["project_path"] = str(project_dir)
            deploy_result["server_url"] = f"http://localhost:8080"
            deploy_result["success"] = True
            
            print(f"✅ Deployed {deploy_result['files_deployed']} files to: {project_dir}")
            print(f"🌐 Ready to serve at: {deploy_result['server_url']}")
            
        except Exception as e:
            deploy_result["error"] = str(e)
            print(f"❌ Deploy failed: {e}")
        
        return deploy_result
    
    def start_local_server(self, project_name: str, port: int = 8080) -> Dict[str, Any]:
        """Start local HTTP server for deployed project"""
        import subprocess
        import webbrowser
        import time
        
        server_result = {
            "success": False,
            "server_url": "",
            "process_id": None,
            "error": None
        }
        
        try:
            project_dir = Path(f"/home/flintx/peacock/apps/{project_name}")
            
            if not project_dir.exists():
                server_result["error"] = f"Project {project_name} not found"
                return server_result
            
            # Start HTTP server in project directory
            server_cmd = [
                "python", "-m", "http.server", str(port), 
                "--directory", str(project_dir)
            ]
            
            print(f"🌐 Starting server: {' '.join(server_cmd)}")
            
            # Start server process (non-blocking)
            process = subprocess.Popen(
                server_cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                cwd=str(project_dir)
            )
            
            # Give server a moment to start
            time.sleep(1)
            
            server_url = f"http://localhost:{port}"
            
            # Open in browser
            print(f"🚀 Opening browser: {server_url}")
            webbrowser.open(server_url)
            
            server_result["success"] = True
            server_result["server_url"] = server_url
            server_result["process_id"] = process.pid
            
            print(f"✅ Server started successfully!")
            print(f"   🌐 URL: {server_url}")
            print(f"   🔢 PID: {process.pid}")
            print(f"   📁 Directory: {project_dir}")
            
        except Exception as e:
            server_result["error"] = str(e)
            print(f"❌ Server start failed: {e}")
        
        return server_result
    
    def deploy_and_run(self, project_files: List[Dict[str, Any]], project_name: str) -> Dict[str, Any]:
        """Complete deploy and run workflow"""
        print(f"🦚 PCOCK DEPLOY: {project_name}")
        
        # Deploy files
        deploy_result = self.deploy_project_files(project_files, project_name)
        
        if not deploy_result["success"]:
            return {
                "success": False,
                "error": f"Deploy failed: {deploy_result['error']}",
                "deploy_result": deploy_result
            }
        
        # Start server and open browser
        server_result = self.start_local_server(project_name)
        
        complete_result = {
            "success": server_result["success"],
            "project_name": project_name,
            "project_path": deploy_result["project_path"],
            "files_deployed": deploy_result["files_deployed"],
            "server_url": server_result["server_url"],
            "process_id": server_result.get("process_id"),
            "deploy_result": deploy_result,
            "server_result": server_result
        }
        
        if complete_result["success"]:
            print(f"🎉 PCOCK DEPLOY COMPLETE!")
            print(f"   📁 Project: {project_name}")
            print(f"   📄 Files: {deploy_result['files_deployed']}")
            print(f"   🌐 Running: {server_result['server_url']}")
        else:
            complete_result["error"] = server_result.get("error", "Unknown server error")
        
        return complete_result
    
    def validate_processing_quality(self, processing_result: Dict[str, Any]) -> Dict[str, Any]:
        """Validate the quality of processing results"""
        
        validation = {
            "overall_quality": "unknown",
            "code_files_valid": False,
            "xedit_paths_valid": False,
            "interface_generated": False,
            "recommendations": []
        }
        
        # Check code files
        if processing_result["project_files"] and len(processing_result["project_files"]) > 0:
            validation["code_files_valid"] = True
            
            # Check if files have content
            total_lines = sum(file_data["lines"] for file_data in processing_result["project_files"])
            if total_lines < 50:
                validation["recommendations"].append("Generated code seems very short - may need more implementation")
        
        # Check XEdit paths
        if processing_result["xedit_paths"] and len(processing_result["xedit_paths"]) > 0:
            validation["xedit_paths_valid"] = True
        else:
            validation["recommendations"].append("No XEdit paths generated - code may lack functions/classes")
        
        # Check interface generation
        if processing_result["xedit_interface"] and len(processing_result["xedit_interface"]) > 1000:
            validation["interface_generated"] = True
        else:
            validation["recommendations"].append("XEdit interface generation may have failed")
        
        # Overall quality assessment
        quality_score = 0
        if validation["code_files_valid"]:
            quality_score += 3
        if validation["xedit_paths_valid"]:
            quality_score += 2
        if validation["interface_generated"]:
            quality_score += 3
        
        if quality_score >= 7:
            validation["overall_quality"] = "excellent"
        elif quality_score >= 5:
            validation["overall_quality"] = "good"
        elif quality_score >= 3:
            validation["overall_quality"] = "fair"
        else:
            validation["overall_quality"] = "poor"
        
        return validation

# Factory function for IN-HOMING processor
def create_return_homing_processor() -> InHomingProcessor:
    """Factory function to create IN-HOMING processor instance"""
    return InHomingProcessor()

# Test function for IN-HOMING processor
def test_in_homing_processor():
    """Test the IN-HOMING processor with sample LLM2 response"""
    processor = create_return_homing_processor()
    
    # Mock LLM2 response
    sample_llm2_response = """**PROJECT OVERVIEW:**
Complete snake game implementation with HTML5 canvas, CSS styling, and JavaScript game logic.

**CODE FILES:**

```filename: index.html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Snake Game</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div id="gameContainer">
        <canvas id="gameCanvas" width="400" height="400"></canvas>
        <div id="score">Score: 0</div>
    </div>
    <script src="script.js"></script>
</body>
</html>
```

```filename: styles.css
body {
    margin: 0;
    padding: 0;
    display: flex;
    justify-content: center;
    align-items: center;
    min-height: 100vh;
    background-color: #2c3e50;
    font-family: Arial, sans-serif;
}

#gameContainer {
    text-align: center;
}

#gameCanvas {
    border: 2px solid #fff;
    background-color: #34495e;
}

#score {
    color: white;
    font-size: 24px;
    margin-top: 10px;
}
```

```filename: script.js
const canvas = document.getElementById('gameCanvas');
const ctx = canvas.getContext('2d');
const scoreElement = document.getElementById('score');

class SnakeGame {
    constructor() {
        this.snake = [{x: 200, y: 200}];
        this.food = this.generateFood();
        this.direction = {x: 0, y: 0};
        this.score = 0;
    }
    
    generateFood() {
        return {
            x: Math.floor(Math.random() * (canvas.width / 20)) * 20,
            y: Math.floor(Math.random() * (canvas.height / 20)) * 20
        };
    }
    
    update() {
        const head = {x: this.snake[0].x + this.direction.x, y: this.snake[0].y + this.direction.y};
        this.snake.unshift(head);
        
        if (head.x === this.food.x && head.y === this.food.y) {
            this.score++;
            this.food = this.generateFood();
        } else {
            this.snake.pop();
        }
        
        this.checkCollision();
    }
    
    checkCollision() {
        const head = this.snake[0];
        if (head.x < 0 || head.x >= canvas.width || head.y < 0 || head.y >= canvas.height) {
            this.gameOver();
        }
        
        for (let i = 1; i < this.snake.length; i++) {
            if (head.x === this.snake[i].x && head.y === this.snake[i].y) {
                this.gameOver();
            }
        }
    }
    
    gameOver() {
        alert('Game Over! Score: ' + this.score);
        this.snake = [{x: 200, y: 200}];
        this.direction = {x: 0, y: 0};
        this.score = 0;
        this.food = this.generateFood();
    }
    
    render() {
        ctx.clearRect(0, 0, canvas.width, canvas.height);
        
        // Draw snake
        ctx.fillStyle = '#27ae60';
        this.snake.forEach(segment => {
            ctx.fillRect(segment.x, segment.y, 20, 20);
        });
        
        // Draw food
        ctx.fillStyle = '#e74c3c';
        ctx.fillRect(this.food.x, this.food.y, 20, 20);
        
        // Update score
        scoreElement.textContent = 'Score: ' + this.score;
    }
}

const game = new SnakeGame();

function gameLoop() {
    game.update();
    game.render();
}

document.addEventListener('keydown', (e) => {
    switch(e.key) {
        case 'ArrowUp':
            if (game.direction.y === 0) game.direction = {x: 0, y: -20};
            break;
        case 'ArrowDown':
            if (game.direction.y === 0) game.direction = {x: 0, y: 20};
            break;
        case 'ArrowLeft':
            if (game.direction.x === 0) game.direction = {x: -20, y: 0};
            break;
        case 'ArrowRight':
            if (game.direction.x === 0) game.direction = {x: 20, y: 0};
            break;
    }
});

setInterval(gameLoop, 100);
```

**IMPLEMENTATION NOTES:**
- Used HTML5 Canvas for smooth game rendering
- Implemented collision detection for walls and self-collision
- Added keyboard controls for snake movement
- Simple scoring system with food consumption

**TESTING CHECKLIST:**
- Test arrow key controls for snake movement
- Verify collision detection works properly
- Check food generation and scoring
- Test game over and restart functionality"""

    # Mock pipeline metadata
    pipeline_metadata = {
        "project_name": "snake_game",
        "total_stages": 4,
        "pipeline_duration": "15.3 seconds"
    }
    
    print("🧪 TESTING IN-HOMING PROCESSOR")
    print("="*70)
    
    # Process the LLM2 response
    processing_result = processor.process_llm2_response(sample_llm2_response, pipeline_metadata)
    
    print("\n📊 PROCESSING RESULTS:")
    print(f"✅ Success: {processing_result['success']}")
    print(f"📁 Project Files: {len(processing_result['project_files'])}")
    print(f"🎯 XEdit Paths: {len(processing_result['xedit_paths'])}")
    
    if processing_result["xedit_interface"]:
        print(f"🌐 XEdit Interface: Generated ({len(processing_result['xedit_interface'])} characters)")
    
    if processing_result.get("xedit_file_path"):
        print(f"💾 Saved to: {processing_result['xedit_file_path']}")
    
    # Show XEdit paths generated
    if processing_result["xedit_paths"]:
        print(f"\n🎯 XEDIT PATHS GENERATED:")
        for xedit_id, path_data in processing_result["xedit_paths"].items():
            print(f"   {xedit_id}: {path_data['display_name']} ({path_data['type']}) - Lines {path_data['lines_display']}")
    
    # Generate project summary
    summary = processor.generate_project_summary(processing_result)
    print(f"\n📋 PROJECT SUMMARY:")
    print(f"   📄 Files: {summary['files_generated']}")
    print(f"   📊 Lines of Code: {summary['total_lines_of_code']}")
    print(f"   🔤 Languages: {', '.join(summary['languages_used'])}")
    print(f"   🎯 XEdit Paths: {summary['xedit_paths_created']}")
    
    # Validate quality
    validation = processor.validate_processing_quality(processing_result)
    print(f"\n🔍 QUALITY VALIDATION:")
    print(f"   📈 Overall Quality: {validation['overall_quality']}")
    print(f"   📝 Code Files Valid: {validation['code_files_valid']}")
    print(f"   🎯 XEdit Paths Valid: {validation['xedit_paths_valid']}")
    print(f"   🌐 Interface Generated: {validation['interface_generated']}")
    
    if validation["recommendations"]:
        print(f"   💡 Recommendations:")
        for rec in validation["recommendations"]:
            print(f"      • {rec}")
    
    if processing_result["error"]:
        print(f"❌ Error: {processing_result['error']}")
    
    return processing_result

if __name__ == "__main__":
    # Test IN-HOMING processor
    test_in_homing_processor()

class InHomingOrchestrator:
    """IN-HOMING orchestrator for processing MCP responses"""
    
    def __init__(self, mcp_client: Optional[Any] = None):
        self.mcp_client = mcp_client or MCPClient()
        self.logger = logging.getLogger(__name__)
    
    def process_mcp_response(self, response_text: str, project_context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process MCP response and update project context
        
        Args:
            response_text: Raw response text from MCP
            project_context: Current project context including previous stages
            
        Returns:
            Updated project context with MCP response integrated
        """
        self.logger.info("Starting IN-HOMING processing of MCP response")
        
        try:
            # Parse the MCP response
            parsed_response = self._parse_mcp_response(response_text)
            
            # Update project context with MCP response
            project_context['mcp_response'] = parsed_response
            
            # Determine which stage to update based on the response
            stage_to_update = self._determine_stage_to_update(parsed_response, project_context)
            
            if stage_to_update:
                self._update_stage_with_mcp_response(stage_to_update, parsed_response, project_context)
                self.logger.info(f"Updated {stage_to_update} with MCP response")
            
            # Generate feedback for the user
            feedback = self._generate_user_feedback(parsed_response, project_context)
            project_context['feedback'] = feedback
            
            # Update project status
            project_context['status'] = self._update_project_status(project_context)
            
            self.logger.info("IN-HOMING processing completed successfully")
            return project_context
            
        except Exception as e:
            self.logger.error(f"Error in IN-HOMING processing: {str(e)}", exc_info=True)
            return {
                'status': 'error',
                'error': str(e),
                'original_response': response_text
            }
    
    def _parse_mcp_response(self, response_text: str) -> Dict[str, Any]:
        """Parse the MCP response into a structured format"""
        try:
            # Try to extract JSON from the response
            json_match = re.search(r'```json\n(.*?)\n```', response_text, re.DOTALL)
            if json_match:
                return json.loads(json_match.group(1))
                
            # If no JSON found, try to parse as plain text
            return {
                'type': 'text_response',
                'content': response_text.strip()
            }
            
        except json.JSONDecodeError:
            # If JSON parsing fails, return as plain text
            return {
                'type': 'text_response',
                'content': response_text.strip()
            }
    
    def _determine_stage_to_update(self, parsed_response: Dict[str, Any], 
                                 project_context: Dict[str, Any]) -> Optional[str]:
        """Determine which project stage to update based on the MCP response"""
        # Check if response contains stage information
        if 'stage' in parsed_response:
            return parsed_response['stage']
            
        # Try to infer stage from response content
        content = parsed_response.get('content', '').lower()
        
        if any(term in content for term in ['requirements', 'analysis', 'spark']):
            return 'requirements_analysis'
            
        if any(term in content for term in ['architecture', 'design', 'falcon']):
            return 'architecture_design'
            
        if any(term in content for term in ['implementation', 'code', 'eagle']):
            return 'implementation'
            
        if any(term in content for term in ['test', 'qa', 'hawk']):
            return 'testing'
            
        return None
    
    def _update_stage_with_mcp_response(self, stage: str, parsed_response: Dict[str, Any], 
                                       project_context: Dict[str, Any]) -> None:
        """Update the specified stage with the MCP response"""
        if stage not in project_context:
            project_context[stage] = {}
            
        # Store the raw response
        project_context[stage]['mcp_response'] = parsed_response
        
        # Update stage status
        project_context[stage]['status'] = 'updated_with_mcp'
        project_context[stage]['last_updated'] = datetime.utcnow().isoformat()
        
        # Extract and store any specific updates
        if 'updates' in parsed_response:
            project_context[stage].update(parsed_response['updates'])
    
    def _generate_user_feedback(self, parsed_response: Dict[str, Any], 
                              project_context: Dict[str, Any]) -> Dict[str, Any]:
        """Generate user-friendly feedback from the MCP response"""
        feedback = {
            'summary': 'MCP response processed',
            'actions': [],
            'next_steps': [],
            'warnings': []
        }
        
        # Extract summary if available
        if 'summary' in parsed_response:
            feedback['summary'] = parsed_response['summary']
        
        # Extract actions
        if 'actions' in parsed_response and isinstance(parsed_response['actions'], list):
            feedback['actions'] = parsed_response['actions']
        
        # Extract next steps
        if 'next_steps' in parsed_response and isinstance(parsed_response['next_steps'], list):
            feedback['next_steps'] = parsed_response['next_steps']
        
        # Extract warnings
        if 'warnings' in parsed_response and isinstance(parsed_response['warnings'], list):
            feedback['warnings'] = parsed_response['warnings']
        
        # Add timestamp
        feedback['timestamp'] = datetime.utcnow().isoformat()
        
        return feedback
    
    def _update_project_status(self, project_context: Dict[str, Any]) -> str:
        """Update the overall project status based on current state"""
        # Check for errors first
        if 'error' in project_context:
            return 'error'
            
        # Check if all stages are complete
        stages = ['requirements_analysis', 'architecture_design', 'implementation', 'testing']
        completed_stages = [
            stage for stage in stages 
            if project_context.get(stage, {}).get('status') in ['completed', 'updated_with_mcp']
        ]
        
        if len(completed_stages) == len(stages):
            return 'completed'
            
        # Default to in-progress
        return 'in_progress'

╭────────────────────────╮
--- File: out_homing.py ---
╰────────────────────────╯

#⅛¼ START OF DOCUMENT ⅜#
#!/usr/bin/env python3
"""
WIRE #3 FIX: out_homing.py - Mixed Content Generation for Parser + REAL LLM CALLS
The key fix: Generate SINGLE MIXED CONTENT response that xedit.py can parse
WITH API KEY ROTATION + PROXY SUPPORT + RETRY LOGIC
"""

import json
import datetime
import sys
import time
import random
import requests
from pathlib import Path
from typing import Dict, List, Any, Optional
import re

# Import all the bird modules (same directory)
from spark import create_spark_analyst
from falcon import create_falcon_architect  
from eagle import create_eagle_implementer
from hawk import create_hawk_qa_specialist

# GROQ API CONFIGURATION WITH KEY ROTATION
GROQ_API_KEYS = [
    "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",
    "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", 
    "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",
    "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"
]

# PROXY CONFIGURATION
PROXY_CONFIG = {
    "http": "http://0aa180faa467ad67809b__cr.us:6dc612d4a08ca89d@gw.dataimpulse.com:823",
    "https": "http://0aa180faa467ad67809b__cr.us:6dc612d4a08ca89d@gw.dataimpulse.com:823"
}

# MODEL ASSIGNMENTS BASED ON TESTING RESULTS
STAGE_MODEL_ASSIGNMENTS = {
    "spark": "meta-llama/llama-4-scout-17b-16e-instruct",       # Speed critical
    "falcon": "meta-llama/llama-4-maverick-17b-128e-instruct",  # 128K context
    "eagle": "meta-llama/llama-4-scout-17b-16e-instruct",       # Fast code gen
    "hawk": "meta-llama/llama-4-maverick-17b-128e-instruct",    # Thorough analysis
    "final": "meta-llama/llama-4-maverick-17b-128e-instruct"    # Comprehensive
}

class OutHomingOrchestrator:
    """OUT-HOMING - Pipeline Conductor & Mixed Content Generator WITH REAL LLM CALLS"""
    
    def __init__(self):
        self.stage_name = "OUT-HOMING"
        self.icon = "🏠"
        self.specialty = "Pipeline Orchestration & LLM Communication"
        
        # Initialize all birds
        self.spark = create_spark_analyst()
        self.falcon = create_falcon_architect()
        self.eagle = create_eagle_implementer()
        self.hawk = create_hawk_qa_specialist()
        
        # API state tracking for key rotation
        self.current_key_index = 0
        self.api_call_count = 0
        
        # Pipeline state
        self.pipeline_results = {}
        self.session_timestamp = self._generate_session_timestamp()
    
    def _generate_session_timestamp(self):
        """Generate session timestamp in military format: week-day-hourminute"""
        now = datetime.datetime.now()
        week = now.isocalendar()[1] 
        day = now.weekday() + 1
        hour_minute = now.strftime("%H%M")
        return f"{week:02d}-{day:02d}-{hour_minute}"
    
    def _get_next_api_key(self):
        """Rotate through API keys evenly"""
        key = GROQ_API_KEYS[self.current_key_index]
        self.current_key_index = (self.current_key_index + 1) % len(GROQ_API_KEYS)
        self.api_call_count += 1
        return key

#¼¼¼¼¼¼ 1/4 MARKER ¼¼¼¼¼#
    
    def _make_real_llm_call(self, prompt: str, stage: str, attempt: int = 1) -> Dict[str, Any]:
        """Make REAL Groq API call with proxy support and fallback"""
        
        api_key = self._get_next_api_key()
        model = STAGE_MODEL_ASSIGNMENTS.get(stage, "meta-llama/llama-4-scout-17b-16e-instruct")
        
        # Groq API endpoint
        url = "https://api.groq.com/openai/v1/chat/completions"
        
        # Request headers
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        # Request payload (optimized for mixed content based on testing)
        payload = {
            "model": model,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.3,
            "max_tokens": 1024,
            "top_p": 0.8,
            "stream": False
        }
        
        # Proxy configuration
        proxies = None
        connection_type = "proxy"
        if attempt == 1:
            proxies = PROXY_CONFIG
        else:
            connection_type = "direct"
            
        print(f"🌐 API Call #{self.api_call_count} - {stage.upper()} - {model} - {connection_type} (attempt {attempt})")
        
        try:
            # Make the request
            response = requests.post(
                url,
                headers=headers,
                json=payload,
                proxies=proxies,
                timeout=30
            )
            
            response.raise_for_status()
            data = response.json()
            
            # Extract response content
            content = data["choices"][0]["message"]["content"]
            
            print(f"✅ {stage.upper()} Success - {len(content)} chars - Key: {api_key[-8:]}")
            
            return {
                "success": True,
                "text": content,
                "model": model,
                "api_key_used": api_key[-8:],
                "char_count": len(content),
                "attempt": attempt,
                "connection_type": connection_type
            }
            
        except requests.exceptions.RequestException as e:
            print(f"❌ {stage.upper()} API Error (attempt {attempt}): {str(e)}")
            
            # Retry with direct connection if proxy failed
            if attempt == 1:
                print(f"🔄 Retrying {stage.upper()} with direct connection...")
                return self._make_real_llm_call(prompt, stage, attempt=2)
            
            return {
                "success": False,
                "error": str(e),
                "model": model,
                "attempt": attempt,
                "connection_type": connection_type
            }
    
    def orchestrate_full_pipeline(self, user_request: str) -> Dict[str, Any]:
        """
        MAIN ORCHESTRATION with REAL LLM API CALLS
        Runs 4-stage pipeline then generates mixed content response for parser
        """
        
        print(f"🚀 OUT-HOMING: Starting pipeline orchestration...")
        print(f"📅 Session: {self.session_timestamp}")
        print(f"🔑 API Keys: {len(GROQ_API_KEYS)} available")
        
        try:
            # Step 1: Run all 4 birds with REAL LLM calls
            bird_results = self._run_all_birds_with_real_llm(user_request)
            
            if not bird_results["success"]:
                return {
                    "success": False,
                    "error": f"Bird pipeline failed: {bird_results.get('error')}"
                }
            
            # Step 2: WIRE #3 FIX - Generate mixed content response for parser
            mixed_content_response = self._generate_mixed_content_response(
                user_request, 
                bird_results["stage_results"]
            )
            
            # Step 3: Structure response for MCP
            return {
                "success": True,
                "session_timestamp": self.session_timestamp,
                "stage_results": bird_results["stage_results"],
                "final_response": mixed_content_response,
                "total_birds": 4,
                "pipeline_type": "full_orchestration",
                "api_calls_made": self.api_call_count
            }
            
        except Exception as e:
            print(f"❌ OUT-HOMING ERROR: {e}")
            return {
                "success": False,
                "error": f"Pipeline orchestration failed: {str(e)}"
            }

#½½½½½½ 1/2 MARKER ½½½½½#
    
    def _run_all_birds_with_real_llm(self, user_request: str) -> Dict[str, Any]:
        """Run all 4 birds with REAL LLM API calls"""
        
        stage_results = {}
        
        try:
            # STAGE 1: SPARK (Requirements Analysis) with REAL LLM
            print("\n⚡ STAGE 1: SPARK - Requirements Analysis")
            spark_prompt_data = self.spark.analyze_project_request(user_request)
            spark_llm_response = self._make_real_llm_call(
                spark_prompt_data["prompt"], 
                "spark"
            )
            
            stage_results["spark"] = {
                "prompt": spark_prompt_data["prompt"],
                "response": spark_llm_response.get("text", ""),
                "model": spark_llm_response.get("model", "unknown"),
                "success": spark_llm_response.get("success", False),
                "char_count": spark_llm_response.get("char_count", 0),
                "api_key_used": spark_llm_response.get("api_key_used", "N/A")
            }
            
            if not spark_llm_response.get("success"):
                return {"success": False, "error": "SPARK LLM call failed"}
            
            # Log SPARK results
            log_file = Path("/home/flintx/peacock/logs/prompt-spark.log")
            log_file.parent.mkdir(exist_ok=True)
            with open(log_file, 'a', encoding='utf-8') as f:
                f.write(f"\n{'='*80}\n")
                f.write(f"TIMESTAMP: {datetime.datetime.now().isoformat()}\n")
                f.write(f"PROMPT ({len(spark_prompt_data['prompt'])} chars):\n")
                f.write(spark_prompt_data['prompt'])
                f.write(f"\nRESPONSE ({len(spark_llm_response.get('text', ''))} chars):\n")
                f.write(spark_llm_response.get('text', ''))
                f.write("\n" + "="*80 + "\n")
            
            # STAGE 2: FALCON (Architecture Design) with REAL LLM
            print("\n🦅 STAGE 2: FALCON - Architecture Design")
            
            # Create proper input for FALCON (single parameter)
            falcon_input = {
                "user_request": user_request,
                "spark_analysis": spark_llm_response["text"],
                "requirements_data": {
                    "core_objective": "Based on SPARK analysis",
                    "analysis_complete": True
                }
            }
            
            falcon_prompt_data = self.falcon.design_architecture(falcon_input)
            falcon_llm_response = self._make_real_llm_call(
                falcon_prompt_data["prompt"],
                "falcon"
            )
            
            stage_results["falcon"] = {
                "prompt": falcon_prompt_data["prompt"],
                "response": falcon_llm_response.get("text", ""),
                "model": falcon_llm_response.get("model", "unknown"),
                "success": falcon_llm_response.get("success", False),
                "char_count": falcon_llm_response.get("char_count", 0),
                "api_key_used": falcon_llm_response.get("api_key_used", "N/A")
            }
            
            if not falcon_llm_response.get("success"):
                return {"success": False, "error": "FALCON LLM call failed"}
            
            # STAGE 3: EAGLE (Code Implementation) with REAL LLM
            print("\n🦅 STAGE 3: EAGLE - Code Implementation")
            
            # Create proper input for EAGLE
            eagle_input = {
                "raw_design": falcon_llm_response["text"],
                "json_data": {
                    "architecture_complete": True,
                    "falcon_analysis": "Architecture design completed"
                },
                "user_request": user_request
            }
            
            eagle_prompt_data = self.eagle.implement_code(eagle_input)
            eagle_llm_response = self._make_real_llm_call(
                eagle_prompt_data["prompt"],
                "eagle"
            )
            
            stage_results["eagle"] = {
                "prompt": eagle_prompt_data["prompt"],
                "response": eagle_llm_response.get("text", ""),
                "model": eagle_llm_response.get("model", "unknown"),
                "success": eagle_llm_response.get("success", False),
                "char_count": eagle_llm_response.get("char_count", 0),
                "api_key_used": eagle_llm_response.get("api_key_used", "N/A")
            }
            
            if not eagle_llm_response.get("success"):
                return {"success": False, "error": "EAGLE LLM call failed"}
            
            # STAGE 4: HAWK (QA & Testing) with REAL LLM
            print("\n🦅 STAGE 4: HAWK - QA & Testing")
            
            # Create proper input for HAWK
            hawk_input = {
                "user_request": user_request,
                "spark_analysis": spark_llm_response["text"],
                "falcon_architecture": falcon_llm_response["text"],
                "eagle_implementation": eagle_llm_response["text"],
                "qa_requirements": {
                    "comprehensive_testing": True,
                    "security_review": True,
                    "performance_analysis": True
                }
            }
            
            hawk_prompt_data = self.hawk.create_qa_strategy(hawk_input)
            hawk_llm_response = self._make_real_llm_call(
                hawk_prompt_data["prompt"],
                "hawk"
            )
            
            stage_results["hawk"] = {
                "prompt": hawk_prompt_data["prompt"],
                "response": hawk_llm_response.get("text", ""),
                "model": hawk_llm_response.get("model", "unknown"),
                "success": hawk_llm_response.get("success", False),
                "char_count": hawk_llm_response.get("char_count", 0),
                "api_key_used": hawk_llm_response.get("api_key_used", "N/A")
            }
            
            if not hawk_llm_response.get("success"):
                return {"success": False, "error": "HAWK LLM call failed"}
            
            print(f"\n🎉 ALL 4 STAGES COMPLETED WITH REAL LLM CALLS!")
            print(f"📊 Total API calls made: {self.api_call_count}")
            
            return {
                "success": True,
                "stage_results": stage_results
            }
            
        except Exception as e:
            print(f"❌ Bird execution error: {e}")
            return {
                "success": False,
                "error": f"Bird execution failed: {str(e)}"
            }

#¾¾¾¾¾¾ 3/4 MARKER ¾¾¾¾¾#
    
    def _generate_mixed_content_response(self, user_request: str, stage_results: Dict[str, Any]) -> str:
        """
        Generate mixed content response that xedit.py can parse
        This is the KEY function - creates the exact format the parser expects
        """
        
        print("🎯 WIRE #3 FIX: Generating mixed content for parser...")
        
        # Extract key data from stage results
        spark_data = stage_results.get("spark", {})
        falcon_data = stage_results.get("falcon", {})
        eagle_data = stage_results.get("eagle", {})
        hawk_data = stage_results.get("hawk", {})
        
        # Build mixed content response in parser-friendly format
        response_parts = []
        
        # Add project header
        response_parts.extend([
            f"# 🦚 PEACOCK PROJECT: {user_request}\n\n",
            "## Project Requirements Analysis (SPARK)\n",
            spark_data.get("response", "No SPARK analysis available") + "\n\n",
            
            "## System Architecture (FALCON)\n", 
            falcon_data.get("response", "No FALCON architecture available") + "\n\n",
            
            "## Implementation Details (EAGLE)\n",
            eagle_data.get("response", "No EAGLE implementation available") + "\n\n",
            
            "## Quality Assurance Strategy (HAWK)\n",
            hawk_data.get("response", "No HAWK QA strategy available") + "\n\n"
        ])
        
        # Extract and format code files from EAGLE response for parser
        eagle_response_text = eagle_data.get("response", "")
        if eagle_response_text:
            response_parts.append("## Code Implementation Files\n\n")
            
            # Extract code blocks with filename patterns that xedit.py expects
            code_files = self._extract_code_files_from_eagle(eagle_response_text)
            
            for i, code_file in enumerate(code_files, 1):
                filename = code_file.get("filename", f"file_{i}")
                code = code_file.get("code", "# No code found")
                language = code_file.get("language", "text")
                
                response_parts.extend([
                    f"**filename: {filename}**\n",
                    f"```{language}\n",
                    code + "\n",
                    "```\n\n"
                ])
        
        # Add implementation notes from all stages
        response_parts.append("## Implementation Notes\n")
        
        # Extract notes from all stages
        all_notes = []
        for stage_name, stage_data in stage_results.items():
            stage_response = stage_data.get("response", "")
            notes = self._extract_implementation_notes(stage_response, stage_name)
            all_notes.extend(notes)
        
        for note in all_notes[:10]:  # Limit to 10 notes
            response_parts.append(f"- {note}\n")
        
        response_parts.append("\n")
        
        # Add testing checklist from HAWK
        if hawk_data.get("response"):
            response_parts.append("## Testing Checklist\n")
            hawk_response = hawk_data.get("response", "")
            test_items = self._extract_test_items(hawk_response)
            
            for test_item in test_items[:8]:  # Limit to 8 items
                response_parts.append(f"- {test_item}\n")
            
            response_parts.append("\n")
        
        # Combine all parts
        final_response = "".join(response_parts)
        
        print(f"📄 Mixed content generated: {len(final_response)} characters")
        print(f"🔍 Code files found: {len(self._extract_code_files_from_eagle(eagle_data.get('response', '')))}")
        
        return final_response
    
    def _extract_code_files_from_eagle(self, eagle_response: str) -> List[Dict[str, Any]]:
        """Extract code files from EAGLE response in format xedit.py expects"""
        
        code_files = []
        
        # Pattern 1: filename: pattern (what EAGLE should generate)
        filename_pattern = r'```filename:\s*([^\n]+)\n(.*?)\n```'
        filename_matches = re.findall(filename_pattern, eagle_response, re.DOTALL | re.IGNORECASE)
        
        for filename, code in filename_matches:
            code_files.append({
                "filename": filename.strip(),
                "code": code.strip(),
                "language": self._detect_language_from_filename(filename.strip())
            })
        
        # Pattern 2: Standard markdown code blocks
        if not code_files:  # Only if no filename patterns found
            code_block_pattern = r'```(\w+)?\n(.*?)\n```'
            code_matches = re.findall(code_block_pattern, eagle_response, re.DOTALL)
            
            for i, (language, code) in enumerate(code_matches):
                if len(code.strip()) > 50:  # Only substantial code blocks
                    filename = self._infer_filename_from_code(code, language)
                    
                    code_files.append({
                        "filename": filename,
                        "code": code.strip(),
                        "language": language or "text"
                    })
        
        return code_files
    
    def _detect_language_from_filename(self, filename: str) -> str:
        """Detect programming language from filename"""
        
        ext_map = {
            '.py': 'python',
            '.js': 'javascript', 
            '.html': 'html',
            '.css': 'css',
            '.json': 'json',
            '.md': 'markdown',
            '.txt': 'text',
            '.sh': 'bash',
            '.sql': 'sql'
        }
        
        for ext, lang in ext_map.items():
            if filename.lower().endswith(ext):
                return lang
        
        return 'text'
    
    def _infer_filename_from_code(self, code: str, language: str) -> str:
        """Infer filename from code content and language"""
        
        # Look for common patterns in code that indicate filename
        if 'class ' in code and language == 'python':
            class_match = re.search(r'class\s+(\w+)', code)
            if class_match:
                return f"{class_match.group(1).lower()}.py"
        
        if 'function ' in code and language == 'javascript':
            return "script.js"
        
        if '<html' in code or '<!DOCTYPE' in code:
            return "index.html"
        
        if language == 'css' or 'body {' in code or '.container' in code:
            return "styles.css"
        
        # Default naming based on language
        language_defaults = {
            'python': 'main.py',
            'javascript': 'app.js', 
            'html': 'index.html',
            'css': 'styles.css',
            'json': 'config.json'
        }
        
        return language_defaults.get(language, f"file.{language or 'txt'}")
    
    def _extract_implementation_notes(self, response_text: str, stage_name: str) -> List[str]:
        """Extract implementation notes from stage response"""
        
        notes = []
        
        # Look for bullet points
        bullet_pattern = r'[•\-\*]\s*([^\n]+)'
        bullet_matches = re.findall(bullet_pattern, response_text)
        
        for match in bullet_matches:
            if len(match.strip()) > 10:  # Substantial notes only
                notes.append(f"[{stage_name.upper()}] {match.strip()}")
        
        # Look for numbered lists
        numbered_pattern = r'\d+\.\s*([^\n]+)'
        numbered_matches = re.findall(numbered_pattern, response_text)
        
        for match in numbered_matches:
            if len(match.strip()) > 10:
                notes.append(f"[{stage_name.upper()}] {match.strip()}")
        
        return notes[:3]  # Limit per stage
    
    def _extract_test_items(self, hawk_response: str) -> List[str]:
        """Extract test items from HAWK response"""
        
        test_items = []
        
        # Look for test-related patterns
        test_patterns = [
            r'test[:\s]+([^\n]+)',
            r'testing[:\s]+([^\n]+)',
            r'verify[:\s]+([^\n]+)',
            r'check[:\s]+([^\n]+)',
            r'validate[:\s]+([^\n]+)'
        ]
        
        for pattern in test_patterns:
            matches = re.findall(pattern, hawk_response, re.IGNORECASE)
            for match in matches:
                if len(match.strip()) > 10:
                    test_items.append(match.strip())
        
        # Look for bullet points in testing sections
        testing_section = re.search(r'test.*?(?=\n\n|\n#|$)', hawk_response, re.DOTALL | re.IGNORECASE)
        if testing_section:
            bullets = re.findall(r'[•\-\*]\s*([^\n]+)', testing_section.group())
            test_items.extend([b.strip() for b in bullets if len(b.strip()) > 10])
        
        return list(set(test_items))  # Remove duplicates

def create_homing_orchestrator() -> OutHomingOrchestrator:
    """Factory function to create OUT-HOMING orchestrator instance"""
    return OutHomingOrchestrator()

# Test function
def test_out_homing_orchestrator():
    """Test the complete OUT-HOMING orchestration with REAL LLM calls"""
    
    print("🧪 TESTING OUT-HOMING ORCHESTRATOR WITH REAL LLM INTEGRATION")
    print("="*60)
    
    # Create orchestrator
    homing = create_homing_orchestrator()
    
    # Test with sample request
    test_request = "Build a simple snake game with HTML, CSS, and JavaScript"
    
    print(f"🎯 Test Request: {test_request}")
    print(f"🔑 API Keys Available: {len(GROQ_API_KEYS)}")
    print(f"🌐 Proxy Configured: {PROXY_CONFIG['http']}")
    
    result = homing.orchestrate_full_pipeline(test_request)
    
    print(f"\n📊 ORCHESTRATION RESULTS:")
    print(f"✅ Success: {result.get('success')}")
    print(f"📅 Session: {result.get('session_timestamp')}")
    print(f"🐦 Birds Run: {result.get('total_birds', 0)}")
    print(f"🌐 API Calls Made: {result.get('api_calls_made', 0)}")
    
    if result.get("success"):
        stage_results = result.get("stage_results", {})
        print(f"\n🎯 STAGE RESULTS WITH REAL LLM RESPONSES:")
        for stage, data in stage_results.items():
            char_count = len(data.get("response", ""))
            model = data.get("model", "unknown")
            api_key = data.get("api_key_used", "N/A")
            print(f"   {stage.upper()}: {char_count} chars ({model}) [Key: {api_key}]")
        
        final_response = result.get("final_response", "")
        print(f"\n🎯 MIXED CONTENT FOR PARSER:")
        print(f"   📏 Length: {len(final_response)} characters")
        print(f"   📝 Preview: {final_response[:200]}...")
        
        # Test parsing readiness
        print(f"\n🔍 PARSER READINESS CHECK:")
        filename_headers = final_response.count("**filename:")
        code_blocks = final_response.count("```")
        print(f"   📁 Filename headers: {filename_headers}")
        print(f"   💻 Code blocks: {code_blocks}")
        print(f"   ✅ Parser ready: {filename_headers > 0 and code_blocks > 0}")
        
    else:
        print(f"❌ Error: {result.get('error')}")
    
    return result

if __name__ == "__main__":
    # Test the orchestrator with real LLM integration
    test_out_homing_orchestrator()

#⅛¼ END OF DOCUMENT ⅜#

┏━━━━•❅•°•❈•°•❅•━━━━┓
--- File: spark.py ---
┗━━━━•❅•°•❈•°•❅•━━━━┛

#!/usr/bin/env python3
"""
spark.py - SPARK Requirements Analysis Bird
The strategic analyst who breaks down user requests into clear, actionable requirements
"""

import json
import re
from typing import Dict, List, Any

class SparkAnalyst:
    """SPARK - The Requirements Whisperer"""
    
    def __init__(self):
        self.stage_name = "SPARK"
        self.icon = "⚡"
        self.specialty = "Strategic Requirements Analysis"
        self.optimal_model = "llama3-8b-8192"  # Speed for requirements
    
    def analyze_project_request(self, user_request: str) -> Dict[str, Any]:
        """
        Generate requirements analysis prompt based on user request.
        MCP will handle the actual LLM call.
        """
        print(f"⚡ SPARK ANALYST: Generating requirements analysis prompt...")
        
        # Generate the SPARK analysis prompt
        spark_prompt = self._build_spark_prompt(user_request)
        
        # Package the analysis for MCP processing
        spark_analysis = {
            "stage": "SPARK",
            "prompt": spark_prompt,
            "user_request": user_request,
            "model": self.optimal_model,
            "temperature": 0.3,
            "max_tokens": 1024,
            "analysis_type": "requirements_extraction"
        }
        
        print(f"✅ SPARK prompt generated: {len(spark_prompt)} characters")
        return spark_analysis
    
    def _build_spark_prompt(self, user_request: str) -> str:
        """Build the strategic SPARK analysis prompt"""
        
        return f"""<thinking>
The user wants me to analyze this project idea strategically. I need to break this down into clear, actionable components.

Project: {user_request}

I should provide:
1. Core objective - what's the main goal?
2. Current state - what problems does this solve?
3. Target state - what's the desired outcome?
4. In scope - what features are included?
5. Out of scope - what's not included?
</thinking>

Act as Spark, a strategic requirements analyst. Analyze this project idea:

Project: {user_request}

Provide analysis in this EXACT format:

**1. Core Objective:**
[One clear sentence describing the main goal]

**2. Current State:**
[Current situation/problems this solves]

**3. Target State:**
[Desired end state after implementation]

**4. In Scope:**
- [Feature 1]
- [Feature 2] 
- [Feature 3]

**5. Out of Scope:**
- [What's NOT included]
- [Future considerations]

Then provide the structured data as JSON:
```json
{{
    "core_objective": "string",
    "current_state": "string",
    "target_state": "string", 
    "in_scope": ["list"],
    "out_of_scope": ["list"],
    "confidence_score": 8
}}
"""
        
        return prompt
    
    def validate_spark_response(self, response_text: str) -> Dict[str, Any]:
        """Validate that SPARK response contains required elements"""
        
        validation_result = {
            "valid": False,
            "has_objective": False,
            "has_scope": False,
            "has_json": False,
            "character_count": len(response_text),
            "quality_score": 0
        }
        
        # Check for core sections
        if "Core Objective:" in response_text:
            validation_result["has_objective"] = True
            validation_result["quality_score"] += 2
        
        if "In Scope:" in response_text and "Out of Scope:" in response_text:
            validation_result["has_scope"] = True
            validation_result["quality_score"] += 2
        
        # Check for JSON data
        json_pattern = r'```json\s*\n(.*?)\n```'
        json_match = re.search(json_pattern, response_text, re.DOTALL)
        if json_match:
            try:
                json.loads(json_match.group(1))
                validation_result["has_json"] = True
                validation_result["quality_score"] += 3
            except json.JSONDecodeError:
                pass
        
        # Determine if valid
        validation_result["valid"] = (
            validation_result["has_objective"] and 
            validation_result["has_scope"] and
            validation_result["character_count"] > 200
        )
        
        return validation_result
    
    def extract_requirements_data(self, response_text: str) -> Dict[str, Any]:
        """Extract structured requirements data from SPARK response"""
        
        requirements = {
            "core_objective": "",
            "current_state": "",
            "target_state": "",
            "in_scope": [],
            "out_of_scope": [],
            "json_data": {},
            "raw_analysis": response_text
        }
        
        # Extract core objective
        obj_match = re.search(r'\*\*1\. Core Objective:\*\*\s*\n([^\n*]+)', response_text)
        if obj_match:
            requirements["core_objective"] = obj_match.group(1).strip()
        
        # Extract current state
        current_match = re.search(r'\*\*2\. Current State:\*\*\s*\n([^\n*]+)', response_text)
        if current_match:
            requirements["current_state"] = current_match.group(1).strip()
        
        # Extract target state
        target_match = re.search(r'\*\*3\. Target State:\*\*\s*\n([^\n*]+)', response_text)
        if target_match:
            requirements["target_state"] = target_match.group(1).strip()
        
        # Extract in scope items
        in_scope_section = re.search(r'\*\*4\. In Scope:\*\*\s*\n((?:- [^\n]+\n?)+)', response_text)
        if in_scope_section:
            scope_items = re.findall(r'- ([^\n]+)', in_scope_section.group(1))
            requirements["in_scope"] = [item.strip() for item in scope_items]
        
        # Extract out of scope items
        out_scope_section = re.search(r'\*\*5\. Out of Scope:\*\*\s*\n((?:- [^\n]+\n?)+)', response_text)
        if out_scope_section:
            out_items = re.findall(r'- ([^\n]+)', out_scope_section.group(1))
            requirements["out_of_scope"] = [item.strip() for item in out_items]
        
        # Extract JSON data
        json_pattern = r'```json\s*\n(.*?)\n```'
        json_match = re.search(json_pattern, response_text, re.DOTALL)
        if json_match:
            try:
                requirements["json_data"] = json.loads(json_match.group(1))
            except json.JSONDecodeError:
                requirements["json_data"] = {}
        
        return requirements

# Factory function for SPARK bird
def create_spark_analyst() -> SparkAnalyst:
    """Factory function to create SPARK analyst instance"""
    return SparkAnalyst()

# Test function for SPARK bird
def test_spark_bird():
    """Test the SPARK bird with sample input"""
    spark = create_spark_analyst()
    
    test_request = "Build a snake game with HTML, CSS, and JavaScript"
    analysis = spark.analyze_project_request(test_request)
    
    print("🧪 TESTING SPARK BIRD")
    print(f"📝 Request: {test_request}")
    print(f"⚡ Stage: {analysis['stage']}")
    print(f"🤖 Model: {analysis['model']}")
    print(f"📊 Analysis Type: {analysis['analysis_type']}")
    print(f"📏 Prompt Length: {len(analysis['prompt'])} characters")
    
    return analysis

if __name__ == "__main__":
    # Test SPARK bird independently
    test_spark_bird()